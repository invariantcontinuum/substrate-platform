{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Structural Integrity Platform (Substrate)","text":"<p>Substrate Platform is a governance layer over modern software delivery \u2014 powered by a live knowledge graph and an internal integration marketplace.</p> <p>AI has massively accelerated code creation but often at the cost of architectural consistency, security guarantees, and shared understanding. This platform restores control, visibility, and confidence by governing AI-generated code and preserving architectural intent.</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>The documentation is organized as follows:</p>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<ul> <li>Architecture Overview: High-level system design and components.</li> <li>Technical Implementation Spec: Deep dive into the GraphRAG pipeline, Rust ingestion, and tech stack choices.</li> </ul>"},{"location":"#product","title":"\ud83d\ude80 Product","text":"<ul> <li>Product Requirements: Detailed feature requirements, user flows, and personas.</li> <li>Strategy &amp; Market: Deployment models, pricing, and go-to-market strategy.</li> <li>Gap Analysis: Current implementation status vs. vision.</li> <li>FAQ: Frequently asked questions.</li> </ul>"},{"location":"#development","title":"\ud83d\udcbb Development","text":"<ul> <li>Frontend Guide: Frontend architecture, component library, and state management.</li> <li>Contributing: Guidelines for contributors.</li> <li>Roadmap: Future development plans.</li> <li>Refactor History: Summary of past refactoring efforts.</li> </ul>"},{"location":"#deployment","title":"\u2699\ufe0f Deployment","text":"<ul> <li>Docker Setup: Full Docker deployment guide.</li> <li>Kubernetes Guide: Deployment manifests and specificaitons.</li> <li>Docker Cheatsheet: Quick reference commands.</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/yourusername/substrate-platform.git\ncd substrate-platform\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>npm install\n</code></pre></p> </li> <li> <p>Start development server <pre><code>npm run dev\n</code></pre></p> </li> </ol> <p>For detailed setup instructions, see the Contributing Guide.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>See LICENSE file.</p>"},{"location":"architecture/overview/","title":"Structural Integrity Platform: System Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>The Structural Integrity Platform is a polyglot, event-driven system that treats software architecture as a living knowledge graph, enforcing architectural intent through policy-as-code evaluated against that graph in real time.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            STRUCTURAL INTEGRITY PLATFORM                          \u2502\n\u2502         (Multi-Modal Knowledge Graph + Policy Engine)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502                      \u2502                      \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Ingestion    \u2502     \u2502 GraphRAG       \u2502    \u2502 Governance   \u2502\n\u2502 Pipeline     \u2502     \u2502 Engine         \u2502    \u2502 Engine       \u2502\n\u2502 (Rust)       \u2502     \u2502 (Python)       \u2502    \u2502 (Go)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                     \u2502                      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Multi-Modal Storage    \u2502\n         \u2502  PostgreSQL  \u2502  Neo4j    \u2502\n         \u2502  Qdrant      \u2502  Redis    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#core-services","title":"Core Services","text":""},{"location":"architecture/overview/#1-ingestion-pipeline-rust","title":"1. Ingestion Pipeline (Rust)","text":"<p>Responsibility: Real-time code analysis and entity extraction.</p> <p>Technology Stack: - tree-sitter v0.24.x: Incremental parsing (2,157 lines in ~6.5ms) - stack-graphs: Cross-file reference resolution - notify v6.x: Filesystem watching - rayon: Parallel parsing - tokio: Async I/O orchestration</p> <p>Capabilities: - Multi-language AST extraction (150+ supported languages) - Deterministic code structure graphs - Import/dependency graph construction - 15/15 correctness on architectural queries (benchmark-proven)</p> <p>Pipeline Flow: <pre><code>File Watcher \u2192 Change Queue \u2192 Parser Pool (tree-sitter + stack-graphs)\n            \u2192 Entity Emitter \u2192 Output Sink (NATS/Kafka)\n</code></pre></p>"},{"location":"architecture/overview/#2-graphrag-intelligence-engine-python","title":"2. GraphRAG Intelligence Engine (Python)","text":"<p>Responsibility: Semantic reasoning with LLM-powered graph generation.</p> <p>Technology Stack: - FastAPI v0.115.x: HTTP/REST API layer - Celery v5.4.x + Redis: Async task processing - vLLM v0.6.x: High-throughput LLM inference with PagedAttention - Prefect v3.x: Workflow orchestration</p> <p>Capabilities: - Microsoft GraphRAG 7-phase indexing pipeline - Hierarchical Leiden Algorithm for community detection (via <code>graspologic</code>) - Three query strategies:   - Local Search: Targeted queries (\"What depends on auth service?\")   - Global Search: Holistic queries (\"What are systemic risks?\")   - DRIFT Search: Combined breadth + depth reasoning</p> <p>GraphRAG Pipeline: 1. Text chunking: 300-600 tokens for code, configurable overlap 2. Graph extraction: LLM-powered entity/relationship extraction 3. Graph augmentation: Multi-level community detection 4. Community summarization: LLM-generated cluster reports 5. Document linking: Provenance tracking 6. Node2Vec + UMAP: Visualization embeddings 7. Text embeddings: Vector search enablement</p> <p>Custom Entity Types: <pre><code>entity_types: [Service, API, Module, Database, Component, Team, \n               Repository, Package, Function, Class, Interface, \n               Endpoint, Queue, Cache]\n</code></pre></p> <p>Custom Relationship Types: <pre><code>depends_on, calls, imports, owns, maintains, reads_from, \nwrites_to, deploys_to, implements, exposes\n</code></pre></p>"},{"location":"architecture/overview/#3-governance-engine-go","title":"3. Governance Engine (Go)","text":"<p>Responsibility: Real-time policy evaluation and enforcement.</p> <p>Technology Stack: - OPA SDK (embedded): Sub-millisecond policy evaluation - go-chi/chi: HTTP routing - nats.go: Event consumption - sony/gobreaker: Circuit breaking - golang.org/x/time/rate: Rate limiting</p> <p>Capabilities: - <code>graph.reachable</code> built-in for transitive closure (O(V + E)) - Circular dependency detection - Layer violation checking - Cross-boundary access enforcement - Graduated enforcement (Observe \u2192 Advise \u2192 Enforce) - Real-time evaluation (&lt;100ms per policy)</p> <p>Example Policy Patterns:</p> <p>Circular Dependency Detection: <pre><code>package architecture.circular\nimport rego.v1\n\ncircular_dependency contains cycle if {\n    some component\n    data.architecture.dependencies[component]\n    some neighbor in data.architecture.dependencies[component]\n    reachable := graph.reachable(data.architecture.dependencies, {neighbor})\n    component in reachable\n    cycle := {\"component\": component, \"via\": neighbor,\n      \"msg\": sprintf(\"Circular: '%s' -&gt; '%s' -&gt; ... -&gt; '%s'\", \n                     [component, neighbor, component])}\n}\n</code></pre></p> <p>Layer Violation Detection: <pre><code>package architecture.layers\nimport rego.v1\n\nlayer_order := {\"presentation\": 4, \"application\": 3, \n                \"domain\": 2, \"infrastructure\": 1, \"data\": 0}\n\nlayer_violation contains violation if {\n    some source, targets in data.architecture.dependencies\n    some target in targets\n    layer_order[data.architecture.component_layers[source]] &lt; \n      layer_order[data.architecture.component_layers[target]]\n    violation := {\"source\": source, \"target\": target,\n      \"msg\": sprintf(\"Layer violation: '%s' (%s) depends on '%s' (%s)\",\n        [source, data.architecture.component_layers[source], \n         target, data.architecture.component_layers[target]])}\n}\n</code></pre></p>"},{"location":"architecture/overview/#data-storage-layer","title":"Data &amp; Storage Layer","text":"<p>The platform uses a CQRS architecture: PostgreSQL is the write model and source of truth; Neo4j and Qdrant are read-optimized projections built from events.</p>"},{"location":"architecture/overview/#storage-breakdown","title":"Storage Breakdown","text":"<ol> <li>PostgreSQL (Write Model - Source of Truth)</li> <li>Entity metadata, timestamps, file hashes</li> <li>User accounts, policies, violations</li> <li>Audit logs, event outbox</li> <li> <p>Outbox Pattern ensures atomic event publication</p> </li> <li> <p>Neo4j (Read Model - Graph Traversal)</p> </li> <li>Entities as typed nodes (<code>:Service</code>, <code>:API</code>, <code>:Module</code>)</li> <li>Relationships as typed edges</li> <li>Communities as <code>:Community</code> nodes</li> <li>TextUnits as <code>:TextUnit</code> nodes</li> <li> <p>Sub-5ms traversal queries (1-5 hops)</p> </li> <li> <p>Qdrant (Read Model - Vector Search)</p> </li> <li>Three collections:<ul> <li><code>entity_embeddings</code></li> <li><code>text_unit_embeddings</code></li> <li><code>community_report_embeddings</code></li> </ul> </li> <li>Hybrid query pattern: Qdrant seed \u2192 Neo4j expansion \u2192 LLM generation</li> <li>20-25% accuracy uplift over pure vector search</li> <li> <p>1-2 second query latency</p> </li> <li> <p>Redis (Caching &amp; Task Queue)</p> </li> <li>LLM response caching</li> <li>Celery task queue</li> <li>Session tokens</li> <li> <p>Rate limit buckets</p> </li> <li> <p>MinIO (Object Storage)</p> </li> <li>Large files, backups</li> <li>Vector index snapshots</li> </ol>"},{"location":"architecture/overview/#dual-graph-strategy","title":"Dual-Graph Strategy","text":"<p>The platform's core innovation combines two complementary approaches:</p> <ol> <li>Deterministic AST-derived graphs (Rust ingestion):</li> <li>Precise code structure</li> <li>Import/dependency resolution</li> <li> <p>15/15 correctness on architectural queries</p> </li> <li> <p>LLM-generated graphs (Python GraphRAG):</p> </li> <li>Documentation semantics</li> <li>Ticket relationships</li> <li>Team/ownership context</li> <li>Community detection</li> </ol> <p>This dual strategy delivers higher accuracy than either approach alone.</p>"},{"location":"architecture/overview/#event-driven-synchronization","title":"Event-Driven Synchronization","text":""},{"location":"architecture/overview/#event-bus","title":"Event Bus","text":"<p>Small deployments (5-20 devs): NATS JetStream - Single ~10MB binary - Sub-millisecond latency - ~200K msg/s persistent throughput</p> <p>Growth/Enterprise (20+ devs): Apache Kafka - Multiple topic partitions - Debezium CDC integration - Kafka Connect ecosystem</p>"},{"location":"architecture/overview/#event-flow-cqrs-outbox-pattern","title":"Event Flow (CQRS + Outbox Pattern)","text":"<pre><code>1. Write to PostgreSQL + event_outbox (single transaction)\n2. Dispatcher publishes events to NATS/Kafka\n3. Consumers update read models:\n   - Neo4j: Cypher MERGE statements\n   - Qdrant: Vector upserts (after embedding generation)\n   - OPA: Policy data synchronization\n</code></pre>"},{"location":"architecture/overview/#debezium-cdc-change-data-capture","title":"Debezium CDC (Change Data Capture)","text":"<p>For Kafka deployments: - Captures PostgreSQL WAL via logical replication - Streams to Kafka topics - Zero code changes needed</p> <p>PostgreSQL Configuration: <pre><code>wal_level = logical\nmax_replication_slots = 4\nREPLICA IDENTITY FULL -- on tracked tables\n</code></pre></p>"},{"location":"architecture/overview/#opa-data-synchronization","title":"OPA Data Synchronization","text":"<p>Three strategies: 1. Bundle API: CI/CD-cycle updates (OPA polls every 10-30s) 2. Push via Data API: Dynamic updates after graph changes 3. OPAL: Real-time event-driven sync (production recommended)</p>"},{"location":"architecture/overview/#quality-metrics-framework","title":"Quality Metrics Framework","text":""},{"location":"architecture/overview/#architectural-drift-conformance","title":"Architectural Drift &amp; Conformance","text":"<p>Coupling Metrics: - Afferent Coupling (Ca): Components depending on you - Efferent Coupling (Ce): Components you depend on - Instability Index: <code>I = Ce / (Ca + Ce)</code> (0.0 = stable, 1.0 = unstable) - Distance from Main Sequence: <code>D = |A + I - 1|</code>   - Zone of Pain: Concrete + Stable (hard to change)   - Zone of Uselessness: Abstract + Unstable</p> <p>Cohesion Metrics: - LCOM4: Connected components in method-attribute graph (target: 1) - Relational Cohesion: <code>H = (R + 1) / N</code> (target: \u22651.5)</p> <p>Conformance Checking (Reflexion Model): <pre><code>ConformanceScore = convergences / (convergences + divergences + absences)\n</code></pre></p> <p>Normalized Cumulative Component Dependency (NCCD): - Compares system CCD vs. balanced binary tree - NCCD &gt; 1.0 indicates worse-than-average modularity</p>"},{"location":"architecture/overview/#documentation-freshness","title":"Documentation Freshness","text":"<p>Exponential Decay Model: <pre><code>Freshness(t) = e^(-\u03bb \u00d7 \u0394t)\n</code></pre></p> <p>Decay Rates (\u03bb): - Architectural docs: 0.01 (~70-day half-life) - API docs: 0.05 (~14-day half-life) - Deployment guides: 0.1 (~7-day half-life)</p> <p>Composite Staleness Score: - Time decay: 25% - Code drift: 30% - Link rot: 15% - Reference validity: 20% - View frequency (inverse): 10%</p> <p>Research shows 28.9% of popular GitHub projects contain outdated code references.</p>"},{"location":"architecture/overview/#technical-debt-quantification","title":"Technical Debt Quantification","text":"<p>SQALE Methodology: <pre><code>Technical Debt Ratio = Remediation_Cost / (Lines_of_Code \u00d7 30_minutes)\n</code></pre></p> <p>Ratings: - A: \u22645% debt ratio - B: \u226410% - C: \u226420% - D: \u226450% - F: &gt;50%</p> <p>Note: Architectural debt (improper boundaries, layer violations) accounts for ~80% of technical debt but is often missed by code-level tools.</p>"},{"location":"architecture/overview/#dora-metrics-integration","title":"DORA Metrics Integration","text":"<p>Elite Performers: - Deployment Frequency: On-demand (multiple/day) - Lead Time for Changes: &lt;1 hour - Change Failure Rate: 0-5% - Mean Time to Restore (MTTR): &lt;1 hour</p> <p>Integration: - GitLab: Native DORA API (<code>/api/v4/projects/{id}/dora/metrics</code>) - GitHub: Workflow Runs API - Multi-platform: Apache DevLake</p>"},{"location":"architecture/overview/#composite-platform-health-score","title":"Composite Platform Health Score","text":"Dimension Weight Primary Inputs Architectural conformance 0.25 Reflexion alignment, coupling/cohesion Documentation health 0.15 Coverage \u00d7 Freshness Intent-reality alignment 0.20 Convergence ratio, violations Delivery performance (DORA) 0.15 Normalized four key metrics Technical debt 0.15 Inverted SQALE ratio Knowledge graph quality 0.10 Completeness \u00d7 Freshness \u00d7 Accuracy <p>Final Ratings: - A: \u22650.9 - B: \u22650.75 - C: \u22650.6 - D: \u22650.4 - F: &lt;0.4</p>"},{"location":"architecture/overview/#data-connector-architecture","title":"Data Connector Architecture","text":""},{"location":"architecture/overview/#connector-sdk-airbyte-inspired","title":"Connector SDK (Airbyte-inspired)","text":"<p>Three-tier approach: 1. No-code YAML: Common REST APIs 2. Low-code YAML + custom functions: Moderate complexity 3. Full Python SDK: Complex sources</p> <p>Key Patterns: - Docker container isolation - Standard protocol: SPEC \u2192 CHECK \u2192 DISCOVER \u2192 READ - Independent versioning - Incremental sync via cursor fields and state checkpoints - Credential management via external secrets store (Vault, AWS Secrets Manager) - Token bucket rate limiting</p>"},{"location":"architecture/overview/#source-specific-patterns","title":"Source-Specific Patterns","text":"<p>GitHub: - Use GitHub Apps (5,000-15,000 req/hr) - Webhooks for real-time updates (push, PR, issue events) - Conditional requests with ETag (304 responses don't count) - Git Trees API for monorepo traversal: <code>/git/trees/{sha}?recursive=1</code></p> <p>Jira: - REST API v3: <code>/rest/api/3/search/jql</code> - Sequential pagination with <code>nextPageToken</code> - <code>maxResults</code> up to 5,000 for smaller datasets - Extract: issues, epics, sprints, components, issue links</p> <p>Confluence: - CQL search: <code>type=page AND space=ARCH AND lastModified &gt;= \"2025-01-01\"</code> - Confluence Storage Format (XHTML with <code>ac:</code> macros) - 50 results per page max with body expansion</p> <p>Slack: - Socket Mode (works behind firewalls) - Bolt framework - Events: <code>message</code>, <code>reaction_added</code>, <code>file_shared</code> - Filter by channel type for privacy</p> <p>Hybrid Strategy: - Webhooks: Real-time event capture - Reconciliation polling: Every 15-60 minutes (catch missed events) - Full polling backfill: Initial setup</p>"},{"location":"architecture/overview/#entity-resolution-across-data-sources","title":"Entity Resolution Across Data Sources","text":"<p>The same person appears as: - GitHub username - Jira account ID - Slack user ID - Confluence author</p> <p>Three-layer resolution: 1. Deterministic matching: Email/SSO ID/username mapping 2. Probabilistic matching: Fellegi-Sunter model (via Splink v4.x) 3. ML-based matching: Active learning</p> <p>Canonical Entity Model: - Stores all identifiers - Resolution confidence scores - Supports entity evolution over time</p>"},{"location":"architecture/overview/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/overview/#resource-requirements-by-scale","title":"Resource Requirements by Scale","text":"<p>Small Team (5-20 devs): Docker Compose - All services in containers - NATS JetStream for events - Single PostgreSQL, Neo4j, Qdrant instances - gpt-4o-mini for GraphRAG indexing (~$5-50 per full re-index)</p> <p>Growth (20-100 devs): Kubernetes + Helm - Migrate to Kafka + Debezium for CDC - Neo4j clustering - Qdrant sharding - Dedicated GPU node for vLLM</p> <p>Enterprise (100+ devs): Multi-cluster Kubernetes - Kafka with multiple topic partitions - Neo4j Fabric for federation - Qdrant distributed mode - Enterprise OPA (Styra DAS) - OPAL for real-time policy synchronization</p>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#cicd-integration","title":"CI/CD Integration","text":"<p>GitHub Actions: <pre><code>- uses: open-policy-agent/setup-opa@v2\n- run: opa test policies/\n- run: opa eval --fail-defined \"data.architecture.violations\"\n</code></pre></p> <p>Conftest: Test configuration files against policies Pre-commit hooks: Local validation before push OPA Gatekeeper: Kubernetes admission controller</p>"},{"location":"architecture/overview/#natural-language-to-rego","title":"Natural Language to Rego","text":"<p>Status: Emerging, not production-ready LACE framework: 100% correctness in verified policy generation Current best practice: LLM authoring assistants + human review + automated <code>opa test</code></p>"},{"location":"architecture/overview/#cost-model-technical-risks","title":"Cost Model &amp; Technical Risks","text":""},{"location":"architecture/overview/#cost-model-small-team","title":"Cost Model (Small Team)","text":"<p>GraphRAG Indexing: - gpt-4o-mini: ~$0.01 per processing pass (45K-word corpus) - Full re-index: ~$5-50 depending on corpus size - Incremental per-service re-indexing: minimal ongoing costs - Built-in LLM caching reduces redundant API calls</p> <p>Infrastructure: - NATS JetStream: Single binary, near-zero overhead - Self-hosted option: Full control, no per-seat fees</p>"},{"location":"architecture/overview/#technical-risks","title":"Technical Risks","text":"<ol> <li>GraphRAG incremental indexing: Currently requires full re-index</li> <li> <p>Mitigation: Corpus partitioning by service/team, re-index only changed partitions</p> </li> <li> <p>stack-graphs archived status: GitHub repository archived</p> </li> <li> <p>Mitigation: Crates remain functional and forkable</p> </li> <li> <p>OPA-Neo4j sync latency: Not real-time by default</p> </li> <li>Mitigation: OPAL for event-driven sync, &lt;100ms policy evaluation</li> </ol>"},{"location":"architecture/overview/#implementation-priority","title":"Implementation Priority","text":"<p>Phase 1: Ingestion + Governance - Rust ingestion pipeline - OPA policy evaluation - PostgreSQL + Neo4j foundation - Basic CI/CD integration</p> <p>Phase 2: GraphRAG Layer - Python GraphRAG indexing - Qdrant vector search - Community detection - Local/Global search</p> <p>Phase 3: Advanced Features - DRIFT search - Entity resolution - Staleness detection - Human-in-the-loop enrichment</p> <p>Ship with ingestion pipeline and governance policies first, then layer on GraphRAG community detection as the knowledge graph matures.</p>"},{"location":"architecture/technical-spec/","title":"Structural Integrity Platform: comprehensive implementation blueprint","text":"<p>The Structural Integrity Platform can be built as a polyglot, event-driven system using Rust for code ingestion, Python for GraphRAG/ML pipelines, Go for real-time governance, and React for visualization\u2014unified by a multi-modal knowledge base spanning PostgreSQL, Neo4j, and Qdrant. This report synthesizes deep technical research across all seven architecture domains into concrete implementation guidance: data models, library selections with version numbers, Rego policy examples, pipeline architectures, and frontend component recommendations. The platform's core innovation is treating software architecture as a living knowledge graph, then enforcing architectural intent through policy-as-code evaluated against that graph in real time.</p>"},{"location":"architecture/technical-spec/#graphrag-indexing-pipeline-adapted-for-code-and-documentation","title":"GraphRAG indexing pipeline adapted for code and documentation","text":"<p>Microsoft's GraphRAG transforms unstructured text into a hierarchical knowledge graph through seven distinct indexing phases, each configurable for a software architecture corpus.</p> <p>Phase 1 \u2014 Text chunking splits input into TextUnits. The default is 300 tokens with 100-token overlap, but Microsoft reports success with 1,200-token chunks using a single \"glean\" step. For code files, 300\u2013600 tokens work best to preserve function-level boundaries. Configuration lives in <code>settings.yaml</code> under <code>chunks.size</code> and <code>chunks.overlap</code>. Short documents like Slack messages can use many-to-many mapping instead of the default one-to-many.</p> <p>Phase 2 \u2014 Graph extraction sends each TextUnit to an LLM that extracts entities (with title, type, description) and relationships (source, target, description). For the Structural Integrity Platform, replace the default entity types (<code>organization, person, geo, event</code>) with software-specific types:</p> <pre><code>entity_extraction:\n  entity_types: [Service, API, Module, Database, Component, Team, Repository, Package, Function, Class, Interface, Endpoint, Queue, Cache]\n  max_gleanings: 1\n</code></pre> <p>Custom relationship types should include <code>depends_on</code>, <code>calls</code>, <code>imports</code>, <code>owns</code>, <code>maintains</code>, <code>reads_from</code>, <code>writes_to</code>, <code>deploys_to</code>, <code>implements</code>, and <code>exposes</code>. The extraction prompt must be rewritten with domain-specific few-shot examples and explicit relationship definitions. GraphRAG's <code>prompt-tune</code> CLI auto-generates tuned prompts from sample data.</p> <p>Phase 3 \u2014 Graph augmentation applies the Hierarchical Leiden Algorithm (via the <code>graspologic</code> Python library) to detect communities at multiple resolution levels. Level 0 is the finest granularity; higher levels are broader. Each node belongs to exactly one community per level. Phase 4 \u2014 Community summarization generates LLM-written reports for each community, producing executive overviews of component clusters. These reports become the backbone of Global Search. Phase 5 links documents to TextUnits for provenance, Phase 6 optionally generates Node2Vec + UMAP embeddings for visualization, and Phase 7 generates text embeddings for vector search.</p> <p>Adapting for multi-modal sources requires normalizing diverse inputs to text before ingestion. Source code should be pre-processed through tree-sitter to extract AST structure, docstrings, and imports as structured text. Jira tickets export as JSON with title/description/status/labels. Slack messages group by thread. A dual-graph strategy yields the best results: deterministic AST-derived graphs for code structure (achieving 15/15 correctness on architectural queries in research benchmarks) combined with LLM-generated graphs for documentation and tickets.</p>"},{"location":"architecture/technical-spec/#query-strategies-and-when-to-use-each","title":"Query strategies and when to use each","text":"<p>Local Search identifies entities related to a query via vector similarity on entity embeddings, then traverses connected entities, relationships, and community reports within a graph neighborhood. Use for targeted questions like \"What depends on the auth service?\" Fast, cheap, focused on graph locality.</p> <p>Global Search uses a map-reduce pattern across all community reports at a specified hierarchy level. Each report chunk generates importance-rated intermediate responses, which are then aggregated. Use for holistic questions like \"What are the systemic risks in our architecture?\" Expensive but comprehensive.</p> <p>DRIFT Search (Dynamic Reasoning and Inference with Flexible Traversal) combines both approaches in three phases: a primer phase comparing the query against top-K community summaries, a follow-up phase drilling into specific entities via local search, and an output phase combining everything into a ranked hierarchy. Use for questions requiring both breadth and depth, like \"How does the payment service's reliability compare to our overall system patterns?\"</p>"},{"location":"architecture/technical-spec/#integration-with-neo4j-and-qdrant","title":"Integration with Neo4j and Qdrant","text":"<p>Three paths exist for Neo4j integration. The most production-ready is exporting GraphRAG parquet output to Neo4j via Cypher <code>LOAD CSV</code> or the <code>neo4j-graphrag-python</code> package's <code>SimpleKGPipeline</code>. The data model maps entities to <code>:Entity</code> nodes (or typed labels like <code>:Service</code>, <code>:API</code>), relationships to typed edges, communities to <code>:Community</code> nodes, and TextUnits to <code>:TextUnit</code> nodes.</p> <p>For Qdrant, implement a custom vector store via GraphRAG's factory pattern at <code>graphrag/vector_stores/factory.py</code>, registering a <code>QdrantVectorStore</code> class. Three collections are needed: <code>entity_embeddings</code>, <code>text_unit_embeddings</code>, and <code>community_report_embeddings</code>. The hybrid query pattern starts with Qdrant vector search to find seed entities, then expands via Neo4j graph traversal to connected entities and communities, then combines contexts for LLM generation. Production deployments report 20\u201325% accuracy uplift over pure vector solutions and 1\u20132 second query latency.</p> <p>Token optimization uses gpt-4o-mini for extraction (reducing costs to ~$0.01 per processing pass for a 45K-word corpus), built-in LLM caching (identical requests return cached results), and incremental per-service re-indexing rather than full corpus re-indexing. GraphRAG currently requires full re-indexing for updates, so partitioning the corpus by service or team and re-indexing only changed partitions is essential.</p>"},{"location":"architecture/technical-spec/#quality-metrics-and-scoring-for-architectural-governance","title":"Quality metrics and scoring for architectural governance","text":"<p>The platform needs a layered metrics framework combining code-level, architecture-level, documentation, delivery, and knowledge graph quality dimensions into a composite health score.</p>"},{"location":"architecture/technical-spec/#architectural-drift-and-conformance","title":"Architectural drift and conformance","text":"<p>Coupling is measured through afferent coupling (Ca: components depending on you), efferent coupling (Ce: components you depend on), and the Instability Index: <code>I = Ce / (Ca + Ce)</code>, ranging from 0.0 (maximally stable) to 1.0 (maximally unstable). The Distance from Main Sequence formula <code>D = |A + I - 1|</code> (where A = abstractness) identifies components in the \"zone of pain\" (concrete and stable, hard to change) or \"zone of uselessness\" (abstract and unstable).</p> <p>Cohesion uses LCOM4, which counts connected components in a method-attribute graph. LCOM4 = 1 means perfect cohesion; LCOM4 &gt; 1 suggests the class should be split. Relational cohesion <code>H = (R + 1) / N</code> (internal relationships divided by types) should be \u22651.5.</p> <p>Architecture conformance checking uses the Reflexion Model approach: compare an intended architecture model (allowed dependencies) with the actual extracted architecture via a mapping. Dependencies classify as convergence (exists in both), divergence (exists in actual but not intended\u2014a violation), or absence (intended but not actual). The conformance score is:</p> <pre><code>ConformanceScore = convergences / (convergences + divergences + absences)\n</code></pre> <p>The Normalized Cumulative Component Dependency (NCCD) from Lakos metrics compares system CCD against a balanced binary tree. NCCD &gt; 1.0 indicates worse-than-average modularity. Tools like ArchUnit, jQAssistant (which stores code in Neo4j for Cypher-based rule checking), and Sonargraph implement these checks.</p>"},{"location":"architecture/technical-spec/#documentation-freshness-and-technical-debt","title":"Documentation freshness and technical debt","text":"<p>Exponential decay models documentation staleness: <code>Freshness(t) = e^(-\u03bb \u00d7 \u0394t)</code>, where \u03bb varies by document type\u20140.01 for architectural docs (~70-day half-life), 0.05 for API docs (~14-day half-life), 0.1 for deployment guides (~7-day half-life). A composite staleness score combines time decay (25%), code drift (30%), link rot (15%), reference validity (20%), and view frequency (10%). Research shows 28.9% of popular GitHub projects contain at least one outdated code reference in documentation, and references typically remain outdated for years.</p> <p>Technical debt quantification follows the SQALE methodology: sum all remediation costs per violation type. SonarQube's Technical Debt Ratio = <code>Remediation_Cost / (Lines_of_Code \u00d7 30_minutes)</code>. An 'A' rating means \u22645% debt ratio. Architectural debt\u2014improper boundaries, layer violations, missing abstractions\u2014is not captured by code-level tools but accounts for an estimated 80% of all technical debt (Gartner projection for 2027). The platform should quantify architectural debt as <code>\u03a3(violation_type \u00d7 estimated_remediation_cost)</code>.</p> <p>DORA metrics integrate via CI/CD pipeline APIs. GitLab provides a native DORA API (<code>/api/v4/projects/{id}/dora/metrics</code>). GitHub Actions data comes from the Workflow Runs API. Elite performers deploy on-demand (multiple/day) with &lt; 1 hour lead time, 0\u20135% change failure rate, and &lt; 1 hour MTTR. Apache DevLake provides open-source DORA metric collection across multiple CI/CD systems.</p>"},{"location":"architecture/technical-spec/#composite-platform-health-score","title":"Composite platform health score","text":"<p>All dimensions normalize to 0\u20131 and combine with configurable weights:</p> Dimension Suggested weight Primary inputs Architectural conformance 0.25 Reflexion model alignment, layer violations, coupling/cohesion Documentation health 0.15 Coverage \u00d7 Freshness Intent-reality alignment 0.20 Convergence ratio, weighted violations Delivery performance (DORA) 0.15 Normalized four key metrics Technical debt 0.15 Inverted SQALE ratio Knowledge graph quality 0.10 Completeness \u00d7 Freshness \u00d7 Accuracy <p>Final scores map to letter ratings: A (\u22650.9), B (\u22650.75), C (\u22650.6), D (\u22650.4), F (&lt;0.4).</p>"},{"location":"architecture/technical-spec/#oparego-patterns-for-knowledge-graph-governance","title":"OPA/Rego patterns for knowledge graph governance","text":"<p>OPA's <code>graph.reachable</code> built-in function computes transitive closure over a directed graph, handling cycles safely. Its signature is <code>output := graph.reachable(graph, initial)</code> where <code>graph</code> is an adjacency list and <code>initial</code> is a starting set. Complexity is O(V + E) per call, and it terminates safely on cyclic graphs.</p>"},{"location":"architecture/technical-spec/#core-policy-patterns-with-complete-rego-examples","title":"Core policy patterns with complete Rego examples","text":"<p>Circular dependency detection checks whether a component can reach itself through its neighbors:</p> <pre><code>package architecture.circular\nimport rego.v1\n\ncircular_dependency contains cycle if {\n    some component\n    data.architecture.dependencies[component]\n    some neighbor in data.architecture.dependencies[component]\n    reachable := graph.reachable(data.architecture.dependencies, {neighbor})\n    component in reachable\n    cycle := {\"component\": component, \"via\": neighbor,\n      \"msg\": sprintf(\"Circular: '%s' -&gt; '%s' -&gt; ... -&gt; '%s'\", [component, neighbor, component])}\n}\n</code></pre> <p>Layer violation detection assigns numeric levels to layers and flags lower layers depending on higher ones:</p> <pre><code>package architecture.layers\nimport rego.v1\n\nlayer_order := {\"presentation\": 4, \"application\": 3, \"domain\": 2, \"infrastructure\": 1, \"data\": 0}\n\nlayer_violation contains violation if {\n    some source, targets in data.architecture.dependencies\n    some target in targets\n    layer_order[data.architecture.component_layers[source]] &lt; layer_order[data.architecture.component_layers[target]]\n    violation := {\"source\": source, \"target\": target,\n      \"msg\": sprintf(\"Layer violation: '%s' (%s) depends on '%s' (%s)\",\n        [source, data.architecture.component_layers[source], target, data.architecture.component_layers[target]])}\n}\n</code></pre> <p>Additional policies enforce API gateway compliance (external clients must route through the gateway), service boundary enforcement (cross-context access only through public APIs), database access restrictions (no direct DB access from presentation/application layers), dependency version constraints (banned dependencies and minimum versions), and team ownership rules (every component must have an owner; cross-team dependencies require approval).</p>"},{"location":"architecture/technical-spec/#syncing-neo4j-state-to-opa","title":"Syncing Neo4j state to OPA","text":"<p>OPA does not query Neo4j directly. The recommended architecture uses a sync service that extracts graph snapshots via Cypher queries and pushes them to OPA's Data API:</p> <pre><code># Extract from Neo4j, push to OPA\nresult = session.run(\"MATCH (a:Component)-[:DEPENDS_ON]-&gt;(b:Component) RETURN a.name AS source, collect(b.name) AS targets\")\nrequests.put(f\"{opa_url}/v1/data/architecture/dependencies\", json=dependencies)\n</code></pre> <p>Three sync strategies apply: Bundle API for CI/CD-cycle updates (OPA polls a bundle server every 10\u201330 seconds), Push via Data API for dynamic updates after graph changes, and OPAL (Open Policy Administration Layer) for production real-time event-driven synchronization. For latency-sensitive paths, OPA evaluates policies in sub-millisecond for indexed rules and 1\u20135ms for typical policies. Enterprise OPA from Styra uses 10\u00d7 less memory and 40% less CPU than open-source OPA.</p>"},{"location":"architecture/technical-spec/#cicd-integration-as-structural-gatekeeper","title":"CI/CD integration as structural gatekeeper","text":"<p>GitHub Actions integration uses the <code>open-policy-agent/setup-opa@v2</code> action to install OPA, extract architecture state from Neo4j, run policy tests via <code>opa test</code>, and evaluate policies with <code>--fail-defined</code> to block merges on violations. Conftest wraps OPA for testing configuration files (Kubernetes manifests, Terraform, Dockerfiles) against policies. Pre-commit hooks catch violations locally before push. OPA Gatekeeper acts as a Kubernetes admission controller for deployment-time enforcement.</p> <p>Natural language to Rego translation is not yet production-ready but emerging. The LACE framework achieved 100% correctness in verified policy generation using LLMs. For now, LLMs serve best as authoring assistants with mandatory human review and automated <code>opa test</code> validation of generated policies.</p>"},{"location":"architecture/technical-spec/#polyglot-backend-architecture-with-event-driven-synchronization","title":"Polyglot backend architecture with event-driven synchronization","text":""},{"location":"architecture/technical-spec/#rust-ingestion-with-tree-sitter-and-stack-graphs","title":"Rust ingestion with tree-sitter and stack-graphs","text":"<p>tree-sitter (crate <code>tree-sitter</code> v0.24.x) is an incremental parsing framework that generates concrete syntax trees from grammar definitions. It parses a 2,157-line Rust file in ~6.5ms and performs incremental re-parses in sub-millisecond time after edits. Over 150 language grammars are available. From Rust, use tree-sitter queries (S-expression patterns) to extract functions, classes, imports, and dependencies:</p> <pre><code>let mut parser = Parser::new();\nparser.set_language(&amp;tree_sitter_rust::LANGUAGE.into()).unwrap();\nlet tree = parser.parse(source_code, None).unwrap();\n// Use TreeCursor to walk AST or queries to match patterns like:\n// (function_item name: (identifier) @fn_name)\n</code></pre> <p>stack-graphs (GitHub's framework for code navigation) build on tree-sitter to resolve references to definitions across files without requiring a full build. While the GitHub repository has been archived, the crates remain functional. Stack-graphs produce Push Symbol Nodes (references) and Pop Symbol Nodes (definitions), enabling construction of call graphs, import dependency graphs, and type hierarchies.</p> <p>The ingestion pipeline uses <code>notify</code> (v6.x) for filesystem watching, <code>rayon</code> for CPU-bound parallel parsing, and <code>tokio</code> for async I/O orchestration. Bounded channels (<code>crossbeam-channel</code>) provide backpressure between stages. The pipeline flows: File Watcher \u2192 Change Queue \u2192 Parser Pool (tree-sitter + stack-graphs) \u2192 Entity Emitter (graph triples) \u2192 Output Sink (NATS/Kafka).</p>"},{"location":"architecture/technical-spec/#python-ml-and-graphrag-services","title":"Python ML and GraphRAG services","text":"<p>FastAPI (v0.115.x) serves as the HTTP/REST API layer, with Celery (v5.4.x) and Redis for async task processing of long-running ML operations. LLM inference goes through vLLM (v0.6.x) with PagedAttention for high-throughput serving, exposed as an OpenAI-compatible endpoint. Pipeline orchestration uses Prefect (v3.x) for workflow management with built-in retry, caching, and observability. Error handling for LLM calls uses <code>tenacity</code> for exponential backoff with jitter and <code>pybreaker</code> for circuit breaking.</p> <p>Incremental indexing tracks file hashes and timestamps in PostgreSQL. On change detection, only affected TextUnits are re-extracted. LLM extraction results are cached keyed by <code>(chunk_hash, model_version, prompt_version)</code> to avoid redundant API calls.</p>"},{"location":"architecture/technical-spec/#go-governance-microservices","title":"Go governance microservices","text":"<p>Go services embed OPA as a library via <code>github.com/open-policy-agent/opa/v1/sdk</code>, achieving sub-millisecond policy evaluation with zero serialization overhead. Prepared queries are goroutine-safe and cache evaluation plans. The service uses <code>go-chi/chi</code> for HTTP routing, <code>nats.go</code> for event consumption, <code>sony/gobreaker</code> for circuit breaking, and <code>golang.org/x/time/rate</code> for rate limiting. Go's goroutine model maps naturally to concurrent webhook processing.</p>"},{"location":"architecture/technical-spec/#event-driven-synchronization-and-cqrs","title":"Event-driven synchronization and CQRS","text":"<p>NATS with JetStream is recommended for initial deployment\u2014a single ~10MB binary supporting pub/sub, request-reply, and streaming with sub-millisecond latency and ~200K msg/s persistent throughput. Migrate to Apache Kafka when throughput exceeds 200K msg/s sustained or when the Kafka Connect ecosystem (especially Debezium) is needed.</p> <p>The platform follows CQRS: PostgreSQL is the write model and source of truth; Neo4j and Qdrant are read-optimized projections built from events. The Outbox Pattern ensures atomicity\u2014write to a PostgreSQL <code>event_outbox</code> table in the same transaction as entity updates, then a dispatcher publishes to the event bus.</p> <p>Debezium captures PostgreSQL WAL changes via logical replication and streams them to Kafka topics. Custom consumers transform CDC events into Neo4j Cypher MERGE statements and Qdrant vector upserts (after generating embeddings via the Python service). Key PostgreSQL configuration: <code>wal_level = logical</code>, <code>max_replication_slots = 4</code>, and <code>REPLICA IDENTITY FULL</code> on tracked tables.</p>"},{"location":"architecture/technical-spec/#entity-resolution-across-data-sources","title":"Entity resolution across data sources","text":"<p>The same developer appears as a GitHub username, Jira account ID, Slack user ID, and Confluence author. A three-layer resolution strategy handles this: deterministic matching (email/SSO ID/username mapping), probabilistic matching via the Fellegi-Sunter model (computing match weights per attribute), and ML-based matching with active learning. Splink (v4.x, Python) implements Fellegi-Sunter with EM parameter estimation and multiple SQL backends. A canonical entity model stores all identifiers with resolution confidence scores and supports entity evolution over time.</p>"},{"location":"architecture/technical-spec/#data-connector-architecture-and-knowledge-freshness","title":"Data connector architecture and knowledge freshness","text":""},{"location":"architecture/technical-spec/#connector-sdk-design-following-airbyte-patterns","title":"Connector SDK design following Airbyte patterns","text":"<p>The connector framework should adopt a three-tier development approach inspired by Airbyte: no-code YAML configuration for common REST APIs, low-code YAML with custom functions for moderate complexity, and a full Python SDK for complex sources. Each connector runs in its own Docker container for isolation, communicates via a standard protocol (SPEC \u2192 CHECK \u2192 DISCOVER \u2192 READ), and is versioned independently.</p> <p>Key design principles from production connector platforms:</p> <ul> <li>Backstage-style extension points for plugin extensibility, where modules register capabilities via typed interfaces</li> <li>Incremental sync via cursor fields and state checkpoints (Airbyte's <code>AirbyteStateMessage</code> pattern)</li> <li>Credential management through an external secrets store (Vault, AWS Secrets Manager)\u2014never stored in connector code</li> <li>Rate limit budgets allocated across connectors using token bucket algorithms</li> </ul>"},{"location":"architecture/technical-spec/#source-specific-ingestion-patterns","title":"Source-specific ingestion patterns","text":"<p>GitHub ingestion should use GitHub Apps (5,000\u201315,000 req/hr based on org size) over OAuth Apps. Webhooks handle real-time updates for push, PR, and issue events; conditional requests with ETag headers avoid counting 304 responses against rate limits. For monorepos, the Git Trees API (<code>/git/trees/{sha}?recursive=1</code>) enables efficient traversal.</p> <p>Jira uses REST API v3 with the new <code>/rest/api/3/search/jql</code> endpoint. Pagination has shifted to sequential <code>nextPageToken</code>-based (no parallel page fetches), with <code>maxResults</code> up to 5,000 for smaller datasets. Extract issues, epics, sprints, components, and issue links\u2014mapping components to services/modules in the knowledge graph.</p> <p>Confluence uses CQL for searching (<code>type=page AND space=ARCH AND lastModified &gt;= \"2025-01-01\"</code>). Body content returns in Confluence Storage Format (XHTML with <code>ac:</code> macros) requiring tag stripping for clean text. With body expansion, pagination limits to 50 results per page.</p> <p>Slack uses Socket Mode (no public endpoint required, works behind firewalls) via the Bolt framework. Subscribe to <code>message</code>, <code>reaction_added</code>, and <code>file_shared</code> events. Filter by channel type for privacy, and consider ingesting only from designated \"knowledge\" channels.</p> <p>The recommended hybrid strategy: webhooks for real-time event capture, periodic reconciliation polling (every 15\u201360 minutes) to catch missed events, and full polling-based backfill on initial setup.</p>"},{"location":"architecture/technical-spec/#automated-staleness-detection-and-knowledge-enrichment","title":"Automated staleness detection and knowledge enrichment","text":"<p>A composite staleness score combines five signals with weighted contributions: time-based exponential decay (25%), code drift where related code changed but docs didn't (30%), link rot from broken URLs (15%), reference validity checking whether referenced APIs/classes still exist (20%), and inverse view frequency (10%). Research confirms that 82.3% of popular GitHub projects have contained outdated code references at some point.</p> <p>Smart notifications route through severity tiers: dashboard flag at score &gt; 0.5, Slack message to the document owner at day 3, team channel escalation at day 7, engineering manager report at day 14, and deprecation/archival suggestion at day 30. Notification fatigue prevention uses batching into daily/weekly digests, a cap of 3 push notifications per hour per user, priority tiering, and user-configurable preferences.</p> <p>Embedding-based duplicate detection uses cosine similarity on document embeddings (via <code>all-MiniLM-L6-v2</code> for self-hosted or <code>text-embedding-3-small</code> for OpenAI). Thresholds: &gt; 0.95 triggers auto-flagging for merge, 0.85\u20130.95 suggests consolidation for human review, 0.70\u20130.85 suggests cross-linking. HDBSCAN clustering on embeddings identifies topic clusters for consolidation suggestions. For scale beyond 100K documents, use FAISS or Qdrant for approximate nearest neighbor search.</p> <p>Human-in-the-loop enrichment uses confidence thresholds: &gt; 0.9 auto-adds graph relationships, 0.7\u20130.9 queues for review, &lt; 0.7 discards. PR-triggered prompts ask authors to review related docs when code changes touch documented components. Acceptance rates feed back into extraction model tuning.</p>"},{"location":"architecture/technical-spec/#frontend-dashboard-with-graph-visualization-and-governance-ux","title":"Frontend dashboard with graph visualization and governance UX","text":""},{"location":"architecture/technical-spec/#graph-visualization-sigmajs-for-exploration-cytoscapejs-for-architecture","title":"Graph visualization: Sigma.js for exploration, Cytoscape.js for architecture","text":"<p>Sigma.js (v3.0.2) with <code>@react-sigma/core</code> is recommended as the primary graph visualization library. It uses WebGL rendering and handles 100K+ edges at 60fps with default styles. Built on <code>graphology</code> for data manipulation and algorithms. ForceAtlas2 layout computation offloads to a Web Worker via <code>graphology-layout-forceatlas2/worker</code>. Custom node renderers display service health indicators (traffic light coloring), and <code>reducers</code> enable dynamic appearance changes for filtering.</p> <p>Cytoscape.js (v3.x) with <code>react-cytoscapejs</code> serves architecture diagram views. It has the richest layout ecosystem: Dagre for directed acyclic dependency flows, CoSE-Bilkent for compound graphs with nested domain groups, hierarchical for org charts, and breadthfirst for tree-like structures. Compound nodes group services by team or bounded context. Use CSS-like selectors for styling edges by health status (red = error, green = healthy) and thickness by traffic volume.</p> <p>Performance benchmarks show SVG falls below 30fps at 10K elements during pan/zoom, Canvas hits limits at ~10K elements with complex interactions, and WebGL holds steady past 100K elements. The recommended hybrid approach: WebGL (Sigma.js) at overview zoom levels, Canvas for mid-range with labels, and SVG export for high-quality architecture diagrams.</p> <p>Diff visualization overlays two graph states with color-coded additions (green), removals (red), and modifications (yellow). Sigma.js supports this via dynamic style updates through reducers.</p>"},{"location":"architecture/technical-spec/#dashboard-design-for-engineering-leadership","title":"Dashboard design for engineering leadership","text":"<p>The dashboard follows a card-based KPI layout with drill-down navigation: top row shows total services, compliance percentage, active drift alerts, and deployment frequency. Below, time-series trend charts (via Recharts v2.x, chosen for its 9M+ weekly npm downloads and composable React API) track metrics over sprints. Nivo supplements with heatmaps for dependency risk matrices and radar charts for team comparisons.</p> <p>Key dashboard views include an architecture health overview with traffic-light scoring, drift detection alerts with trend lines, team-based ownership matrices, a service catalog with health indicators (inspired by Backstage's entity model), and a dependency risk heatmap. Real-time updates use TanStack Query (v5.x) with <code>refetchInterval</code> for polling and WebSocket integration via Zustand for live drift detection alerts.</p>"},{"location":"architecture/technical-spec/#settings-ui-and-llm-configuration","title":"Settings UI and LLM configuration","text":"<p>LLM configuration presents a provider selector (OpenAI, Anthropic, Azure OpenAI, Ollama, custom endpoint) with API key management using masked input with show/hide toggle, model selection dropdown, and parameter sliders for temperature and max tokens. A \"Test Connection\" button validates through the backend proxy and displays latency and model availability. API keys must be stored server-side only\u2014never in frontend environment variables, which embed into build output.</p> <p>Data source connector configuration uses OAuth 2.0 popup flows for GitHub/Jira/Confluence, with connection status indicators (Connected/Disconnected/Error/Syncing), sync scheduling dropdowns, and repository selection interfaces.</p>"},{"location":"architecture/technical-spec/#reactvite-technical-stack","title":"React/Vite technical stack","text":"Layer Choice Rationale Build Vite 6.x Native ESM, fast HMR UI framework React 19 Hooks, Suspense, transitions Components shadcn/ui + Radix UI + Tailwind CSS 4.x Accessible, composable, dark mode built-in; used by OpenAI and Adobe State (client) Zustand 5.x ~3KB, no Provider needed, middleware for persistence and devtools State (server) TanStack Query 5.x Caching, background refetching, optimistic mutations Routing TanStack Router File-based, type-safe, automatic code splitting Validation Zod 3.x TypeScript-first runtime validation Graph (exploration) Sigma.js 3.0.2 + <code>@react-sigma/core</code> WebGL performance for 1000+ node graphs Graph (architecture) Cytoscape.js 3.x + <code>react-cytoscapejs</code> Rich layout algorithms (Dagre, hierarchical) Charts Recharts 2.x + Nivo for specialized views Largest community, composable React API <p>The project uses feature-based file organization: <code>routes/</code> for TanStack Router pages, <code>components/</code> split by domain (graph/, dashboard/, settings/, layout/), <code>stores/</code> for Zustand stores, <code>hooks/queries/</code> and <code>hooks/mutations/</code> for TanStack Query hooks, and <code>lib/</code> for utilities. Performance optimization includes lazy loading for heavy graph components, <code>@tanstack/react-virtual</code> for long service lists, and Web Workers for ForceAtlas2 layout computation.</p>"},{"location":"architecture/technical-spec/#deployment-architecture-and-scaling-path","title":"Deployment architecture and scaling path","text":"<p>The platform targets Docker and Kubernetes with a clear scaling path:</p> <ul> <li>Small team (5\u201320 devs): Docker Compose with all services; NATS JetStream for events; single PostgreSQL, Neo4j, and Qdrant instances; gpt-4o-mini for GraphRAG indexing to control costs</li> <li>Growth (20\u2013100 devs): Kubernetes with Helm charts; migrate to Kafka for CDC integration with Debezium; Neo4j clustering; Qdrant sharding; dedicated GPU node for vLLM</li> <li>Enterprise (100+ devs): Multi-cluster Kubernetes; Kafka with multiple topic partitions; Neo4j Fabric for federation; Qdrant distributed mode; Enterprise OPA (Styra DAS) for policy lifecycle management; OPAL for real-time policy data synchronization</li> </ul> <p>The Outbox Pattern with Debezium CDC provides the atomic consistency guarantee between PostgreSQL writes and event publication across all scale levels. Event schema versioning with backward compatibility (Protobuf for inter-service communication) ensures smooth upgrades.</p>"},{"location":"architecture/technical-spec/#conclusion","title":"Conclusion","text":"<p>Three architectural decisions define this platform's success. First, the dual-graph strategy\u2014deterministic AST-derived graphs for code structure combined with LLM-generated graphs for documentation and tickets\u2014delivers higher accuracy than either approach alone. Second, CQRS with PostgreSQL as the write-side source of truth and Neo4j/Qdrant as read-optimized projections eliminates the consistency challenges of multi-model databases. Third, OPA's <code>graph.reachable</code> function enables expressing complex architectural policies (circular dependencies, transitive layer violations, cross-boundary access) in 10\u201320 lines of Rego, evaluated in sub-millisecond time.</p> <p>The cost model is favorable for small teams: GraphRAG indexing with gpt-4o-mini costs ~$5\u201350 per full re-index depending on corpus size, and incremental per-service re-indexing keeps ongoing costs minimal. NATS JetStream adds near-zero operational overhead as a single binary. The primary technical risks are GraphRAG's current lack of true incremental indexing (mitigated by corpus partitioning) and stack-graphs' archived status (mitigated by the codebase remaining functional and forkable). The platform should ship with the ingestion pipeline and governance policies first, then layer on GraphRAG community detection and DRIFT search as the knowledge graph matures.</p>"},{"location":"deployment/docker-cheatsheet/","title":"\ud83d\udc33 Docker Quick Reference","text":""},{"location":"deployment/docker-cheatsheet/#quick-start-commands","title":"Quick Start Commands","text":"<pre><code># HTTP Mode (Development)\ndocker compose --profile http up -d\n\n# HTTPS Mode (Production)\ndocker compose --profile https up -d\n\n# With Monitoring\ndocker compose --profile http --profile monitoring up -d\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#essential-commands","title":"Essential Commands","text":"Command Description <code>docker build -t substrate-platform:latest .</code> Build the Docker image <code>docker compose --profile http up -d</code> Start in HTTP mode <code>docker compose --profile https up -d</code> Start in HTTPS mode <code>docker compose --profile http --profile monitoring up -d</code> Start with Prometheus + Grafana <code>docker compose down</code> Stop all containers <code>docker compose logs -f</code> View logs <code>curl http://localhost:9113/metrics</code> Show nginx metrics <code>curl http://localhost:8080/health</code> Test HTTP endpoint"},{"location":"deployment/docker-cheatsheet/#access-urls","title":"Access URLs","text":""},{"location":"deployment/docker-cheatsheet/#http-mode","title":"HTTP Mode","text":"<ul> <li>Application: http://localhost:8080</li> <li>Health Check: http://localhost:8080/health</li> <li>Metrics: http://localhost:9113/metrics</li> </ul>"},{"location":"deployment/docker-cheatsheet/#https-mode","title":"HTTPS Mode","text":"<ul> <li>Application: https://localhost</li> <li>Health Check: https://localhost/health</li> <li>Metrics: http://localhost:9113/metrics</li> </ul>"},{"location":"deployment/docker-cheatsheet/#monitoring-stack","title":"Monitoring Stack","text":"<ul> <li>Application: http://localhost:8080</li> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Nginx Exporter: http://localhost:9114/metrics</li> </ul>"},{"location":"deployment/docker-cheatsheet/#environment-variables","title":"Environment Variables","text":""},{"location":"deployment/docker-cheatsheet/#core-settings","title":"Core Settings","text":"<pre><code>ENABLE_HTTPS=false          # Enable HTTPS (true/false)\nENABLE_LOGGING=true         # Enable logging (true/false)\nENABLE_METRICS=true         # Enable metrics (true/false)\nLOG_LEVEL=warn             # Log level\nSERVER_NAME=localhost       # Domain name\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#quick-configurations","title":"Quick Configurations","text":"<p>Development <pre><code>ENABLE_HTTPS=false\nENABLE_LOGGING=true\nENABLE_METRICS=true\nLOG_LEVEL=info\n</code></pre></p> <p>Production <pre><code>ENABLE_HTTPS=true\nENABLE_LOGGING=true\nENABLE_METRICS=true\nLOG_LEVEL=warn\nSERVER_NAME=yourdomain.com\n</code></pre></p> <p>Minimal <pre><code>ENABLE_HTTPS=false\nENABLE_LOGGING=false\nENABLE_METRICS=false\n</code></pre></p>"},{"location":"deployment/docker-cheatsheet/#common-tasks","title":"Common Tasks","text":""},{"location":"deployment/docker-cheatsheet/#view-logs","title":"View Logs","text":"<pre><code># All logs\ndocker compose logs -f\n\n# Specific container logs\ndocker logs -f substrate-platform-http\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#check-status","title":"Check Status","text":"<pre><code># Container status\ndocker compose ps\n\n# Health check\ncurl http://localhost:8080/health\n\n# Nginx configuration test\ndocker exec substrate-platform-http nginx -t\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#ssl-setup","title":"SSL Setup","text":"<pre><code># Generate self-signed certificate\nmkdir -p ssl\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n    -keyout ssl/key.pem \\\n    -out ssl/cert.pem \\\n    -subj \"/C=US/ST=State/L=City/O=Organization/CN=localhost\"\n\n# Or place your certificates\nmkdir -p ssl\ncp your-cert.pem ssl/cert.pem\ncp your-key.pem ssl/key.pem\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#debugging","title":"Debugging","text":"<pre><code># Open shell\ndocker exec -it substrate-platform-http /bin/sh\n\n# Check nginx config\ndocker exec substrate-platform-http nginx -t\n\n# View real-time logs\ndocker logs -f substrate-platform-http\n\n# Check metrics\ncurl http://localhost:9113/metrics\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#troubleshooting","title":"Troubleshooting","text":"Issue Solution Port already in use Change ports in docker-compose.yml SSL certificate error Generate certs manually or provide valid certs Container won't start Check logs with <code>docker logs substrate-platform-http</code> 502 Bad Gateway Check if build completed: <code>docker compose logs</code> Can't access metrics Ensure <code>ENABLE_METRICS=true</code>"},{"location":"deployment/docker-cheatsheet/#file-structure","title":"File Structure","text":"<pre><code>substrate-platform/\n\u251c\u2500\u2500 Dockerfile                          # Multi-stage build configuration\n\u251c\u2500\u2500 docker-compose.yml                  # Orchestration configuration\n\u251c\u2500\u2500 DOCKER.md                          # Full documentation\n\u251c\u2500\u2500 .dockerignore                      # Build optimization\n\u251c\u2500\u2500 .env.docker.example                # Environment template\n\u2514\u2500\u2500 docker/\n    \u251c\u2500\u2500 entrypoint.sh                  # Dynamic configuration script\n    \u251c\u2500\u2500 nginx/\n    \u2502   \u251c\u2500\u2500 nginx.conf.template       # Main nginx config\n    \u2502   \u251c\u2500\u2500 default.conf.template     # HTTP config\n    \u2502   \u251c\u2500\u2500 ssl.conf.template         # HTTPS config\n    \u2502   \u2514\u2500\u2500 metrics.conf.template     # Metrics config\n    \u251c\u2500\u2500 prometheus/\n    \u2502   \u2514\u2500\u2500 prometheus.yml            # Prometheus config\n    \u2514\u2500\u2500 grafana/\n        \u251c\u2500\u2500 datasources/\n        \u2502   \u2514\u2500\u2500 prometheus.yml        # Grafana datasource\n        \u2514\u2500\u2500 dashboards/\n            \u2514\u2500\u2500 dashboard.yml         # Dashboard provider\n</code></pre>"},{"location":"deployment/docker-cheatsheet/#next-steps","title":"Next Steps","text":"<ol> <li>Development: <code>docker compose --profile http up -d</code></li> <li>Production: Generate/provide SSL certs, then <code>docker compose --profile https up -d</code></li> <li>Monitoring: <code>docker compose --profile http --profile monitoring up -d</code></li> <li>Customize: Edit templates in <code>docker/nginx/</code></li> <li>Read More: See <code>DOCKER.md</code> for detailed documentation</li> </ol>"},{"location":"deployment/docker/","title":"Docker Deployment Guide","text":"<p>This guide explains how to containerize and run the Substrate Platform using Docker with configurable Nginx settings.</p>"},{"location":"deployment/docker/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Features</li> <li>Quick Start</li> <li>Configuration</li> <li>Deployment Modes</li> <li>SSL/HTTPS Setup</li> <li>Monitoring</li> <li>Environment Variables</li> <li>Building and Running</li> </ul>"},{"location":"deployment/docker/#features","title":"\u2728 Features","text":"<ul> <li>\ud83d\udd12 Flexible HTTPS/HTTP: Switch between HTTPS and HTTP modes using environment variables</li> <li>\ud83d\udcca Built-in Metrics: Nginx metrics endpoint for Prometheus monitoring</li> <li>\ud83d\udcdd Configurable Logging: Enable/disable access and error logs</li> <li>\ud83d\ude80 Multi-stage Build: Optimized Docker image with build and production stages</li> <li>\ud83d\udd10 Security Headers: Modern security headers included by default</li> <li>\u26a1 Performance: Gzip compression, caching, and optimized Nginx settings</li> <li>\ud83c\udfe5 Health Checks: Built-in health check endpoints</li> </ul>"},{"location":"deployment/docker/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"deployment/docker/#http-mode-default","title":"HTTP Mode (Default)","text":"<pre><code># Build and run with HTTP\ndocker compose --profile http up -d\n\n# Access the application\nopen http://localhost:8080\n</code></pre>"},{"location":"deployment/docker/#https-mode","title":"HTTPS Mode","text":"<pre><code># Build and run with HTTPS\ndocker compose --profile https up -d\n\n# Access the application\nopen https://localhost\n</code></pre>"},{"location":"deployment/docker/#minimal-mode-no-loggingmetrics","title":"Minimal Mode (No Logging/Metrics)","text":"<pre><code># Build and run minimal setup\ndocker compose --profile minimal up -d\n</code></pre>"},{"location":"deployment/docker/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"deployment/docker/#environment-variables","title":"Environment Variables","text":"<p>The application can be configured using the following environment variables:</p> Variable Default Description <code>ENABLE_HTTPS</code> <code>false</code> Enable HTTPS mode and HTTP to HTTPS redirect <code>ENABLE_LOGGING</code> <code>true</code> Enable nginx logging <code>ENABLE_METRICS</code> <code>true</code> Enable metrics endpoint <code>ENABLE_ACCESS_LOG</code> <code>true</code> Enable access log (requires <code>ENABLE_LOGGING=true</code>) <code>ENABLE_ERROR_LOG</code> <code>true</code> Enable error log (requires <code>ENABLE_LOGGING=true</code>) <code>LOG_LEVEL</code> <code>warn</code> Log level: <code>debug</code>, <code>info</code>, <code>notice</code>, <code>warn</code>, <code>error</code>, <code>crit</code> <code>SERVER_NAME</code> <code>localhost</code> Server name for nginx <code>HTTP_PORT</code> <code>80</code> HTTP port inside container <code>HTTPS_PORT</code> <code>443</code> HTTPS port inside container <code>METRICS_PORT</code> <code>9113</code> Metrics endpoint port <code>SSL_CERTIFICATE_PATH</code> <code>/etc/nginx/ssl/cert.pem</code> Path to SSL certificate <code>SSL_CERTIFICATE_KEY_PATH</code> <code>/etc/nginx/ssl/key.pem</code> Path to SSL private key"},{"location":"deployment/docker/#deployment-modes","title":"\ud83c\udfaf Deployment Modes","text":""},{"location":"deployment/docker/#1-development-http","title":"1. Development (HTTP)","text":"<p>Perfect for local development without SSL complexity:</p> <pre><code>docker run -d \\\n  -p 8080:80 \\\n  -p 9113:9113 \\\n  -e ENABLE_HTTPS=false \\\n  -e ENABLE_LOGGING=true \\\n  -e ENABLE_METRICS=true \\\n  -e LOG_LEVEL=info \\\n  --name substrate-platform \\\n  substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#2-production-https","title":"2. Production (HTTPS)","text":"<p>For production with SSL certificates:</p> <pre><code>docker run -d \\\n  -p 80:80 \\\n  -p 443:443 \\\n  -p 9113:9113 \\\n  -e ENABLE_HTTPS=true \\\n  -e ENABLE_LOGGING=true \\\n  -e ENABLE_METRICS=true \\\n  -e LOG_LEVEL=warn \\\n  -e SERVER_NAME=yourdomain.com \\\n  -v $(pwd)/ssl:/etc/nginx/ssl:ro \\\n  -v $(pwd)/logs:/var/log/nginx \\\n  --name substrate-platform \\\n  substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#3-minimal-no-loggingmetrics","title":"3. Minimal (No Logging/Metrics)","text":"<p>Lightweight setup for resource-constrained environments:</p> <pre><code>docker run -d \\\n  -p 8080:80 \\\n  -e ENABLE_HTTPS=false \\\n  -e ENABLE_LOGGING=false \\\n  -e ENABLE_METRICS=false \\\n  --name substrate-platform \\\n  substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#sslhttps-setup","title":"\ud83d\udd10 SSL/HTTPS Setup","text":""},{"location":"deployment/docker/#option-1-self-signed-certificate-auto-generated","title":"Option 1: Self-Signed Certificate (Auto-generated)","text":"<p>If no SSL certificates are provided, the entrypoint script will automatically generate a self-signed certificate:</p> <pre><code>docker compose --profile https up -d\n</code></pre>"},{"location":"deployment/docker/#option-2-provide-your-own-certificates","title":"Option 2: Provide Your Own Certificates","text":"<ol> <li> <p>Create an <code>ssl</code> directory: <pre><code>mkdir -p ssl\n</code></pre></p> </li> <li> <p>Place your certificates: <pre><code>cp your-certificate.crt ssl/cert.pem\ncp your-private-key.key ssl/key.pem\n</code></pre></p> </li> <li> <p>Run with certificate volume: <pre><code>docker compose --profile https up -d\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#option-3-lets-encrypt-with-certbot","title":"Option 3: Let's Encrypt with Certbot","text":"<ol> <li> <p>Generate certificates using Certbot: <pre><code>certbot certonly --standalone -d yourdomain.com\n</code></pre></p> </li> <li> <p>Mount the Let's Encrypt directory: <pre><code>docker run -d \\\n  -p 80:80 \\\n  -p 443:443 \\\n  -e ENABLE_HTTPS=true \\\n  -e SERVER_NAME=yourdomain.com \\\n  -e SSL_CERTIFICATE_PATH=/etc/letsencrypt/live/yourdomain.com/fullchain.pem \\\n  -e SSL_CERTIFICATE_KEY_PATH=/etc/letsencrypt/live/yourdomain.com/privkey.pem \\\n  -v /etc/letsencrypt:/etc/letsencrypt:ro \\\n  substrate-platform:latest\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#monitoring","title":"\ud83d\udcca Monitoring","text":""},{"location":"deployment/docker/#accessing-metrics","title":"Accessing Metrics","text":"<p>With <code>ENABLE_METRICS=true</code>, the following endpoints are available:</p> <ul> <li>Nginx stub_status: <code>http://localhost:9113/metrics</code></li> <li>Stats endpoint: <code>http://localhost:9113/stats</code></li> <li>Extended status: <code>http://localhost:9113/nginx_status</code></li> </ul>"},{"location":"deployment/docker/#prometheus-integration","title":"Prometheus Integration","text":"<p>Start the full monitoring stack:</p> <pre><code>docker compose --profile http --profile monitoring up -d\n</code></pre> <p>This starts: - Substrate Platform on port 8080 - Prometheus on port 9090 - Nginx Exporter on port 9114 - Grafana on port 3000</p> <p>Access Grafana at <code>http://localhost:3000</code> (default credentials: <code>admin/admin</code>)</p>"},{"location":"deployment/docker/#viewing-logs","title":"Viewing Logs","text":"<p>With <code>ENABLE_LOGGING=true</code>, logs are stored in <code>/var/log/nginx</code>:</p> <pre><code># View access logs\ndocker exec substrate-platform-http tail -f /var/log/nginx/access.log\n\n# View error logs\ndocker exec substrate-platform-http tail -f /var/log/nginx/error.log\n\n# Or mount logs volume for host access\ndocker compose --profile http up -d\ntail -f logs/access.log\n</code></pre>"},{"location":"deployment/docker/#building-and-running","title":"\ud83c\udfd7\ufe0f Building and Running","text":""},{"location":"deployment/docker/#build-the-docker-image","title":"Build the Docker Image","text":"<pre><code>docker build -t substrate-platform:latest .\n</code></pre>"},{"location":"deployment/docker/#run-with-docker","title":"Run with Docker","text":"<pre><code># HTTP mode\ndocker run -d -p 8080:80 substrate-platform:latest\n\n# HTTPS mode with custom settings\ndocker run -d \\\n  -p 80:80 \\\n  -p 443:443 \\\n  -e ENABLE_HTTPS=true \\\n  -e LOG_LEVEL=info \\\n  substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#run-with-docker-compose","title":"Run with Docker Compose","text":"<pre><code># HTTP mode\ndocker compose --profile http up -d\n\n# HTTPS mode\ndocker compose --profile https up -d\n\n# With monitoring\ndocker compose --profile http --profile monitoring up -d\n\n# Stop services\ndocker compose --profile http down\n</code></pre>"},{"location":"deployment/docker/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"deployment/docker/#custom-nginx-configuration","title":"Custom Nginx Configuration","text":"<p>If you need to customize nginx configuration beyond environment variables:</p> <ol> <li>Create custom templates in <code>docker/nginx/</code></li> <li>Mount them at runtime: <pre><code>docker run -d \\\n  -v $(pwd)/custom-nginx.conf:/etc/nginx/templates/nginx.conf.template \\\n  substrate-platform:latest\n</code></pre></li> </ol>"},{"location":"deployment/docker/#health-checks","title":"Health Checks","text":"<p>The application includes a built-in health check endpoint:</p> <pre><code>curl http://localhost:8080/health\n# Response: healthy\n</code></pre> <p>Docker health check is configured automatically: <pre><code>docker ps\n# STATUS should show \"healthy\" after a few seconds\n</code></pre></p>"},{"location":"deployment/docker/#examples","title":"\ud83d\udcdd Examples","text":""},{"location":"deployment/docker/#complete-production-setup","title":"Complete Production Setup","text":"<pre><code># Create necessary directories\nmkdir -p ssl logs\n\n# Place your SSL certificates\ncp /path/to/cert.pem ssl/\ncp /path/to/key.pem ssl/\n\n# Create custom environment file\ncat &gt; .env.production &lt;&lt; EOF\nENABLE_HTTPS=true\nENABLE_LOGGING=true\nENABLE_METRICS=true\nLOG_LEVEL=warn\nSERVER_NAME=myapp.example.com\nEOF\n\n# Run with production settings\ndocker run -d \\\n  -p 80:80 \\\n  -p 443:443 \\\n  -p 9113:9113 \\\n  --env-file .env.production \\\n  -v $(pwd)/ssl:/etc/nginx/ssl:ro \\\n  -v $(pwd)/logs:/var/log/nginx \\\n  --restart unless-stopped \\\n  --name substrate-platform-prod \\\n  substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#development-with-hot-reload-source-mounted","title":"Development with Hot Reload (Source Mounted)","text":"<p>For development with source code changes:</p> <pre><code># Build development image\ndocker build --target builder -t substrate-platform:dev .\n\n# Run with source mounted\ndocker run -d \\\n  -p 8080:80 \\\n  -v $(pwd)/src:/app/src \\\n  -e ENABLE_HTTPS=false \\\n  -e LOG_LEVEL=debug \\\n  substrate-platform:dev\n</code></pre>"},{"location":"deployment/docker/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"deployment/docker/#check-container-logs","title":"Check Container Logs","text":"<pre><code>docker logs substrate-platform-http\n</code></pre>"},{"location":"deployment/docker/#test-nginx-configuration","title":"Test Nginx Configuration","text":"<pre><code>docker exec substrate-platform-http nginx -t\n</code></pre>"},{"location":"deployment/docker/#restart-container","title":"Restart Container","text":"<pre><code>docker restart substrate-platform-http\n</code></pre>"},{"location":"deployment/docker/#check-metrics","title":"Check Metrics","text":"<pre><code>curl http://localhost:9113/metrics\n</code></pre>"},{"location":"deployment/docker/#verify-ssl-certificate","title":"Verify SSL Certificate","text":"<pre><code>docker exec substrate-platform-https openssl x509 -in /etc/nginx/ssl/cert.pem -text -noout\n</code></pre>"},{"location":"deployment/docker/#image-optimization","title":"\ud83d\udce6 Image Optimization","text":"<p>The Docker image uses multi-stage builds for optimal size:</p> <ul> <li>Builder stage: Compiles and builds the application</li> <li>Production stage: Only includes nginx and built assets</li> <li>Final size: ~50-60MB (nginx:alpine + app)</li> </ul> <p>Check image size: <pre><code>docker images substrate-platform\n</code></pre></p>"},{"location":"deployment/docker/#updates-and-maintenance","title":"\ud83d\udd04 Updates and Maintenance","text":""},{"location":"deployment/docker/#rebuild-image","title":"Rebuild Image","text":"<pre><code>docker compose build --no-cache\n</code></pre>"},{"location":"deployment/docker/#update-running-container","title":"Update Running Container","text":"<pre><code>docker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"deployment/docker/#clean-up","title":"Clean Up","text":"<pre><code># Remove containers\ndocker compose down\n\n# Remove volumes\ndocker compose down -v\n\n# Remove images\ndocker rmi substrate-platform:latest\n</code></pre>"},{"location":"deployment/docker/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Nginx Documentation</li> <li>Docker Documentation</li> <li>Prometheus Monitoring</li> <li>Let's Encrypt</li> </ul>"},{"location":"deployment/docker/#support","title":"\ud83e\udd1d Support","text":"<p>For issues or questions, please check the container logs first: <pre><code>docker logs substrate-platform-http -f\n</code></pre></p>"},{"location":"deployment/kubernetes/","title":"Kubernetes Deployment Guide","text":"<p>This guide explains how to deploy the Substrate Platform to a Kubernetes cluster. The configuration files are located in the <code>k8s/</code> directory.</p>"},{"location":"deployment/kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (Minikube, EKS, GKE, AKS, etc.)</li> <li><code>kubectl</code> installed and configured locally</li> <li>Docker image available in a container registry (built via <code>docker build -t substrate-platform .</code>)</li> </ul>"},{"location":"deployment/kubernetes/#configuration-structure","title":"Configuration Structure","text":"<p>The <code>k8s/</code> directory contains standard manifest files:</p> <ul> <li><code>deployment.yaml</code>: Defines the application pods, replicas, and resources.</li> <li><code>service.yaml</code>: Exposes the pods internally via ClusterIP.</li> <li><code>ingress.yaml</code>: Exposes the service externally (requires an Ingress Controller).</li> <li><code>configmap.yaml</code>: Stores environment variables and configuration.</li> <li><code>kustomization.yaml</code>: Bundles resources for easier deployment.</li> </ul>"},{"location":"deployment/kubernetes/#quick-start-with-kustomize","title":"Quick Start (with Kustomize)","text":"<p>The easiest way to deploy is using <code>kubectl</code> with Kustomize support (built-in).</p> <ol> <li> <p>Deploy resources:     <pre><code>kubectl apply -k k8s/\n</code></pre></p> </li> <li> <p>Verify deployment:     <pre><code>kubectl get pods -l app=substrate-platform\n</code></pre></p> </li> <li> <p>Check logs:     <pre><code>kubectl logs -l app=substrate-platform\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/#manual-deployment","title":"Manual Deployment","text":"<p>If you prefer applying files individually:</p> <pre><code>kubectl apply -f k8s/configmap.yaml\nkubectl apply -f k8s/deployment.yaml\nkubectl apply -f k8s/service.yaml\nkubectl apply -f k8s/ingress.yaml\n</code></pre>"},{"location":"deployment/kubernetes/#customizing-configuration","title":"Customizing Configuration","text":""},{"location":"deployment/kubernetes/#updating-configmap","title":"updating ConfigMap","text":"<p>Edit <code>k8s/configmap.yaml</code> to change environment variables:</p> <pre><code>data:\n  LOG_LEVEL: \"debug\"\n  ENABLE_METRICS: \"false\"\n</code></pre> <p>After editing, apply the changes and restart the rollout:</p> <pre><code>kubectl apply -f k8s/configmap.yaml\nkubectl rollout restart deployment/substrate-platform\n</code></pre>"},{"location":"deployment/kubernetes/#ingress-host","title":"Ingress Host","text":"<p>Update <code>k8s/ingress.yaml</code> to match your actual domain name:</p> <pre><code>rules:\n  - host: my-platform.example.com\n</code></pre>"},{"location":"deployment/kubernetes/#resource-limits","title":"Resource Limits","text":"<p>The default deployment is configured with conservative resource limits in <code>deployment.yaml</code>. Adjust these based on your actual load and metrics.</p> <pre><code>resources:\n  limits:\n    memory: \"1Gi\"\n    cpu: \"1000m\"\n</code></pre>"},{"location":"deployment/kubernetes/#scaling","title":"Scaling","text":"<p>To scale the application manually:</p> <pre><code>kubectl scale deployment/substrate-platform --replicas=3\n</code></pre>"},{"location":"development/contributing/","title":"Contributing to Substrate Platform","text":"<p>First off, thank you for considering contributing to Substrate Platform! It's people like you that make this project such a great tool.</p>"},{"location":"development/contributing/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>How Can I Contribute?</li> <li>Development Setup</li> <li>Pull Request Process</li> <li>Coding Standards</li> <li>Commit Message Guidelines</li> <li>Docker Development</li> </ul>"},{"location":"development/contributing/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":"<p>This project and everyone participating in it is governed by our Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior to the project maintainers.</p>"},{"location":"development/contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful: Treat everyone with respect and kindness</li> <li>Be collaborative: Work together and help each other</li> <li>Be inclusive: Welcome newcomers and diverse perspectives</li> <li>Be professional: Keep discussions focused and constructive</li> </ul>"},{"location":"development/contributing/#how-can-i-contribute","title":"\ud83e\udd1d How Can I Contribute?","text":""},{"location":"development/contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>Before creating bug reports, please check the existing issues to avoid duplicates. When creating a bug report, include as many details as possible using the bug report template.</p>"},{"location":"development/contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>Enhancement suggestions are tracked as GitHub issues. Use the feature request template and provide detailed information about the proposed feature.</p>"},{"location":"development/contributing/#your-first-code-contribution","title":"Your First Code Contribution","text":"<p>Unsure where to begin? Look for issues labeled: - <code>good first issue</code> - Good for newcomers - <code>help wanted</code> - Extra attention needed - <code>documentation</code> - Documentation improvements</p>"},{"location":"development/contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> <li>Make your changes</li> <li>Commit your changes (see commit guidelines below)</li> <li>Push to your fork (<code>git push origin feature/amazing-feature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"development/contributing/#development-setup","title":"\ud83d\udcbb Development Setup","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18.x or higher</li> <li>npm 9.x or higher</li> <li>Docker 24.x or higher (optional, for Docker development)</li> <li>Git</li> </ul>"},{"location":"development/contributing/#local-setup","title":"Local Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/yourusername/substrate-platform.git\ncd substrate-platform\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>npm install\n</code></pre></p> </li> <li> <p>Set up environment variables <pre><code>cp .env.example .env\n# Edit .env with your local settings\n</code></pre></p> </li> <li> <p>Start development server <pre><code>npm run dev\n</code></pre></p> </li> <li> <p>Access the application</p> </li> <li>Open http://localhost:5173 (or the port shown in terminal)</li> </ol>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Run tests with coverage\nnpm run test:coverage\n</code></pre>"},{"location":"development/contributing/#linting-and-formatting","title":"Linting and Formatting","text":"<pre><code># Run ESLint\nnpm run lint\n\n# Fix linting issues\nnpm run lint:fix\n\n# Format code (if Prettier is configured)\nnpm run format\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"\ud83d\udd04 Pull Request Process","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li> <p>Update your fork: Sync with the latest changes from main    <pre><code>git checkout main\ngit pull upstream main\ngit checkout your-feature-branch\ngit rebase main\n</code></pre></p> </li> <li> <p>Run tests: Ensure all tests pass    <pre><code>npm test\n</code></pre></p> </li> <li> <p>Lint your code: Fix any linting errors    <pre><code>npm run lint\n</code></pre></p> </li> <li> <p>Build the project: Verify the build succeeds    <pre><code>npm run build\n</code></pre></p> </li> <li> <p>Update documentation: Update relevant docs for your changes</p> </li> </ol>"},{"location":"development/contributing/#pr-guidelines","title":"PR Guidelines","text":"<ul> <li>Fill out the PR template completely</li> <li>Link to related issues using \"Fixes #123\" or \"Closes #123\"</li> <li>Keep changes focused: One feature/fix per PR</li> <li>Write meaningful descriptions: Explain what and why, not just what</li> <li>Add tests: Include tests for new features</li> <li>Update documentation: Keep docs in sync with code changes</li> <li>Be responsive: Address review comments promptly</li> </ul>"},{"location":"development/contributing/#pr-review-process","title":"PR Review Process","text":"<ol> <li>A maintainer will review your PR</li> <li>Address any requested changes</li> <li>Once approved, a maintainer will merge your PR</li> <li>Your contribution will be included in the next release!</li> </ol>"},{"location":"development/contributing/#coding-standards","title":"\ud83c\udfa8 Coding Standards","text":""},{"location":"development/contributing/#typescriptjavascript","title":"TypeScript/JavaScript","text":"<ul> <li>Use TypeScript for all new code</li> <li>Follow the existing code style</li> <li>Use meaningful variable and function names</li> <li>Write clear comments for complex logic</li> <li>Prefer <code>const</code> over <code>let</code>, avoid <code>var</code></li> <li>Use arrow functions where appropriate</li> </ul>"},{"location":"development/contributing/#file-organization","title":"File Organization","text":"<pre><code>src/\n\u251c\u2500\u2500 components/       # Reusable UI components\n\u251c\u2500\u2500 pages/           # Page components\n\u251c\u2500\u2500 services/        # API and business logic\n\u251c\u2500\u2500 types/           # TypeScript type definitions\n\u251c\u2500\u2500 utils/           # Utility functions\n\u251c\u2500\u2500 constants/       # Constants and configuration\n\u2514\u2500\u2500 mock/            # Mock data for development\n</code></pre>"},{"location":"development/contributing/#component-guidelines","title":"Component Guidelines","text":"<ul> <li>One component per file</li> <li>Use functional components with hooks</li> <li>Keep components small and focused</li> <li>Extract reusable logic into custom hooks</li> <li>Document props with TypeScript interfaces</li> </ul>"},{"location":"development/contributing/#cssstyling","title":"CSS/Styling","text":"<ul> <li>Use CSS modules or styled-components</li> <li>Follow BEM naming convention for classes</li> <li>Keep styles close to components</li> <li>Use CSS variables for theming</li> <li>Ensure responsive design</li> </ul>"},{"location":"development/contributing/#commit-message-guidelines","title":"\ud83d\udcdd Commit Message Guidelines","text":"<p>We follow the Conventional Commits specification.</p>"},{"location":"development/contributing/#format","title":"Format","text":"<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>"},{"location":"development/contributing/#types","title":"Types","text":"<ul> <li>feat: A new feature</li> <li>fix: A bug fix</li> <li>docs: Documentation only changes</li> <li>style: Code style changes (formatting, semicolons, etc.)</li> <li>refactor: Code refactoring (no functional changes)</li> <li>perf: Performance improvements</li> <li>test: Adding or updating tests</li> <li>chore: Maintenance tasks (dependencies, config, etc.)</li> <li>ci: CI/CD changes</li> <li>build: Build system changes</li> </ul>"},{"location":"development/contributing/#examples","title":"Examples","text":"<pre><code># Feature\nfeat(docker): add HTTPS support with configurable SSL\n\n# Bug fix\nfix(nginx): correct metrics endpoint configuration\n\n# Documentation\ndocs(readme): update Docker deployment instructions\n\n# Refactoring\nrefactor(api): simplify data fetching logic\n\n# Chore\nchore(deps): update dependencies to latest versions\n</code></pre>"},{"location":"development/contributing/#best-practices","title":"Best Practices","text":"<ul> <li>Use the imperative mood (\"add\" not \"added\")</li> <li>Capitalize the first letter</li> <li>No period at the end of subject</li> <li>Limit subject line to 50 characters</li> <li>Wrap body at 72 characters</li> <li>Explain what and why, not how</li> </ul>"},{"location":"development/contributing/#docker-development","title":"\ud83d\udc33 Docker Development","text":""},{"location":"development/contributing/#building-the-docker-image","title":"Building the Docker Image","text":"<pre><code># Build the image\ndocker build -t substrate-platform:dev .\n</code></pre>"},{"location":"development/contributing/#testing-docker-configuration","title":"Testing Docker Configuration","text":"<pre><code># Test HTTP mode\ndocker compose --profile http up -d\n\n# Test HTTPS mode\ndocker compose --profile https up -d\n\n# Test with monitoring\ndocker compose --profile http --profile monitoring up -d\n</code></pre>"},{"location":"development/contributing/#docker-guidelines","title":"Docker Guidelines","text":"<ul> <li>Test both HTTP and HTTPS modes</li> <li>Verify health check endpoints</li> <li>Check logs for errors</li> <li>Ensure metrics work if enabled</li> <li>Test with different environment variables</li> </ul>"},{"location":"development/contributing/#code-review-checklist","title":"\ud83d\udd0d Code Review Checklist","text":""},{"location":"development/contributing/#for-authors","title":"For Authors","text":"<ul> <li>[ ] Code follows project style guidelines</li> <li>[ ] Self-reviewed the code</li> <li>[ ] Added/updated tests</li> <li>[ ] All tests pass</li> <li>[ ] Updated documentation</li> <li>[ ] No new warnings or errors</li> <li>[ ] Tested locally</li> <li>[ ] Tested in Docker (if applicable)</li> </ul>"},{"location":"development/contributing/#for-reviewers","title":"For Reviewers","text":"<ul> <li>[ ] Code is clear and maintainable</li> <li>[ ] Tests are adequate</li> <li>[ ] Documentation is updated</li> <li>[ ] No security issues</li> <li>[ ] Performance is acceptable</li> <li>[ ] Breaking changes are documented</li> </ul>"},{"location":"development/contributing/#issue-and-pr-labels","title":"\ud83c\udff7\ufe0f Issue and PR Labels","text":"<ul> <li><code>bug</code> - Something isn't working</li> <li><code>enhancement</code> - New feature or request</li> <li><code>documentation</code> - Documentation improvements</li> <li><code>good first issue</code> - Good for newcomers</li> <li><code>help wanted</code> - Extra attention needed</li> <li><code>dependencies</code> - Dependency updates</li> <li><code>docker</code> - Docker-related changes</li> <li><code>security</code> - Security improvements</li> <li><code>performance</code> - Performance improvements</li> <li><code>needs-triage</code> - Needs initial review</li> </ul>"},{"location":"development/contributing/#getting-help","title":"\ud83d\udcde Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> <li>Documentation: Check DOCKER.md and README.md</li> </ul>"},{"location":"development/contributing/#recognition","title":"\ud83c\udf89 Recognition","text":"<p>Contributors who make significant contributions will be: - Listed in the project's CONTRIBUTORS.md file - Mentioned in release notes - Thanked in the community</p>"},{"location":"development/contributing/#license","title":"\ud83d\udcc4 License","text":"<p>By contributing, you agree that your contributions will be licensed under the same license as the project (see LICENSE file).</p> <p>Thank you for contributing to Substrate Platform! \ud83d\ude80</p>"},{"location":"development/frontend/","title":"Frontend Implementation Guide","text":""},{"location":"development/frontend/#overview","title":"Overview","text":"<p>The frontend serves as the visualization and interaction layer for the Structural Integrity Platform, providing graph</p> <p>visualization, governance dashboards, and knowledge exploration interfaces.</p>"},{"location":"development/frontend/#technology-stack-from-blueprint","title":"Technology Stack (From Blueprint)","text":"Layer Technology Version Rationale Build Vite 6.x Native ESM, fast HMR UI Framework React 19 Hooks, Suspense, transitions Components shadcn/ui + Radix UI + Tailwind CSS 4.x Accessible, composable, dark mode; used by OpenAI and Adobe Client State Zustand 5.x ~3KB, no Provider needed, middleware for persistence Server State TanStack Query 5.x Caching, background refetching, optimistic mutations Routing TanStack Router - File-based, type-safe, automatic code splitting Validation Zod 3.x TypeScript-first runtime validation Graph (Exploration) Sigma.js 3.0.2 WebGL performance for 100K+ nodes at 60fps Graph (Architecture) Cytoscape.js 3.x Rich layout algorithms (Dagre, hierarchical) Charts Recharts + Nivo 2.x 9M+ weekly downloads, composable React API Utilities @tanstack/react-virtual - Virtualization for long lists"},{"location":"development/frontend/#project-structure","title":"Project Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 routes/                      # TanStack Router pages\n\u2502   \u251c\u2500\u2500 index.tsx               # Dashboard home\n\u2502   \u251c\u2500\u2500 graph/                  # Graph visualization routes\n\u2502   \u2502   \u251c\u2500\u2500 explorer.tsx        # WebGL exploration view\n\u2502   \u2502   \u2514\u2500\u2500 architecture.tsx    # Architecture diagram view\n\u2502   \u251c\u2500\u2500 governance/             # Policy &amp; drift management\n\u2502   \u2502   \u251c\u2500\u2500 policies.tsx        # Policy builder\n\u2502   \u2502   \u251c\u2500\u2500 violations.tsx      # Drift detection alerts\n\u2502   \u2502   \u2514\u2500\u2500 enforcement.tsx     # Graduated enforcement settings\n\u2502   \u251c\u2500\u2500 knowledge/              # GraphRAG knowledge interface\n\u2502   \u2502   \u251c\u2500\u2500 search.tsx          # Semantic search\n\u2502   \u2502   \u251c\u2500\u2500 communities.tsx     # Community summaries\n\u2502   \u2502   \u2514\u2500\u2500 documents.tsx       # Documentation browser\n\u2502   \u2514\u2500\u2500 settings/               # Configuration\n\u2502       \u251c\u2500\u2500 llm.tsx             # LLM provider settings\n\u2502       \u251c\u2500\u2500 connectors.tsx      # Data source configuration\n\u2502       \u2514\u2500\u2500 profile.tsx         # User preferences\n\u2502\n\u251c\u2500\u2500 components/                  # Feature-based organization\n\u2502   \u251c\u2500\u2500 graph/                  # Graph visualization components\n\u2502   \u2502   \u251c\u2500\u2500 SigmaGraph.tsx      # WebGL exploration (Sigma.js)\n\u2502   \u2502   \u251c\u2500\u2500 ArchitectureDiagram.tsx  # Cytoscape architecture view\n\u2502   \u2502   \u251c\u2500\u2500 GraphControls.tsx   # Zoom, pan, layout controls\n\u2502   \u2502   \u251c\u2500\u2500 NodeDetails.tsx     # Entity information panel\n\u2502   \u2502   \u2514\u2500\u2500 DiffVisualization.tsx    # Graph diff view\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 dashboard/              # Dashboard widgets\n\u2502   \u2502   \u251c\u2500\u2500 HealthScoreCard.tsx    # Composite health score (A-F)\n\u2502   \u2502   \u251c\u2500\u2500 DriftAlertsCard.tsx    # Active drift violations\n\u2502   \u2502   \u251c\u2500\u2500 MetricsTrendChart.tsx  # Time-series trends (Recharts)\n\u2502   \u2502   \u251c\u2500\u2500 DependencyHeatmap.tsx  # Risk matrix (Nivo)\n\u2502   \u2502   \u2514\u2500\u2500 ServiceCatalog.tsx     # Backstage-inspired entity catalog\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 governance/             # Policy management components\n\u2502   \u2502   \u251c\u2500\u2500 PolicyBuilder.tsx       # Visual Rego policy editor\n\u2502   \u2502   \u251c\u2500\u2500 ViolationCard.tsx       # Drift violation display\n\u2502   \u2502   \u251c\u2500\u2500 EnforcementToggle.tsx   # Observe/Advise/Enforce selector\n\u2502   \u2502   \u2514\u2500\u2500 PolicyEffectiveness.tsx # False positive tracking\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 knowledge/              # GraphRAG interfaces\n\u2502   \u2502   \u251c\u2500\u2500 SemanticSearch.tsx      # Natural language query\n\u2502   \u2502   \u251c\u2500\u2500 CommunityCard.tsx       # Leiden community summaries\n\u2502   \u2502   \u251c\u2500\u2500 DocumentTimeline.tsx    # Staleness indicators\n\u2502   \u2502   \u2514\u2500\u2500 EntityResolution.tsx    # Cross-source identity matching\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 settings/               # Configuration interfaces\n\u2502   \u2502   \u251c\u2500\u2500 LLMConfiguration.tsx    # Provider/model selection\n\u2502   \u2502   \u251c\u2500\u2500 ConnectorList.tsx       # Data source management\n\u2502   \u2502   \u251c\u2500\u2500 OAuth Popup.tsx          # OAuth 2.0 flows\n\u2502   \u2502   \u2514\u2500\u2500 TestConnection.tsx      # Connection validation\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 layout/                 # Application shell\n\u2502       \u251c\u2500\u2500 Header.tsx          # Top navigation\n\u2502       \u251c\u2500\u2500 Sidebar.tsx         # Main navigation\n\u2502       \u251c\u2500\u2500 ThemeToggle.tsx     # Dark/light mode\n\u2502       \u2514\u2500\u2500 Breadcrumbs.tsx     # Route navigation\n\u2502\n\u251c\u2500\u2500 hooks/\n\u2502   \u251c\u2500\u2500 queries/                # TanStack Query data fetching\n\u2502   \u2502   \u251c\u2500\u2500 useGraphData.ts\n\u2502   \u2502   \u251c\u2500\u2500 useViolations.ts\n\u2502   \u2502   \u251c\u2500\u2500 usePolicies.ts\n\u2502   \u2502   \u251c\u2500\u2500 useCommunities.ts\n\u2502   \u2502   \u2514\u2500\u2500 useMetrics.ts\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 mutations/              # TanStack Query mutations\n\u2502   \u2502   \u251c\u2500\u2500 usePolicyUpdate.ts\n\u2502   \u2502   \u251c\u2500\u2500 useConnectorSync.ts\n\u2502   \u2502   \u2514\u2500\u2500 useViolationAck.ts\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 useWebWorker.ts         # ForceAtlas2 layout offload\n\u2502\n\u251c\u2500\u2500 stores/                     # Zustand state management\n\u2502   \u251c\u2500\u2500 authStore.ts            # Authentication state\n\u2502   \u251c\u2500\u2500 themeStore.ts           # Dark mode persistence\n\u2502   \u251c\u2500\u2500 graphStore.ts           # Graph filter/layout state\n\u2502   \u2514\u2500\u2500 settingsStore.ts        # User preferences\n\u2502\n\u251c\u2500\u2500 lib/                        # Utilities and helpers\n\u2502   \u251c\u2500\u2500 api/                    # API client configuration\n\u2502   \u251c\u2500\u2500 formatters/             # Data formatting utilities\n\u2502   \u251c\u2500\u2500 validators/             # Zod schemas\n\u2502   \u2514\u2500\u2500 constants.ts            # Application constants\n\u2502\n\u2514\u2500\u2500 types/                      # TypeScript definitions\n    \u251c\u2500\u2500 graph.ts                # Graph entities/edges\n    \u251c\u2500\u2500 policy.ts               # Governance types\n    \u251c\u2500\u2500 metrics.ts              # Quality metrics\n    \u2514\u2500\u2500 api.ts                  # API response types\n</code></pre>"},{"location":"development/frontend/#core-components","title":"Core Components","text":""},{"location":"development/frontend/#graph-visualization","title":"Graph Visualization","text":""},{"location":"development/frontend/#sigmajs-exploration-view","title":"Sigma.js (Exploration View)","text":"<p>Use Case: Large-scale graph exploration (100K+ nodes)</p> <p>Key Features: - WebGL rendering: 60fps at 100K+ edges - Web Worker layout: ForceAtlas2 computation offload - Built on graphology: Rich graph algorithms - Custom reducers: Dynamic styling for filtering</p> <p>Implementation Pattern: <pre><code>import { SigmaContainer, useLoadGraph, useRegisterEvents } from '@react-sigma/core';\nimport '@react-sigma/core/lib/react-sigma.min.css';\n\nexport function SigmaGraph({ nodes, edges }: GraphData) {\n  const loadGraph = useLoadGraph();\n\n  useEffect(() =&gt; {\n    const graph = new Graph();\n    nodes.forEach(node =&gt; graph.addNode(node.id, node));\n    edges.forEach(edge =&gt; graph.addEdge(edge.source, edge.target, edge));\n    loadGraph(graph);\n  }, [nodes, edges, loadGraph]);\n\n  // Custom node renderer for health indicators\n  const nodeReducer = (node, data) =&gt; ({\n    ...data,\n    color: getHealthColor(data.healthStatus), // traffic light coloring\n    size: data.importance * 10\n  });\n\n  return &lt;SigmaContainer style={{ height: '100vh' }} /&gt;;\n}\n</code></pre></p> <p>Performance: Switches between rendering layers based on zoom: - Overview zoom: WebGL (Sigma.js) - Mid-range: Canvas with labels - Export: SVG for high-quality diagrams</p>"},{"location":"development/frontend/#cytoscapejs-architecture-diagrams","title":"Cytoscape.js (Architecture Diagrams)","text":"<p>Use Case: Structured architecture views with rich layouts</p> <p>Layout Algorithms: - Dagre: Directed acyclic dependency flows - CoSE-Bilkent: Compound graphs with nested domain groups - Hierarchical: Org charts, team structures - Breadthfirst: Tree-like structures</p> <p>Implementation Pattern: <pre><code>import CytoscapeComponent from 'react-cytoscapejs';\n\nexport function ArchitectureDiagram({ services, dependencies }: ArchData) {\n  const elements = [\n    ...services.map(s =&gt; ({ data: { id: s.id, label: s.name, team: s.team } })),\n    ...dependencies.map(d =&gt; ({ data: { source: d.from, target: d.to, weight: d.traffic } }))\n  ];\n\n  const stylesheet = [\n    {\n      selector: 'node',\n      style: {\n        'background-color': 'data(healthColor)',\n        'label': 'data(label)'\n      }\n    },\n    {\n      selector: 'edge',\n      style: {\n        'width': 'data(weight)',\n        'line-color': 'data(healthStatus)' // red=error, green=healthy\n      }\n    }\n  ];\n\n  return (\n    &lt;CytoscapeComponent\n      elements={elements}\n      stylesheet={stylesheet}\n      layout={{ name: 'dagre', rankDir: 'TB' }}\n    /&gt;\n  );\n}\n</code></pre></p> <p>Compound Nodes: Group services by team or bounded context</p> <p>Edge Styling: Thickness by traffic volume, color by health status</p>"},{"location":"development/frontend/#diff-visualization","title":"Diff Visualization","text":"<p>Implementation: Overlay two graph states with color coding - Green: Additions - Red: Removals - Yellow: Modifications</p> <p>Sigma.js supports this via dynamic style updates through reducers.</p>"},{"location":"development/frontend/#dashboard-design","title":"Dashboard Design","text":""},{"location":"development/frontend/#card-based-kpi-layout","title":"Card-Based KPI Layout","text":"<p>Top Row (Key Metrics): - Total Services - Compliance Percentage (traffic light) - Active Drift Alerts - Deployment Frequency (DORA)</p> <p>Below (Trends): - Time-series charts via Recharts (composable React API) - Heatmaps via Nivo (dependency risk matrices) - Radar charts for team comparisons</p> <p>Key Views: 1. Architecture Health Overview: Traffic-light scoring (A-F) 2. Drift Detection Alerts: Trend lines over sprints 3. Team Ownership Matrices: Who owns what 4. Service Catalog: Backstage-inspired entity model 5. Dependency Risk Heatmap: Coupling/instability matrix</p> <p>Real-time Updates: <pre><code>import { useQuery } from '@tanstack/react-query';\n\nfunction DashboardMetrics() {\n  const { data, isRefetching } = useQuery({\n    queryKey: ['metrics', 'dashboard'],\n    queryFn: fetchDashboardMetrics,\n    refetchInterval: 30000, // Poll every 30s\n  });\n\n  // WebSocket integration for live alerts\n  useEffect(() =&gt; {\n    const ws = new WebSocket('ws://localhost:8080/drift-alerts');\n    ws.onmessage = (event) =&gt; {\n      const alert = JSON.parse(event.data);\n      useDriftStore.getState().addAlert(alert);\n    };\n    return () =&gt; ws.close();\n  }, []);\n\n  return &lt;MetricsGrid data={data} /&gt;;\n}\n</code></pre></p>"},{"location":"development/frontend/#settings-ui-llm-configuration","title":"Settings UI &amp; LLM Configuration","text":""},{"location":"development/frontend/#llm-configuration","title":"LLM Configuration","text":"<p>Provider Selector: - OpenAI - Anthropic - Azure OpenAI - Ollama (self-hosted) - Custom endpoint</p> <p>Configuration Fields: <pre><code>interface LLMConfig {\n  provider: 'openai' | 'anthropic' | 'azure' | 'ollama' | 'custom';\n  apiKey: string;        // Masked input with show/hide\n  model: string;         // Dropdown of available models\n  temperature: number;   // Slider 0.0-2.0\n  maxTokens: number;    // Slider with presets\n  endpoint?: string;     // For custom providers\n}\n</code></pre></p> <p>Test Connection Button: - Validates through backend proxy - Displays latency and model availability - Security: API keys stored server-side only, NEVER in frontend env vars</p>"},{"location":"development/frontend/#data-source-connector-configuration","title":"Data Source Connector Configuration","text":"<p>OAuth 2.0 Popup Flows: <pre><code>function ConnectorOAuth({ provider }: { provider: 'github' | 'jira' | 'confluence' }) {\n  const openOAuthPopup = () =&gt; {\n    const popup = window.open(\n      `/api/auth/${provider}/authorize`,\n      'oauth',\n      'width=500,height=600'\n    );\n\n    window.addEventListener('message', (event) =&gt; {\n      if (event.data.type === 'oauth-success') {\n        queryClient.invalidateQueries(['connectors']);\n        popup?.close();\n      }\n    });\n  };\n\n  return &lt;Button onClick={openOAuthPopup}&gt;Connect {provider}&lt;/Button&gt;;\n}\n</code></pre></p> <p>Connection Status Indicators: - Connected (green checkmark) - Disconnected (gray dot) - Error (red X with tooltip) - Syncing (animated spinner)</p> <p>Sync Scheduling: Dropdown for polling intervals (15min, 30min, 1hr, 4hr, daily)</p> <p>Repository Selection: Multi-select for GitHub repos to ingest</p>"},{"location":"development/frontend/#performance-optimization","title":"Performance Optimization","text":""},{"location":"development/frontend/#lazy-loading","title":"Lazy Loading","text":"<pre><code>// Heavy graph components loaded on demand\nconst SigmaGraph = lazy(() =&gt; import('./components/graph/SigmaGraph'));\nconst ArchitectureDiagram = lazy(() =&gt; import('./components/graph/ArchitectureDiagram'));\n\nfunction GraphExplorer() {\n  return (\n    &lt;Suspense fallback={&lt;GraphSkeleton /&gt;}&gt;\n      &lt;SigmaGraph data={graphData} /&gt;\n    &lt;/Suspense&gt;\n  );\n}\n</code></pre>"},{"location":"development/frontend/#virtualization","title":"Virtualization","text":"<pre><code>import { useVirtualizer } from '@tanstack/react-virtual';\n\nfunction ServiceList({ services }: { services: Service[] }) {\n  const parentRef = useRef&lt;HTMLDivElement&gt;(null);\n\n  const rowVirtualizer = useVirtualizer({\n    count: services.length,\n    getScrollElement: () =&gt; parentRef.current,\n    estimateSize: () =&gt; 80,\n  });\n\n  return (\n    &lt;div ref={parentRef} style={{ height: '600px', overflow: 'auto' }}&gt;\n      &lt;div style={{ height: `${rowVirtualizer.getTotalSize()}px` }}&gt;\n        {rowVirtualizer.getVirtualItems().map((virtualRow) =&gt; (\n          &lt;ServiceCard key={virtualRow.index} service={services[virtualRow.index]} /&gt;\n        ))}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"development/frontend/#web-worker-for-layout-computation","title":"Web Worker for Layout Computation","text":"<pre><code>// hooks/useWebWorker.ts\nexport function useForceAtlas2Layout(graph: Graph) {\n  const workerRef = useRef&lt;Worker&gt;();\n\n  useEffect(() =&gt; {\n    workerRef.current = new Worker(\n      new URL('graphology-layout-forceatlas2/worker', import.meta.url)\n    );\n\n    workerRef.current.postMessage({ graph: graph.export() });\n\n    workerRef.current.onmessage = (event) =&gt; {\n      graph.import(event.data.positions);\n    };\n\n    return () =&gt; workerRef.current?.terminate();\n  }, [graph]);\n}\n</code></pre>"},{"location":"development/frontend/#state-management-patterns","title":"State Management Patterns","text":""},{"location":"development/frontend/#zustand-client-state","title":"Zustand (Client State)","text":"<pre><code>// stores/graphStore.ts\nimport { create } from 'zustand';\nimport { persist } from 'zustand/middleware';\n\ninterface GraphState {\n  layout: 'force' | 'dagre' | 'hierarchical';\n  filters: {\n    teams: string[];\n    healthStatus: ('healthy' | 'warning' | 'critical')[];\n  };\n  setLayout: (layout: GraphState['layout']) =&gt; void;\n  toggleTeamFilter: (team: string) =&gt; void;\n}\n\nexport const useGraphStore = create&lt;GraphState&gt;()(\n  persist(\n    (set) =&gt; ({\n      layout: 'force',\n      filters: { teams: [], healthStatus: [] },\n      setLayout: (layout) =&gt; set({ layout }),\n      toggleTeamFilter: (team) =&gt; set((state) =&gt; ({\n        filters: {\n          ...state.filters,\n          teams: state.filters.teams.includes(team)\n            ? state.filters.teams.filter(t =&gt; t !== team)\n            : [...state.filters.teams, team]\n        }\n      })),\n    }),\n    { name: 'graph-settings' }\n  )\n);\n</code></pre>"},{"location":"development/frontend/#tanstack-query-server-state","title":"TanStack Query (Server State)","text":"<pre><code>// hooks/queries/useGraphData.ts\nimport { useQuery } from '@tanstack/react-query';\n\nexport function useGraphData() {\n  return useQuery({\n    queryKey: ['graph', 'full'],\n    queryFn: async () =&gt; {\n      const response = await fetch('/api/graph');\n      if (!response.ok) throw new Error('Failed to fetch graph');\n      return response.json();\n    },\n    staleTime: 5 * 60 * 1000, // 5 minutes\n    cacheTime: 10 * 60 * 1000, // 10 minutes\n    refetchOnWindowFocus: false,\n  });\n}\n</code></pre>"},{"location":"development/frontend/#seo-accessibility","title":"SEO &amp; Accessibility","text":""},{"location":"development/frontend/#seo-best-practices","title":"SEO Best Practices","text":"<ul> <li>Title Tags: Descriptive titles per route</li> <li>Meta Descriptions: Compelling summaries</li> <li>Heading Structure: Single <code>&lt;h1&gt;</code> per page, proper hierarchy</li> <li>Semantic HTML: HTML5 semantic elements</li> <li>Unique IDs: All interactive elements have descriptive IDs</li> </ul>"},{"location":"development/frontend/#accessibility-radix-ui-shadcnui","title":"Accessibility (Radix UI + shadcn/ui)","text":"<ul> <li>ARIA attributes: Built-in accessibility</li> <li>Keyboard navigation: Full keyboard support</li> <li>Screen reader support: Semantic markup</li> <li>Focus management: Visible focus indicators</li> <li>Dark mode: Built-in theme support</li> </ul>"},{"location":"development/frontend/#development-workflow","title":"Development Workflow","text":""},{"location":"development/frontend/#file-based-routing-tanstack-router","title":"File-based Routing (TanStack Router)","text":"<ul> <li>Routes in <code>routes/</code> directory</li> <li>Automatic code splitting</li> <li>Type-safe navigation</li> <li>Lazy loading per route</li> </ul>"},{"location":"development/frontend/#validation-with-zod","title":"Validation with Zod","text":"<pre><code>import { z } from 'zod';\n\nconst PolicySchema = z.object({\n  name: z.string().min(3).max(100),\n  severity: z.enum(['low', 'medium', 'high', 'critical']),\n  enforcement: z.enum(['observe', 'advise', 'enforce']),\n  regoCode: z.string().min(10),\n});\n\ntype Policy = z.infer&lt;typeof PolicySchema&gt;;\n</code></pre>"},{"location":"development/frontend/#testing-strategy","title":"Testing Strategy","text":"<ul> <li>Unit Tests: Vitest for component/utility testing</li> <li>Integration Tests: React Testing Library</li> <li>E2E Tests: Playwright for critical user flows</li> <li>Visual Regression: Chromatic/Percy for UI snapshots</li> </ul>"},{"location":"development/frontend/#build-deployment","title":"Build &amp; Deployment","text":""},{"location":"development/frontend/#vite-configuration","title":"Vite Configuration","text":"<ul> <li>Development: Fast HMR with native ESM</li> <li>Production: Optimized bundles with code splitting</li> <li>Preview: Test production build locally</li> </ul>"},{"location":"development/frontend/#environment-variables","title":"Environment Variables","text":"<p>NEVER expose sensitive data in frontend env: <pre><code>// \u274c WRONG - Embeds in build output\nconst API_KEY = import.meta.env.VITE_API_KEY;\n\n// \u2705 CORRECT - Stored server-side only\nconst response = await fetch('/api/llm/generate', {\n  headers: { Authorization: 'Bearer &lt;server-side-token&gt;' }\n});\n</code></pre></p>"},{"location":"development/frontend/#migration-from-current-stack","title":"Migration from Current Stack","text":"<p>Current: React Native Web + MUI Target: React 19 + shadcn/ui + Tailwind CSS</p> <p>Migration Path: 1. Set up Vite 6 + React 19 project 2. Install shadcn/ui + Radix UI components 3. Migrate components one feature area at a time 4. Replace MUI components with shadcn/ui equivalents 5. Update routing to TanStack Router 6. Replace Context API with Zustand where appropriate 7. Introduce TanStack Query for server state 8. Add Sigma.js and Cytoscape.js for graph visualization</p>"},{"location":"development/frontend/#design-aesthetics-from-requirements","title":"Design Aesthetics (from Requirements)","text":"<ul> <li>Rich, vibrant colors: Curated HSL palettes, no plain RGB</li> <li>Modern typography: Google Fonts (Inter, Roboto, Outfit)</li> <li>Smooth gradients: Subtle backgrounds</li> <li>Micro-animations: Hover effects, transitions</li> <li>Dark mode first: Premium feel with glassmorphism</li> <li>Responsive: Mobile-first, works on all screen sizes</li> </ul>"},{"location":"development/frontend/#summary","title":"Summary","text":"<p>The frontend leverages modern React 19 features, WebGL-powered graph visualization, and shadcn/ui for accessible, beautiful components. Performance is optimized through lazy loading, virtualization, and Web Worker offloading. State management is split between Zustand (client) and TanStack Query (server), with Zod validation and TypeScript for type safety.</p> <p>The goal: WOW the user at first glance with a premium, dynamic, and highly interactive experience.</p>"},{"location":"development/refactor-history/","title":"Refactor Summary - API-First Architecture with Mock Fallback","text":""},{"location":"development/refactor-history/#overview","title":"Overview","text":"<p>This refactor implements a clean, SOLID, DRY architecture with an API-first approach that automatically falls back to mock data when the real API is unavailable.</p>"},{"location":"development/refactor-history/#principles-applied","title":"Principles Applied","text":""},{"location":"development/refactor-history/#solid-principles","title":"SOLID Principles","text":"<ul> <li>Single Responsibility: Each module has one reason to change</li> <li><code>ApiClient</code> handles HTTP only</li> <li>Services handle domain logic only</li> <li>Hooks handle React Query integration only</li> <li> <p>Mock provider handles mock data only</p> </li> <li> <p>Open/Closed: New services can be added without modifying existing code</p> </li> <li>Liskov Substitution: Mock and real APIs are interchangeable</li> <li>Interface Segregation: Small, focused interfaces for each domain</li> <li>Dependency Inversion: Code depends on abstractions, not implementations</li> </ul>"},{"location":"development/refactor-history/#dry-dont-repeat-yourself","title":"DRY (Don't Repeat Yourself)","text":"<ul> <li>Single barrel export for all API operations</li> <li>Reusable query key factory</li> <li>Base service class with common HTTP methods</li> <li>Consolidated mock data registry</li> </ul>"},{"location":"development/refactor-history/#kiss-keep-it-simple-stupid","title":"KISS (Keep It Simple, Stupid)","text":"<ul> <li>Predictable file structure</li> <li>Consistent naming conventions</li> <li>Minimal boilerplate</li> <li>Clear separation of concerns</li> </ul>"},{"location":"development/refactor-history/#new-architecture","title":"New Architecture","text":"<pre><code>src/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 index.ts           # Barrel export - single entry point\n\u2502   \u251c\u2500\u2500 setup.ts           # Initialize API with mock provider\n\u2502   \u251c\u2500\u2500 client.ts          # API client with fallback logic\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u2514\u2500\u2500 index.ts       # All services (DRY - uses BaseService)\n\u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u2514\u2500\u2500 index.ts       # All React Query hooks (typed)\n\u2502   \u2514\u2500\u2500 mock/\n\u2502       \u2514\u2500\u2500 data/\n\u2502           \u251c\u2500\u2500 index.ts   # Mock data registry &amp; provider\n\u2502           \u251c\u2500\u2500 graph/     # Graph-related mock data\n\u2502           \u251c\u2500\u2500 policies/  # Policy mock data\n\u2502           \u251c\u2500\u2500 drift/     # Drift violation mock data\n\u2502           \u251c\u2500\u2500 health/    # Health check mock data\n\u2502           \u251c\u2500\u2500 sync/      # Sync job mock data\n\u2502           \u251c\u2500\u2500 ui/        # UI config mock data\n\u2502           \u251c\u2500\u2500 memory/    # Memory/audit mock data\n\u2502           \u251c\u2500\u2500 search/    # Search evidence mock data\n\u2502           \u2514\u2500\u2500 *.yaml     # YAML files alongside JSON\n\u251c\u2500\u2500 hooks/\n\u2502   \u2514\u2500\u2500 index.ts           # Re-exports from api/hooks\n\u251c\u2500\u2500 types/\n\u2502   \u2514\u2500\u2500 index.ts           # All TypeScript types\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"development/refactor-history/#key-features","title":"Key Features","text":""},{"location":"development/refactor-history/#1-api-first-with-automatic-fallback","title":"1. API-First with Automatic Fallback","text":"<pre><code>// The client tries real API first, falls back to mock on failure\nconst response = await api.graph.getNodes();\n// If API fails and VITE_ENABLE_MOCK_API=true, returns mock data\n</code></pre>"},{"location":"development/refactor-history/#2-centralized-mock-data","title":"2. Centralized Mock Data","text":"<ul> <li>JSON and YAML files organized by domain</li> <li>Single <code>MockDataRegistry</code> for access</li> <li>Smart <code>mockProvider</code> maps endpoints to data</li> </ul>"},{"location":"development/refactor-history/#3-type-safe-hooks","title":"3. Type-Safe Hooks","text":"<p>All hooks are fully typed: <pre><code>const { data: policies } = usePolicies(); // Policy[] | undefined\nconst { data: graph } = useFullGraph();   // GraphData | undefined\n</code></pre></p>"},{"location":"development/refactor-history/#4-dry-service-layer","title":"4. DRY Service Layer","text":"<pre><code>// BaseService provides common HTTP methods\nclass GraphService extends BaseService {\n  getNodes() { return this.get('/nodes'); }\n  getEdges() { return this.get('/edges'); }\n}\n</code></pre>"},{"location":"development/refactor-history/#usage-examples","title":"Usage Examples","text":""},{"location":"development/refactor-history/#basic-query","title":"Basic Query","text":"<pre><code>import { usePolicies, useFullGraph } from '@/api';\n\nfunction MyComponent() {\n  const { data: policies, isLoading } = usePolicies();\n  const { data: graph } = useFullGraph();\n\n  if (isLoading) return &lt;Loading /&gt;;\n  return &lt;div&gt;{policies?.map(p =&gt; p.name)}&lt;/div&gt;;\n}\n</code></pre>"},{"location":"development/refactor-history/#mutation","title":"Mutation","text":"<pre><code>import { useResolveViolation } from '@/api';\n\nfunction ViolationCard({ id }: { id: string }) {\n  const resolve = useResolveViolation();\n\n  const handleResolve = () =&gt; {\n    resolve.mutate({ \n      id, \n      resolution: { strategy: 'update_intent', reason: 'Fixed' }\n    });\n  };\n\n  return &lt;button onClick={handleResolve}&gt;Resolve&lt;/button&gt;;\n}\n</code></pre>"},{"location":"development/refactor-history/#direct-api-access-if-needed","title":"Direct API Access (if needed)","text":"<pre><code>import { api } from '@/api';\n\n// Direct service access\nconst response = await api.policies.getAll();\nconst policies = response.data;\n</code></pre>"},{"location":"development/refactor-history/#migration-guide","title":"Migration Guide","text":""},{"location":"development/refactor-history/#old-new","title":"Old \u2192 New","text":"<pre><code>// Old way (multiple imports)\nimport { usePolicies } from '@/hooks/queries/usePolicyQueries';\nimport { api } from '@/services/api/unified';\n\n// New way (single import)\nimport { usePolicies, api } from '@/api';\n</code></pre>"},{"location":"development/refactor-history/#environment-variables","title":"Environment Variables","text":"<p>Mock behavior is controlled by: - <code>VITE_ENABLE_MOCK_API=true</code> - Enable mock fallback - <code>VITE_MOCK_API_DELAY=500</code> - Mock response delay</p>"},{"location":"development/refactor-history/#files-removed","title":"Files Removed","text":"<ul> <li><code>src/services/</code> - Old service layer</li> <li><code>src/data/</code> - Old scattered data files</li> <li><code>src/mock/</code> - Old mock structure</li> <li><code>src/hooks/queries/</code> - Old query hooks</li> <li><code>src/hooks/mutations/</code> - Old mutation hooks</li> </ul>"},{"location":"development/refactor-history/#files-addedmodified","title":"Files Added/Modified","text":"<ul> <li><code>src/api/</code> - New consolidated API module</li> <li><code>src/hooks/index.ts</code> - Now re-exports from API</li> <li><code>src/main.tsx</code> - Added <code>setupApi()</code> call</li> <li><code>src/types/index.ts</code> - Added missing <code>AuditItem</code> type</li> </ul>"},{"location":"development/refactor-history/#build-verification","title":"Build Verification","text":"<pre><code>npm run build\n# \u2713 TypeScript compilation successful\n# \u2713 Vite build successful\n# \u2713 No errors\n</code></pre>"},{"location":"development/roadmap/","title":"Development Roadmap","text":"<p>This roadmap outlines the phased implementation of the Structural Integrity Platform, aligned with the blueprint's recommendation: \"Ship with the ingestion pipeline and governance policies first, then layer on GraphRAG community detection as the knowledge graph matures.\"</p>"},{"location":"development/roadmap/#phase-1-foundation-ingestion-governance-months-0-3","title":"Phase 1: Foundation - Ingestion + Governance (Months 0-3)","text":""},{"location":"development/roadmap/#core-infrastructure-weeks-1-2","title":"Core Infrastructure (Weeks 1-2)","text":"<ul> <li>[ ] Project Setup:</li> <li>[ ] Initialize monorepo structure (Rust + Python + Go + React)</li> <li>[ ] Docker Compose configuration for local development</li> <li>[ ] CI/CD pipeline (GitHub Actions)</li> <li> <p>[ ] Development environment documentation</p> </li> <li> <p>[ ] Data Layer:</p> </li> <li>[ ] PostgreSQL schema (entities, policies, violations, audit_logs, event_outbox)</li> <li>[ ] Neo4j schema (<code>:Component</code>, <code>:Service</code>, <code>:Team</code> nodes; <code>:DEPENDS_ON</code>, <code>:OWNS</code> edges)</li> <li>[ ] Redis configuration (caching, session tokens)</li> <li>[ ] Database migration system (Flyway/Liquibase for PostgreSQL)</li> </ul>"},{"location":"development/roadmap/#rust-ingestion-pipeline-weeks-3-6","title":"Rust Ingestion Pipeline (Weeks 3-6)","text":"<ul> <li>[ ] tree-sitter Integration:</li> <li>[ ] Parser initialization for top languages (Rust, Python, Go, TypeScript, Java)</li> <li>[ ] AST extraction queries (functions, classes, imports, dependencies)</li> <li> <p>[ ] Incremental parsing with <code>notify</code> filesystem watcher</p> </li> <li> <p>[ ] stack-graphs Integration:</p> </li> <li>[ ] Cross-file reference resolution</li> <li>[ ] Call graph construction</li> <li> <p>[ ] Import dependency graph</p> </li> <li> <p>[ ] Pipeline Orchestration:</p> </li> <li>[ ] File Watcher \u2192 Change Queue (bounded channels with backpressure)</li> <li>[ ] Parser Pool (rayon parallel processing)</li> <li>[ ] Entity Emitter (graph triples: <code>[source, relationship, target]</code>)</li> <li> <p>[ ] Output Sink (NATS JetStream integration)</p> </li> <li> <p>[ ] Data Connectors:</p> </li> <li>[ ] GitHub connector (repos, PRs, issues via GitHub Apps)</li> <li>[ ] GitLab connector (projects, merge requests, issues)</li> <li> <p>[ ] File system connector (local monorepo support)</p> </li> <li> <p>[ ] Testing:</p> </li> <li>[ ] Unit tests for AST extraction (benchmark: 2,157 lines in ~6.5ms)</li> <li>[ ] Integration tests with sample repositories</li> <li>[ ] Performance benchmarks (target: 15/15 correctness on architectural queries)</li> </ul>"},{"location":"development/roadmap/#go-governance-engine-weeks-7-10","title":"Go Governance Engine (Weeks 7-10)","text":"<ul> <li>[ ] OPA Integration:</li> <li>[ ] Embedded OPA SDK (<code>github.com/open-policy-agent/opa/v1/sdk</code>)</li> <li>[ ] Prepared query caching (goroutine-safe)</li> <li> <p>[ ] Policy loader (read Rego files from <code>policies/</code> directory)</p> </li> <li> <p>[ ] Core Policies (Pre-built Library):</p> </li> <li>[ ] Circular dependency detection (using <code>graph.reachable</code>)</li> <li>[ ] Layer violation detection (presentation \u2192 domain \u2192 infrastructure \u2192 data)</li> <li>[ ] Cross-boundary access enforcement (no direct DB access from presentation)</li> <li>[ ] Dependency version constraints (banned libraries, minimum versions)</li> <li> <p>[ ] Team ownership rules (every component must have an owner)</p> </li> <li> <p>[ ] HTTP API (<code>go-chi/chi</code>):</p> </li> <li>[ ] <code>POST /api/governance/evaluate</code> - Evaluate policies against graph state</li> <li>[ ] <code>GET /api/policies</code> - List active policies</li> <li>[ ] <code>POST /api/policies</code> - Create/update policy</li> <li> <p>[ ] <code>GET /api/violations</code> - Query violations with filters</p> </li> <li> <p>[ ] Event Consumption:</p> </li> <li>[ ] NATS JetStream subscriber (consume ingestion events)</li> <li>[ ] PostgreSQL event sync (update <code>event_outbox</code> table)</li> <li>[ ] Circuit breaking (<code>sony/gobreaker</code>)</li> <li> <p>[ ] Rate limiting (<code>golang.org/x/time/rate</code>)</p> </li> <li> <p>[ ] Observability:</p> </li> <li>[ ] Structured logging (JSON format)</li> <li>[ ] Prometheus metrics (policy evaluation latency, violation counts)</li> <li>[ ] Distributed tracing (OpenTelemetry)</li> </ul>"},{"location":"development/roadmap/#cicd-integration-weeks-11-12","title":"CI/CD Integration (Weeks 11-12)","text":"<ul> <li>[ ] GitHub Actions Integration:</li> <li>[ ] <code>setup-opa@v2</code> action</li> <li>[ ] Extract architecture state from Neo4j</li> <li>[ ] Run <code>opa test</code> on policy changes</li> <li> <p>[ ] Evaluate policies with <code>--fail-defined</code> to block merges</p> </li> <li> <p>[ ] Pre-commit Hooks:</p> </li> <li>[ ] Local policy validation before push</li> <li>[ ] AST extraction for changed files</li> <li> <p>[ ] Fast-fail on critical violations</p> </li> <li> <p>[ ] Documentation:</p> </li> <li>[ ] Policy authoring guide</li> <li>[ ] CI/CD integration examples (GitHub, GitLab, Jenkins)</li> <li>[ ] Troubleshooting guide</li> </ul>"},{"location":"development/roadmap/#phase-2-graphrag-intelligence-layer-months-4-6","title":"Phase 2: GraphRAG Intelligence Layer (Months 4-6)","text":""},{"location":"development/roadmap/#python-graphrag-pipeline-weeks-13-18","title":"Python GraphRAG Pipeline (Weeks 13-18)","text":"<ul> <li>[ ] FastAPI Service:</li> <li>[ ] HTTP/REST API endpoints</li> <li>[ ] Bearer token authentication</li> <li>[ ] Rate limiting (per-user quotas)</li> <li> <p>[ ] OpenAPI/Swagger documentation</p> </li> <li> <p>[ ] GraphRAG 7-Phase Indexing:</p> </li> <li>[ ] Phase 1: Text chunking (300-600 tokens for code, configurable overlap)</li> <li>[ ] Phase 2: Graph extraction (LLM-powered entity/relationship extraction)<ul> <li>[ ] Custom entity types: <code>Service</code>, <code>API</code>, <code>Module</code>, <code>Component</code>, <code>Team</code>, <code>Function</code>, <code>Class</code></li> <li>[ ] Custom relationships: <code>depends_on</code>, <code>calls</code>, <code>imports</code>, <code>owns</code>, <code>implements</code></li> </ul> </li> <li>[ ] Phase 3: Graph augmentation (Hierarchical Leiden Algorithm via <code>graspologic</code>)</li> <li>[ ] Phase 4: Community summarization (LLM-generated cluster reports)</li> <li>[ ] Phase 5: Document linking (provenance tracking)</li> <li>[ ] Phase 6: Node2Vec + UMAP embeddings (visualization)</li> <li> <p>[ ] Phase 7: Text embeddings (vector search via Qdrant)</p> </li> <li> <p>[ ] LLM Integration (<code>vLLM</code> v0.6.x):</p> </li> <li>[ ] OpenAI-compatible endpoint</li> <li>[ ] Model: gpt-4o-mini (cost optimization: ~$0.01 per pass)</li> <li>[ ] LLM response caching (keyed by <code>(chunk_hash, model_version, prompt_version)</code>)</li> <li>[ ] Exponential backoff with jitter (<code>tenacity</code>)</li> <li> <p>[ ] Circuit breaking (<code>pybreaker</code>)</p> </li> <li> <p>[ ] Qdrant Vector Store:</p> </li> <li>[ ] Three collections: <code>entity_embeddings</code>, <code>text_unit_embeddings</code>, <code>community_report_embeddings</code></li> <li>[ ] Custom GraphRAG vector store implementation (<code>graphrag/vector_stores/factory.py</code>)</li> <li> <p>[ ] Hybrid query pattern: Qdrant seed \u2192 Neo4j expansion \u2192 LLM generation</p> </li> <li> <p>[ ] Workflow Orchestration (Prefect v3.x):</p> </li> <li>[ ] Incremental indexing (track file hashes/timestamps in PostgreSQL)</li> <li>[ ] Per-service re-indexing (avoid full corpus re-indexing)</li> <li> <p>[ ] Retry logic, caching, observability</p> </li> <li> <p>[ ] Query Strategies:</p> </li> <li>[ ] Local Search: Entity similarity \u2192 graph neighborhood traversal</li> <li>[ ] Global Search: Map-reduce across community reports</li> <li>[ ] DRIFT Search: Primer (community summaries) \u2192 Follow-up (local entities) \u2192 Output (ranked hierarchy)</li> </ul>"},{"location":"development/roadmap/#neo4j-integration-weeks-19-20","title":"Neo4j Integration (Weeks 19-20)","text":"<ul> <li>[ ] GraphRAG \u2192 Neo4j Export:</li> <li>[ ] Cypher <code>LOAD CSV</code> for parquet output</li> <li>[ ] OR <code>neo4j-graphrag-python</code> package's <code>SimpleKGPipeline</code></li> <li> <p>[ ] Data model:</p> <ul> <li>Entities \u2192 typed nodes (<code>:Service</code>, <code>:API</code>, etc.)</li> <li>Relationships \u2192 typed edges</li> <li>Communities \u2192 <code>:Community</code> nodes</li> <li>TextUnits \u2192 <code>:TextUnit</code> nodes</li> </ul> </li> <li> <p>[ ] Query Optimization:</p> </li> <li>[ ] Index creation (entity IDs, relationship types)</li> <li>[ ] Query profiling (target: &lt;5ms for 1-5 hop traversals)</li> </ul>"},{"location":"development/roadmap/#staleness-detection-weeks-21-22","title":"Staleness Detection (Weeks 21-22)","text":"<ul> <li>[ ] Exponential Decay Model:</li> <li>[ ] <code>Freshness(t) = e^(-\u03bb \u00d7 \u0394t)</code> implementation</li> <li> <p>[ ] Decay rates: Architectural docs (\u03bb=0.01), API docs (\u03bb=0.05), Deployment guides (\u03bb=0.1)</p> </li> <li> <p>[ ] Composite Staleness Score:</p> </li> <li>[ ] Time decay: 25%</li> <li>[ ] Code drift: 30% (tracked files changed, doc unchanged)</li> <li>[ ] Link rot: 15% (broken URLs via HTTP HEAD requests)</li> <li>[ ] Reference validity: 20% (check if referenced APIs/classes still exist)</li> <li> <p>[ ] Inverse view frequency: 10%</p> </li> <li> <p>[ ] Smart Notifications:</p> </li> <li>[ ] Dashboard flag at score &gt; 0.5</li> <li>[ ] Slack DM to document owner at day 3</li> <li>[ ] Team channel escalation at day 7</li> <li>[ ] Engineering manager report at day 14</li> <li> <p>[ ] Deprecation/archival suggestion at day 30</p> </li> <li> <p>[ ] Fatigue Prevention:</p> </li> <li>[ ] Batching into daily/weekly digests</li> <li>[ ] Cap of 3 push notifications/hour/user</li> <li>[ ] User-configurable notification preferences</li> </ul>"},{"location":"development/roadmap/#phase-3-advanced-frontend-months-7-9","title":"Phase 3: Advanced Frontend (Months 7-9)","text":""},{"location":"development/roadmap/#technology-migration-weeks-23-24","title":"Technology Migration (Weeks 23-24)","text":"<ul> <li>[ ] New Stack Setup:</li> <li>[ ] Vite 6 + React 19 project initialization</li> <li>[ ] shadcn/ui + Radix UI + Tailwind CSS 4.x installation</li> <li>[ ] TanStack Router configuration (file-based routing)</li> <li>[ ] Zustand 5.x for client state</li> <li>[ ] TanStack Query 5.x for server state</li> <li> <p>[ ] Zod 3.x for validation</p> </li> <li> <p>[ ] Component Library:</p> </li> <li>[ ] Dark mode theme system (built-in with shadcn/ui)</li> <li>[ ] Accessible form components (Radix UI primitives)</li> <li>[ ] Loading states, skeletons, error boundaries</li> </ul>"},{"location":"development/roadmap/#graph-visualization-weeks-25-28","title":"Graph Visualization (Weeks 25-28)","text":"<ul> <li>[ ] Sigma.js Integration (v3.0.2):</li> <li>[ ] <code>@react-sigma/core</code> wrapper component</li> <li>[ ] WebGL rendering for 100K+ nodes at 60fps</li> <li>[ ] ForceAtlas2 layout with Web Worker offload</li> <li>[ ] Custom node renderers (health indicators, traffic light coloring)</li> <li> <p>[ ] Reducers for dynamic filtering</p> </li> <li> <p>[ ] Cytoscape.js Integration (v3.x):</p> </li> <li>[ ] <code>react-cytoscapejs</code> wrapper component</li> <li>[ ] Layout algorithms: Dagre (DAG flows), CoSE-Bilkent (compound graphs), Hierarchical</li> <li>[ ] Compound nodes (group services by team/domain)</li> <li>[ ] Edge styling (thickness by traffic, color by health status)</li> <li> <p>[ ] CSS-like selectors for styling</p> </li> <li> <p>[ ] Diff Visualization:</p> </li> <li>[ ] Overlay two graph states</li> <li>[ ] Color coding: Green (additions), Red (removals), Yellow (modifications)</li> <li> <p>[ ] Sigma.js reducer-based implementation</p> </li> <li> <p>[ ] Performance Optimization:</p> </li> <li>[ ] Hybrid rendering: WebGL (overview) \u2192 Canvas (mid-range) \u2192 SVG (export)</li> <li>[ ] Lazy loading for heavy graph components</li> <li>[ ] <code>@tanstack/react-virtual</code> for long service lists</li> <li>[ ] Web Worker for layout computation</li> </ul>"},{"location":"development/roadmap/#dashboard-implementation-weeks-29-32","title":"Dashboard Implementation (Weeks 29-32)","text":"<ul> <li>[ ] Card-Based KPI Layout:</li> <li>[ ] Top Row: Total Services, Compliance %, Active Drift Alerts, Deployment Frequency</li> <li>[ ] Trend Charts (Recharts 2.x): Time-series over sprints</li> <li>[ ] Heatmaps (Nivo): Dependency risk matrices</li> <li> <p>[ ] Radar Charts (Nivo): Team comparisons</p> </li> <li> <p>[ ] Key Views:</p> </li> <li>[ ] Architecture Health Overview (composite score A-F with traffic light)</li> <li>[ ] Drift Detection Alerts (real-time WebSocket updates)</li> <li>[ ] Team Ownership Matrices (who owns what)</li> <li>[ ] Service Catalog (Backstage-inspired entity model)</li> <li> <p>[ ] Dependency Risk Heatmap (coupling \u00d7 instability)</p> </li> <li> <p>[ ] Real-time Updates:</p> </li> <li>[ ] TanStack Query polling (<code>refetchInterval: 30000</code>)</li> <li>[ ] WebSocket integration via Zustand for live alerts</li> <li>[ ] Optimistic mutations for policy updates</li> </ul>"},{"location":"development/roadmap/#settings-configuration-weeks-33-34","title":"Settings &amp; Configuration (Weeks 33-34)","text":"<ul> <li>[ ] LLM Configuration:</li> <li>[ ] Provider selector (OpenAI, Anthropic, Azure OpenAI, Ollama, Custom)</li> <li>[ ] Masked API key input with show/hide toggle</li> <li>[ ] Model selection dropdown (validated via backend)</li> <li>[ ] Parameter sliders (temperature, max tokens)</li> <li> <p>[ ] \"Test Connection\" button (latency + model availability display)</p> </li> <li> <p>[ ] Data Source Connectors:</p> </li> <li>[ ] OAuth 2.0 popup flows (GitHub, Jira, Confluence)</li> <li>[ ] Connection status indicators (Connected/Disconnected/Error/Syncing)</li> <li>[ ] Sync scheduling dropdowns (15min, 30min, 1hr, 4hr, daily)</li> <li> <p>[ ] Repository selection interface (multi-select for GitHub repos)</p> </li> <li> <p>[ ] User Preferences:</p> </li> <li>[ ] Notification settings (email, Slack, push)</li> <li>[ ] Theme toggle (dark/light mode)</li> <li>[ ] Language selection (i18n preparation)</li> </ul>"},{"location":"development/roadmap/#phase-4-enterprise-features-months-10-12","title":"Phase 4: Enterprise Features (Months 10-12)","text":""},{"location":"development/roadmap/#multi-tenancy-rbac-weeks-35-38","title":"Multi-Tenancy &amp; RBAC (Weeks 35-38)","text":"<ul> <li>[ ] Tenant Isolation:</li> <li>[ ] PostgreSQL row-level security (RLS)</li> <li>[ ] Neo4j subgraph filtering by tenant ID</li> <li> <p>[ ] Qdrant collection-per-tenant or metadata filtering</p> </li> <li> <p>[ ] Role-Based Access Control:</p> </li> <li>[ ] Roles: Admin, Architect, Developer, Viewer</li> <li>[ ] Permissions: Create policies, view violations, acknowledge violations, configure connectors</li> <li>[ ] Team-based ownership (cross-team dependency approval workflows)</li> </ul>"},{"location":"development/roadmap/#authentication-sso-weeks-39-40","title":"Authentication &amp; SSO (Weeks 39-40)","text":"<ul> <li>[ ] OAuth 2.0 / OIDC:</li> <li>[ ] GitHub, Google, Okta, Azure AD providers</li> <li>[ ] JWT token validation</li> <li> <p>[ ] Session management (Redis-backed)</p> </li> <li> <p>[ ] SAML 2.0 (Enterprise Tier):</p> </li> <li>[ ] SAML assertion parsing</li> <li>[ ] IdP metadata configuration</li> <li>[ ] Just-In-Time (JIT) provisioning</li> </ul>"},{"location":"development/roadmap/#audit-logging-week-41","title":"Audit Logging (Week 41)","text":"<ul> <li>[ ] Immutable Audit Trail:</li> <li>[ ] PostgreSQL <code>audit_logs</code> table (who, what, when, where)</li> <li>[ ] Log entries: Policy creation/update, violation acknowledgment, connector configuration</li> <li>[ ] Searchable audit log UI (filterable by user, action, date range)</li> </ul>"},{"location":"development/roadmap/#advanced-connectors-weeks-42-44","title":"Advanced Connectors (Weeks 42-44)","text":"<ul> <li>[ ] Jira Connector:</li> <li>[ ] REST API v3 integration (<code>/rest/api/3/search/jql</code>)</li> <li>[ ] Extract: issues, epics, sprints, components, issue links</li> <li> <p>[ ] Sequential pagination with <code>nextPageToken</code></p> </li> <li> <p>[ ] Confluence Connector:</p> </li> <li>[ ] CQL search (<code>type=page AND space=ARCH AND lastModified &gt;= \"...\"</code>)</li> <li>[ ] Confluence Storage Format parsing (strip <code>ac:</code> macros)</li> <li> <p>[ ] Pagination (50 results/page max with body expansion)</p> </li> <li> <p>[ ] Slack Connector:</p> </li> <li>[ ] Socket Mode (Bolt framework)</li> <li>[ ] Events: <code>message</code>, <code>reaction_added</code>, <code>file_shared</code></li> <li> <p>[ ] Filter by designated \"knowledge\" channels</p> </li> <li> <p>[ ] Hybrid Sync Strategy:</p> </li> <li>[ ] Webhooks for real-time event capture</li> <li>[ ] Reconciliation polling (every 15-60 minutes)</li> <li>[ ] Full polling backfill on initial setup</li> </ul>"},{"location":"development/roadmap/#entity-resolution-weeks-45-46","title":"Entity Resolution (Weeks 45-46)","text":"<ul> <li>[ ] Three-Layer Resolution:</li> <li>[ ] Deterministic matching (email/SSO ID/username mapping)</li> <li>[ ] Probabilistic matching (Fellegi-Sunter model via Splink v4.x)</li> <li> <p>[ ] ML-based matching (active learning)</p> </li> <li> <p>[ ] Canonical Entity Model:</p> </li> <li>[ ] Store all identifiers (GitHub username, Jira account ID, Slack user ID, Confluence author)</li> <li>[ ] Resolution confidence scores</li> <li>[ ] Entity evolution tracking (handle name changes, role changes)</li> </ul>"},{"location":"development/roadmap/#dora-metrics-integration-week-47","title":"DORA Metrics Integration (Week 47)","text":"<ul> <li>[ ] GitLab Integration:</li> <li> <p>[ ] Native DORA API (<code>/api/v4/projects/{id}/dora/metrics</code>)</p> </li> <li> <p>[ ] GitHub Integration:</p> </li> <li>[ ] Workflow Runs API for deployment frequency, lead time</li> <li>[ ] Pull Request API for change failure rate</li> <li> <p>[ ] Issue API for MTTR</p> </li> <li> <p>[ ] Apache DevLake (Optional):</p> </li> <li>[ ] Multi-platform DORA metric collection</li> <li>[ ] Unified data model</li> </ul>"},{"location":"development/roadmap/#compliance-prep-week-48","title":"Compliance Prep (Week 48)","text":"<ul> <li>[ ] SOC 2 Type II Preparation:</li> <li>[ ] Security policy documentation</li> <li>[ ] Access control reviews</li> <li>[ ] Incident response plan</li> <li> <p>[ ] Vendor risk management</p> </li> <li> <p>[ ] GDPR Compliance:</p> </li> <li>[ ] Data processing agreements (DPAs)</li> <li>[ ] Right to erasure (user data deletion)</li> <li>[ ] Data portability (export user data)</li> </ul>"},{"location":"development/roadmap/#phase-5-scale-optimization-months-13-18","title":"Phase 5: Scale &amp; Optimization (Months 13-18)","text":""},{"location":"development/roadmap/#kubernetes-migration-months-13-14","title":"Kubernetes Migration (Months 13-14)","text":"<ul> <li>[ ] Helm Charts:</li> <li>[ ] PostgreSQL, Neo4j, Qdrant, Redis StatefulSets</li> <li>[ ] Rust ingestion, Python GraphRAG, Go governance Deployments</li> <li> <p>[ ] NATS JetStream \u2192 Kafka migration (if throughput &gt; 200K msg/s)</p> </li> <li> <p>[ ] Horizontal Scaling:</p> </li> <li>[ ] Ingestion pipeline: Scale parser pool replicas</li> <li>[ ] GraphRAG: Celery worker auto-scaling</li> <li>[ ] Governance: Stateless OPA service replicas</li> </ul>"},{"location":"development/roadmap/#debezium-cdc-months-15-16","title":"Debezium CDC (Months 15-16)","text":"<ul> <li>[ ] PostgreSQL Configuration:</li> <li>[ ] <code>wal_level = logical</code></li> <li>[ ] <code>max_replication_slots = 4</code></li> <li> <p>[ ] <code>REPLICA IDENTITY FULL</code> on tracked tables</p> </li> <li> <p>[ ] Kafka Connect:</p> </li> <li>[ ] Debezium PostgreSQL connector</li> <li>[ ] Stream WAL changes to Kafka topics</li> <li>[ ] Custom consumers for Neo4j/Qdrant updates</li> </ul>"},{"location":"development/roadmap/#opa-policy-data-sync-month-17","title":"OPA Policy Data Sync (Month 17)","text":"<ul> <li>[ ] OPAL Integration (Open Policy Administration Layer):</li> <li>[ ] Real-time event-driven policy data synchronization</li> <li>[ ] Neo4j \u2192 OPA sync on graph changes</li> <li>[ ] Target latency: &lt;100ms policy evaluation</li> </ul>"},{"location":"development/roadmap/#observability-monitoring-month-18","title":"Observability &amp; Monitoring (Month 18)","text":"<ul> <li>[ ] Metrics:</li> <li>[ ] Prometheus exporters (ingestion throughput, policy latency, LLM API costs)</li> <li> <p>[ ] Grafana dashboards (system health, SLA tracking)</p> </li> <li> <p>[ ] Logging:</p> </li> <li>[ ] Centralized logging (ELK stack or Loki)</li> <li> <p>[ ] Structured JSON logs with correlation IDs</p> </li> <li> <p>[ ] Tracing:</p> </li> <li>[ ] OpenTelemetry distributed tracing</li> <li>[ ] Trace ingestion \u2192 GraphRAG \u2192 governance \u2192 response path</li> </ul>"},{"location":"development/roadmap/#beyond-months-18","title":"Beyond (Months 18+)","text":""},{"location":"development/roadmap/#advanced-features","title":"Advanced Features","text":"<ul> <li>[ ] Natural Language to Rego Translation:</li> <li>[ ] LLM-assisted policy authoring (human review + automated <code>opa test</code>)</li> <li> <p>[ ] LACE framework integration (if production-ready)</p> </li> <li> <p>[ ] Architecture Health Scoring:</p> </li> <li>[ ] Composite score: Architectural conformance (0.25) + Documentation health (0.15) + Intent-reality alignment (0.20) + DORA (0.15) + Technical debt (0.15) + Knowledge graph quality (0.10)</li> <li> <p>[ ] Letter ratings: A (\u22650.9), B (\u22650.75), C (\u22650.6), D (\u22650.4), F (&lt;0.4)</p> </li> <li> <p>[ ] Human-in-the-Loop Enrichment:</p> </li> <li>[ ] Confidence thresholds: &gt;0.9 auto-add, 0.7-0.9 queue for review, &lt;0.7 discard</li> <li>[ ] PR-triggered prompts for doc reviews</li> <li> <p>[ ] Acceptance rate feedback loop for model tuning</p> </li> <li> <p>[ ] Embedding-Based Duplicate Detection:</p> </li> <li>[ ] Cosine similarity on document embeddings</li> <li>[ ] Thresholds: &gt;0.95 auto-flag merge, 0.85-0.95 suggest consolidation, 0.70-0.85 cross-link</li> <li>[ ] HDBSCAN clustering for topic consolidation</li> </ul>"},{"location":"development/roadmap/#enterprise-scaling","title":"Enterprise Scaling","text":"<ul> <li>[ ] Multi-Cluster Kubernetes:</li> <li>[ ] Kafka with multiple topic partitions</li> <li>[ ] Neo4j Fabric for federation</li> <li> <p>[ ] Qdrant distributed mode</p> </li> <li> <p>[ ] Enterprise OPA (Styra DAS):</p> </li> <li>[ ] Policy lifecycle management</li> <li>[ ] Impact analysis for policy changes</li> <li>[ ] 10\u00d7 less memory, 40% less CPU vs open-source OPA</li> </ul>"},{"location":"development/roadmap/#industry-specific-solutions","title":"Industry-Specific Solutions","text":"<ul> <li>[ ] FinTech: PCI DSS compliance, payment flow validation</li> <li>[ ] Healthcare: HIPAA compliance, PHI access auditing</li> <li>[ ] Government: FedRAMP Moderate, ITAR controls</li> </ul>"},{"location":"development/roadmap/#success-criteria","title":"Success Criteria","text":""},{"location":"development/roadmap/#phase-1-months-0-3","title":"Phase 1 (Months 0-3)","text":"<ul> <li>\u2705 Rust ingestion pipeline processes 2,157 lines in &lt;10ms</li> <li>\u2705 OPA policies evaluate in &lt;100ms</li> <li>\u2705 CI/CD integration blocks PRs on violations</li> <li>\u2705 5 pre-built policies (circular deps, layer violations, etc.)</li> </ul>"},{"location":"development/roadmap/#phase-2-months-4-6","title":"Phase 2 (Months 4-6)","text":"<ul> <li>\u2705 GraphRAG indexing completes 100K-line codebase in &lt;10 minutes</li> <li>\u2705 Local/Global search returns results in &lt;3 seconds</li> <li>\u2705 Staleness detection flags 80%+ of outdated docs (precision &gt;85%)</li> <li>\u2705 LLM costs &lt;$50 per full re-index</li> </ul>"},{"location":"development/roadmap/#phase-3-months-7-9","title":"Phase 3 (Months 7-9)","text":"<ul> <li>\u2705 Sigma.js renders 10K+ nodes at 60fps</li> <li>\u2705 Dashboard loads in &lt;2 seconds</li> <li>\u2705 Real-time drift alerts arrive in &lt;5 seconds</li> <li>\u2705 Dark mode + responsive design works on mobile</li> </ul>"},{"location":"development/roadmap/#phase-4-months-10-12","title":"Phase 4 (Months 10-12)","text":"<ul> <li>\u2705 Multi-tenant isolation validated (penetration testing)</li> <li>\u2705 SAML SSO functional with 3+ IdPs</li> <li>\u2705 Audit log captures 100% of sensitive actions</li> <li>\u2705 SOC 2 Type I audit initiated</li> </ul>"},{"location":"development/roadmap/#phase-5-months-13-18","title":"Phase 5 (Months 13-18)","text":"<ul> <li>\u2705 Kubernetes deployment supports 100+ devs</li> <li>\u2705 Debezium CDC &lt;1s latency PostgreSQL \u2192 Neo4j</li> <li>\u2705 System handles 1M+ graph nodes, 10M+ edges</li> <li>\u2705 99.9% uptime SLA achieved</li> </ul>"},{"location":"development/roadmap/#notes","title":"Notes","text":"<ul> <li>Iterative Delivery: Each phase ships a working increment; no \"big bang\" release</li> <li>Dogfooding: Use the platform to govern its own development (meta-governance)</li> <li>Community Feedback: Open-source components first for early feedback</li> <li>Documentation: Keep <code>blueprint.md</code> as the source of truth; update as architecture evolves</li> </ul>"},{"location":"product/faq/","title":"FAQ","text":"<p>Invariant Continuum Technologies:  FOUNDATIONAL QUESTIONS 1. What problem are you solving? As AI accelerates software development velocity by 3-5x, the gap between \"what we designed\" and \"what actually runs\" is widening at machine speed. AI code assistants generate syntactically correct but architecturally incoherent code\u2014creating \"AI slop\" that bypasses design patterns, violates layer boundaries, and introduces shadow dependencies invisible to traditional linters. This isn't just technical debt\u2014it's structural entropy that makes systems unmaintainable, unauditable, and unreliable. The same problem exists in supply chains (digital twins drift from physical reality), clinical trials (protocol deviations), and autonomous systems (AI agents violate safety constraints). The core problem: Reality continuously diverges from intent across all complex adaptive systems, and existing tools can only detect symptoms after damage occurs.</p> <ol> <li> <p>Who is your customer? Primary Persona (Year 1-2): VP Engineering / Head of Platform Engineering Company size: 50-500 engineers Tech stack: Modern (TypeScript, Python, microservices, K8s) Pain: \"Our codebase quality collapsed after adopting GitHub Copilot. Manual code reviews can't keep up with AI-generated PRs.\" Budget authority: $50k-200k annual software spend Decision timeline: 3-6 months Secondary Persona (Year 3+): CIO / VP Operations Company size: F500, regulated industries (pharma, manufacturing, logistics) Pain: \"We have no way to ensure our AI agents and autonomous systems follow compliance rules in real-time.\" Budget authority: $500k-2M annual spend Decision timeline: 12-18 months Expansion Personas: CISO (security compliance angle) Director of Supply Chain (logistics governance) VP Clinical Operations (trial protocol enforcement)</p> </li> <li> <p>What is your solution? The Continuum Graph Platform: A knowledge graph infrastructure that ingests multi-modal data (code, docs, tickets, runtime state, external signals), constructs a unified semantic graph, and enables: Observation: Continuous monitoring of reality vs. intent gaps Reasoning: GraphRAG-powered analysis of structural violations Governance: Policy-as-code enforcement before changes reach production Action: Automated remediation and human-in-loop escalation Delivered as 6 products (built on one platform): CodeGraft: AI code governance (blocks architectural drift in PRs) Chronicle: Institutional memory (captures tacit knowledge) Sentinel: Supply chain integrity (governs AI logistics twins) TrialGuard: Clinical compliance (prevents protocol deviations) Nexus EA: Enterprise architecture (living, auto-updated topology) Nexus Agent: Robotics safety (validates AI agent actions) Core differentiation: We don't just monitor\u2014we govern. We don't just detect drift\u2014we prevent it. We don't replace existing tools\u2014we create a governance layer above them.</p> </li> <li> <p>How big is the market? TAM (Total Addressable Market): ~$15-20B by 2030 Calculated from adjacent markets we disrupt: Internal Developer Platforms: $2.1B (Backstage ecosystem) SAST/DAST/ASPM: $8.4B (SonarQube, Snyk, Wiz) Enterprise Architecture: $1.8B (LeanIX, Ardoq) Supply Chain Software: $37B (we target 5% = $1.85B) Clinical Trial Tech: $2.1B (we target compliance subset = $400M) Robotics Software: $9.7B (safety/governance = 10% = $970M) SAM (Serviceable Addressable Market): ~$5B by 2030 Companies with &gt;50 engineers using AI code tools F500 with complex supply chains deploying AI orchestration Pharma running decentralized clinical trials Manufacturers deploying collaborative robotics SOM (Serviceable Obtainable Market): ~$150-300M by Year 5 Realistic market share: 2-5% of SAM Based on: Platform engineering is nascent category (early mover advantage)</p> </li> <li> <p>What's your business model? Hybrid: Usage-based + Enterprise licensing CodeGraft/Chronicle (SMB/Mid-market): Free: Public repos, community features Pro: $39/repo/month or $499/month unlimited Team: $79/user/month (adds governance features) Revenue model: Land with free, expand to paid, upsell to Team Sentinel/TrialGuard/Nexus (Enterprise): Platform fee: $100k-500k annual base Usage tier: Per-transaction or per-asset pricing Professional services: 20-30% of license (implementation, training) Revenue model: Top-down sales, multi-year contracts Platform licensing (ISVs): White-label: Partners embed our graph platform Revenue share: 20-30% of partner's sales Targets: DevOps tool vendors, supply chain software companies Gross margin targets: Year 1-2: 40% (high AI costs, infrastructure learning curve) Year 3-5: 60-70% (economies of scale, local models, optimizations)</p> </li> <li> <p>Who are your competitors? We don't have direct competitors\u2014we have partial overlaps: Category Players What They Do What We Do Differently IDPs Backstage, Port, Cortex Catalog services (passive) Enforce architecture (active) SAST SonarQube, Snyk, Semgrep Find bugs, vulnerabilities Detect structural violations CSPM Wiz, Orca, Lacework Secure cloud infra Govern architecture correctness EA Tools LeanIX, Ardoq Strategic planning Continuous reality validation AI Code Copilot, Cody, Cursor Generate code Govern generated code</p> </li> </ol> <p>Why we win: Cross-domain knowledge graph: Nobody spans code + ops + supply chain + robotics GraphRAG precision: Semantic + structural reasoning beats vector-only RAG Active governance: We block violations, not just report them AI-native: Built for the AI era, not retrofitted Competitive moat evolution: Year 1: Technical (GraphRAG + Stack-Graphs implementation difficulty) Year 3: Data (customer knowledge graphs = switching costs) Year 5: Network (ecosystem of policies, connectors, integrations)</p> <ol> <li>What's your unfair advantage?</li> <li>Technical Architecture Moat Multi-model database (SurrealDB): Competitors need 3 databases (Neo4j + Pinecone + Postgres); we use one Stack-Graphs: Compiler-level precision on code dependencies (competitors use fuzzy matching) GraphRAG: We have 6-12 month head start on implementation vs. anyone trying to catch up</li> <li>Founder Expertise (hypothetical\u2014customize to your team) Deep systems experience (distributed systems, compilers, databases) Domain expertise in target verticals (supply chain, healthcare, robotics) Previous exits or successful product launches</li> <li>Timing Advantage AI slop crisis is now (2025-2026)\u2014first mover captures market definition Platform engineering budgets exist now (not 3 years ago) Regulatory pressure is rising (DORA, AI Act, FDA digital health guidance)</li> <li> <p>Platform Leverage Build once (Continuum Graph), deploy 6x (all products) Each product makes others more valuable (institutional memory feeds governance) Competitors must build point solutions; we deliver platform</p> </li> <li> <p>What are your traction metrics? Current State (Pre-seed / Seed stage assumptions): Product: MVP in development Customers: 0 paying, 3-5 design partners committed Revenue: $0 ARR Team: Founders + 2-4 early engineers Funding: Bootstrapped or $500k-1M friends &amp; family Target Metrics by Stage: Year 1 (Post-Seed): Customers: 10 paying, 50 free tier active users ARR: $100-150k Churn: &lt;10% monthly (acceptable for early stage) NPS: &gt;40 (product-market fit indicator) Expansion rate: 2-3 customers upsell from free to paid Year 2 (Series A): Customers: 50-75 paying ARR: $500k-1M Net Dollar Retention: &gt;110% (expansion &gt; churn) CAC Payback: &lt;12 months Team: 20-25 people Year 3 (Series B): Customers: 100-150 paying ARR: $3-5M NDR: &gt;120% Enterprise customers: 5-10 (&gt;$100k ACV each) Logo retention: &gt;90%</p> </li> <li> <p>What's your team? Founding Team (Ideal composition): CEO (You?): Background: Product management, enterprise sales, or technical founder with business acumen Responsibilities: Vision, fundraising, GTM strategy, customer development CTO: Background: Distributed systems, databases, or compiler engineering Responsibilities: Platform architecture, technical hiring, R&amp;D roadmap Chief Scientist / AI Lead (optional for Seed, required by Series A): Background: PhD in ML/NLP, GraphRAG research, or production LLM deployments Responsibilities: GraphRAG implementation, AI accuracy, hallucination reduction Early Hires (Year 1): Backend Engineer (Rust, graph databases) Frontend Engineer (React, WebGL visualization) AI/ML Engineer (LLM integration, embeddings) DevRel / Sales Engineer (developer adoption, design partners) Advisory Board: Enterprise architect from F500 (validates product vision) CISO from regulated industry (compliance guidance) Supply chain expert (Sentinel product validation) Open-source leader (community credibility)</p> </li> <li> <p>How much are you raising? Seed Round (Year 1): $2-3M Runway: 18-24 months Milestones: Ship CodeGraft MVP Sign 10 paying customers Reach $100k ARR Validate GraphRAG accuracy &gt;85% Use of funds: Engineering: 60% ($1.2-1.8M) \u2192 6-8 engineers Infrastructure: 15% ($300-450k) \u2192 AWS/GCP, LLM costs GTM: 15% ($300-450k) \u2192 DevRel, marketing, sales Operations: 10% ($200-300k) \u2192 Legal, finance, admin Series A (Year 2): $5-8M Runway: 18-24 months Milestones: Launch Chronicle $1M ARR 50+ paying customers Prove multi-product strategy Use of funds: Engineering: 50% (scale platform, build products) Sales &amp; Marketing: 35% (hire AEs, demand gen) Operations: 15% Series B (Year 3): $15-25M Runway: 24+ months to profitability Milestones: Launch Sentinel (enterprise product) $5M ARR Enterprise customer wins Platform ecosystem (partners, marketplace) Use of funds: GTM: 50% (enterprise sales team, marketing) Engineering: 30% (product expansion) International: 10% (EU expansion) Operations: 10%</p> </li> </ol> <p>STRATEGIC QUESTIONS 11. What's your go-to-market strategy? Phase 1: Developer-Led (Year 1-2) Bottom-up adoption through free tier Content marketing (engineering blogs, case studies) Community building (Discord, GitHub discussions) Product-led growth (self-service signup, instant value) Developer advocates at conferences (KubeCon, QCon, etc.) Channels: Product Hunt launch Hacker News (Show HN: CodeGraft) Engineering podcasts (Software Engineering Daily, Changelog) GitHub stars / open-source strategy Technical blog (SEO for \"architectural drift\", \"AI code quality\") Phase 2: Sales-Assisted (Year 2-3) Hire first Account Executives Outbound to platform engineering teams Expand within existing accounts (land with CodeGraft, expand to Chronicle) Partner with system integrators Phase 3: Enterprise (Year 3+) Direct sales to F500 Industry-specific GTM (pharma for TrialGuard, logistics for Sentinel) RFP responses for large deals Executive relationships (CIO/CTO level)</p> <ol> <li> <p>What's your product roadmap priority? Year 1: Platform + CodeGraft Q1-Q2: Continuum Graph Platform foundation Q3-Q4: CodeGraft MVP (TypeScript/Python) Goal: Prove the platform can power real products Year 2: Multi-product validation Q1-Q2: Chronicle launch (knowledge capture) Q3-Q4: CodeGraft expansion (Java, Go, policy marketplace) Goal: Prove platform leverage (one foundation, multiple products) Year 3: Enterprise expansion Q1-Q2: Sentinel development Q3-Q4: First enterprise pilots Goal: Prove vertical expansion into non-software domains Year 4-5: Complete portfolio TrialGuard (clinical trials) Nexus EA (enterprise architecture) Nexus Agent (robotics governance) Goal: Become horizontal infrastructure layer Guiding Principles: Build platform capabilities only when 2+ products need them (avoid over-engineering) Launch products sequentially (no parallel development) Validate each product before starting next (discipline over ambition)</p> </li> <li> <p>What could kill this company? Execution Risks:</p> </li> <li>Platform complexity spiral Risk: Building database + AI + visualization = 3 startups Mitigation: Use off-the-shelf where possible (SurrealDB not custom DB, OpenAI not custom LLMs initially)</li> <li>AI accuracy failure Risk: If GraphRAG is &lt;80% accurate, customers won't trust it Mitigation: Human-in-loop validation, confidence scoring, show-your-work evidence linking</li> <li>Cold start problem Risk: Platform needs data to be valuable, but customers won't give data until they see value Mitigation: Free tier with instant value (doc search works on day 1), gradual quality improvements Market Risks:</li> <li>Category education exhaustion Risk: \"Structural integrity\" is not a known category\u2014market education is expensive Mitigation: Lead with known pain (\"stop AI slop\") not category (\"structural integrity platform\")</li> <li>Competitor response Risk: GitHub/Microsoft adds architectural linting to Copilot Mitigation: Our graph knowledge is deeper (we see across repos, tools, domains)</li> <li>Economic downturn Risk: Developer tools budgets get cut first in recession Mitigation: Focus on compliance use cases (non-discretionary), ROI calculators Operational Risks:</li> <li>Key person dependency Risk: If CTO leaves, GraphRAG expertise walks out door Mitigation: Documentation, knowledge sharing, pair programming</li> <li> <p>Infrastructure costs Risk: LLM API costs scale faster than revenue Mitigation: Local models, aggressive caching, pricing covers costs + margin</p> </li> <li> <p>What's your exit strategy? Likely Acquirers (5-7 year horizon): Strategic Buyers: Microsoft/GitHub</p> </li> </ol> <p>Why: Add governance layer to Copilot Rationale: They create the AI slop problem; we solve it Comparable: GitHub acquired Dependabot ($100M+), Semmle ($200M+) GitLab / Atlassian</p> <p>Why: Complete their DevOps platform Rationale: They have pipelines, we have architectural intelligence Comparable: Atlassian acquired OpsGenie ($295M), Halp ($100M+) Datadog / New Relic</p> <p>Why: Expand from runtime observability to structural observability Rationale: \"The Datadog of architectural integrity\" Comparable: Datadog acquired Sqreen ($200M+), Timber ($50M+) HashiCorp / Pulumi</p> <p>Why: Add governance to infrastructure-as-code Rationale: They provision infrastructure; we govern it Comparable: HashiCorp IPO'd ($14B valuation) SAP / Oracle</p> <p>Why: Add to enterprise architecture suite Rationale: Modernize LeanIX/EA offerings with AI Comparable: SAP acquired LeanIX ($300M) Financial Buyers: Private equity (Vista Equity, Thoma Bravo) if we reach $50M+ ARR with strong margins IPO Path (10+ year, ambitious): If we become the de facto standard for AI governance Requires: $100M+ ARR, 40%+ YoY growth, expanding TAM Comparable: Datadog IPO'd at $8B valuation (2019) Most Realistic Outcome (7-10 year): Strategic acquisition at $300M-1B valuation Assumes: $30-50M ARR, strong growth, category leadership Multiple: 10-20x ARR (typical for high-growth infrastructure software)</p> <ol> <li>Why now? Why you? Why Now (Market Timing): 2025-2026 is the inflection point: AI code generation crossed adoption threshold</li> </ol> <p>70% of developers use AI assistants (GitHub 2024 survey) AI-generated code % crossing 30-40% of total code written Quality crisis emerging (documented in research papers) Platform engineering budget shift</p> <p>Gartner predicts 80% of enterprises will have platform teams by 2026 Budget authority moving from DevOps (tactical) to Platform (strategic) Willingness to pay for developer productivity infrastructure Regulatory pressure accelerating</p> <p>EU AI Act (2024): Requires governance for high-risk AI systems DORA (2025): Financial services operational resilience mandates FDA guidance (2024): Digital health and clinical trial compliance NIST AI RMF: Framework for AI risk management Technology maturity</p> <p>GraphRAG algorithms published (Microsoft, 2024) Multi-model databases production-ready (SurrealDB stable) LLM costs dropping 10x/year (makes platform economics viable) WebGL performance enables complex visualization in browser Too early 3 years ago: AI code generation wasn't mainstream, platform engineering didn't exist, GraphRAG wasn't discovered Too late 3 years from now: Incumbents will have built governance into existing tools Why You (Team Differentiation): [Customize this based on actual founder background] Unique combination: Technical depth: Distributed systems experience to build the platform Domain expertise: Understanding of target verticals (supply chain, healthcare, etc.) Product intuition: Ability to simplify complexity into usable interfaces Market timing sense: Recognized the AI slop problem before it became mainstream Execution bias: Willing to start with brain-first MVP instead of perfect architecture This can only be built by someone who: Understands graph databases AND LLMs AND policy engines (rare combination) Has felt the pain personally (worked in complex codebases with drift) Can resist scope creep (platform companies fail by building everything)</p> <ol> <li>What are your key assumptions? What needs to be true? Technical Assumptions: \u2705 GraphRAG achieves &gt;85% accuracy on code architecture queries</li> </ol> <p>Test: Design partner validation in first 3 months If false: Fallback to simpler heuristics + manual curation \u2705 SurrealDB scales to 100M nodes per tenant</p> <p>Test: Benchmark testing with synthetic data If false: Migrate to PostgreSQL + Neo4j + Pinecone stack \u2705 Stack-Graphs covers 80% of codebases (TypeScript, Python, Java, Go)</p> <p>Test: Survey of design partners' tech stacks If false: Fall back to Tree-sitter (degraded accuracy) or manual annotation \u2705 LLM costs drop to $0.01/1M tokens by Year 3</p> <p>Test: Monitor OpenAI/Anthropic pricing trends If false: Migrate to open-source models (Llama, Mixtral) Market Assumptions: 5. \u2705 Developers perceive AI slop as a real problem worth paying to solve Test: Conversion rate from free to paid &gt;5% If false: Pivot positioning to compliance (CISOs buy, not developers) \u2705 Platform engineering budgets are growing (not a fad)</p> <p>Test: Analyst reports (Gartner, Forrester) show sustained investment If false: Sell to traditional DevOps/infrastructure teams instead \u2705 Customers willing to trust AI governance (not just human reviewers)</p> <p>Test: Enterprise pilots run AI-blocked PRs in production If false: Downgrade to advisory mode (recommendations, not blocking) Business Model Assumptions: 8. \u2705 CAC payback &lt;12 months for SMB/mid-market Test: Track first cohorts' payback period If false: Shift to enterprise-only (higher ACV justifies longer sales cycles) \u2705 Customers expand usage over time (NDR &gt;110%)</p> <p>Test: Year 1 customers add repos, users, or upgrade tiers If false: Focus on new logo acquisition (land-and-expand isn't working) \u2705 Platform creates defensibility (switching costs accumulate)</p> <p>Test: Customer churn &lt;5% annually by Year 3 If false: Open-source the platform (make it a standard, monetize services)</p> <ol> <li>What's your pricing philosophy? Core Principles:</li> <li>Align with value delivered CodeGraft: Per-repo pricing (value = codebase size/complexity) Chronicle: Per-user pricing (value = team knowledge capture) Enterprise products: Per-asset/transaction (value = scale)</li> <li>Start low to drive adoption, expand over time Free tier creates demand generation Pro tier ($39-79/month) captures SMB budgets Enterprise tier (custom) captures F500 budgets</li> <li>Transparent, predictable No hidden fees, no surprise overages Usage limits clearly communicated Annual contracts with monthly billing option</li> <li> <p>Expansion built into pricing Easy to start small (1 repo, 5 users) Natural growth triggers (add repos, add users, add products) Volume discounts kick in at scale (encourages consolidation) Pricing Evolution: Year 1-2: Land with low prices Goal: Adoption &gt; margin CodeGraft Pro: $39/month (below SonarQube, Snyk) Willing to lose money on small customers to build brand Year 3-4: Raise prices as value proven CodeGraft Pro: $79/month (2x increase) Justify with: Feature expansion, accuracy improvements, case studies Grandfather early customers at original price (loyalty reward) Year 5+: Premium positioning \"The structural integrity platform\" commands premium Enterprise deals: $500k-2M annually Competitive with Datadog, Wiz, LeanIX pricing</p> </li> <li> <p>What open-source strategy makes sense? Hybrid: Open Core Model Open Source (Free, MIT License): Continuum Graph Platform Core SurrealDB schema and query patterns Basic GraphRAG implementation Connector framework (SDK for writing custom connectors) Policy template library (community-contributed OPA policies) Why open-source the core: Developer trust: \"Not locked into proprietary graph format\" Community contributions: Free connectors, policies, bug fixes Market education: \"See how structural integrity works\" Talent acquisition: Contributors become employees Ecosystem creation: ISVs build on our platform Closed Source (Paid, Commercial): CodeGraft Studio (UI/UX layer) GraphRAG advanced features (multi-hop reasoning, confidence scoring) Enterprise connectors (SAP, Oracle, proprietary systems) Self-hosted deployment (Kubernetes Helm charts with licensing) Support &amp; SLAs Monetization Strategy: Open-source users \u2192 Convert to cloud-hosted paid tier (easier than self-hosting) Large enterprises \u2192 Pay for self-hosted license ISVs \u2192 Pay for commercial embedding license Example: Comparable to Supabase (open-source Postgres platform, monetize hosting) or Airbyte (open-source connectors, monetize cloud)</p> </li> <li> <p>What partnerships accelerate growth? Strategic Partnerships: Year 1-2: Integration Partners GitHub / GitLab</p> </li> </ol> <p>Integration: CodeGraft as GitHub Action / GitLab CI template Value: Featured in marketplace, co-marketing Revenue: None initially, builds distribution Datadog / New Relic</p> <p>Integration: Link runtime observability to architectural drift Value: \"See which architectures cause incidents\" Revenue: Referral fees (10-20%) Jira / Confluence (Atlassian)</p> <p>Integration: Chronicle captures Jira context, links to code Value: \"Close the requirements-to-code gap\" Revenue: Atlassian Marketplace listing (20% rev share) Year 3-4: Channel Partners 4. System Integrators (Accenture, Deloitte, Capgemini) Use case: Enterprise implementation services Value: Access to F500 CIOs, shorten sales cycles Revenue: 20-30% partner discount on licenses Cloud Providers (AWS, Azure, GCP) Use case: Co-sell to enterprise customers Value: Featured in marketplace, AWS/Azure credits for customers Revenue: Marketplace fees (3-5%), but worth it for distribution Year 5+: Ecosystem Partners 6. ISV Embedding (Supply chain software, robotics platforms) Use case: White-label Continuum Graph Platform Value: They get governance layer without building it Revenue: 20-30% of their product revenue Partnership Philosophy: Year 1-2: Give away integrations (build ecosystem) Year 3+: Monetize partnerships (established value) Avoid: Exclusivity deals (stay platform-agnostic)</p> <ol> <li>How do you define success? Quantitative Metrics: Year 1: Validation \u2705 10 paying customers \u2705 $100k ARR \u2705 &gt;85% GraphRAG accuracy \u2705 &lt;10% monthly churn \u2705 1 customer case study published Year 3: Growth \u2705 $3-5M ARR \u2705 100+ paying customers \u2705 &gt;120% Net Dollar Retention \u2705 5+ enterprise customers (&gt;$100k ACV) \u2705 3 products launched Year 5: Market Leadership \u2705 $20-30M ARR \u2705 40%+ market share in AI code governance \u2705 500+ customers across 6 products \u2705 Positive unit economics (path to profitability) \u2705 Category leader (\"the Datadog of structural integrity\") Qualitative Indicators: Developer Love: Developers advocate for CodeGraft in their companies GitHub stars &gt;10k Conference speaking invitations \"I can't imagine working without it\" testimonials Market Recognition: Gartner Magic Quadrant inclusion (Cool Vendor) Forrester Wave recognition \"Best DevOps Tool\" awards Analyst inquiries (not us pitching them) Strategic Value: Microsoft/GitHub reaches out about acquisition Customers block competitors (we're deeply embedded) ISVs ask to embed our platform Governments reference us in AI governance frameworks Team Culture: Glassdoor rating &gt;4.5 Employee retention &gt;90% Diverse team (gender, geography, background) High-talent density (would re-hire 95% of team)</li> </ol> <p>THE HARD QUESTIONS (Investor Diligence) 21. What would make you shut this down? Kill Criteria (18-month checkpoints): After 18 months, if we have: \u274c &lt;3 paying customers (no product-market fit) \u274c &lt;50% retention (customers don't see value) \u274c &lt;70% GraphRAG accuracy (tech doesn't work) \u274c Unable to raise Series A (market doesn't believe) Then we pivot or shut down. Pivot Options: Pivot to services: Become consulting company (manual architecture reviews) Pivot to narrower scope: Just CodeGraft (drop multi-product vision) Pivot to acqui-hire: Sell team to GitHub/Microsoft Return capital: Shut down, return remaining capital to investors What would change our minds: If 1 customer is wildly successful (shows product works, just need more distribution) If technology breakthrough occurs (e.g., GPT-5 makes GraphRAG 10x better) If regulatory mandate creates forced demand (e.g., EU requires AI governance)</p> <ol> <li>What keeps you up at night?</li> <li>The cold start problem Platforms need data to be valuable, but empty platforms have no value Chicken-egg: Need customers to populate graph, but graph is empty without customers</li> <li>AI accuracy ceiling What if 85% accuracy is the limit? Is that good enough? One bad recommendation that causes production outage could kill company reputation</li> <li>Market timing mis-read What if \"AI slop\" panic is overhyped? What if developers don't actually care? What if platform engineering is a fad, budgets dry up in 2027?</li> <li>Execution complexity Building a database + AI + visualization + 6 products = insane scope Risk of shipping nothing because we're building everything</li> <li>Talent retention If founding engineer leaves after 12 months, product timeline slips 6+ months Hard to hire graph database + LLM + policy engine experts (tiny talent pool)</li> <li>Competitor speed Microsoft could add this to Copilot in 6 months if they wanted We're racing against someone with infinite resources How we sleep anyway: Focus: Ship one product (CodeGraft) first, not six De-risk: Validate core assumptions with design partners before scaling Iterate: Fast feedback loops, weekly customer calls Transparency: Honest about what we can't do yet Team: Hire people who've built platforms before (reduce learning curve)</li> </ol> <p>FINAL SUMMARY: THE INVARIANT PITCH One-sentence: We prevent AI-generated architectural drift before it ships\u2014using a knowledge graph that understands what your code should do, not just what it does. Three-sentence: As AI accelerates development velocity 3-5x, architectural drift is happening at machine speed. Invariant Continuum Technologies builds the structural integrity layer for complex adaptive systems\u2014a knowledge graph platform that continuously validates reality against intent. Our first product, CodeGraft, blocks AI-generated code that violates your architecture before it reaches production. One-paragraph:  Invariant Continuum Technologies is building the governance layer for the AI era. Our Continuum Graph Platform ingests multi-modal data (code, docs, operations, supply chains) into a unified knowledge graph, then uses GraphRAG to detect when reality diverges from intent. We deliver this as six products spanning software governance (CodeGraft), institutional memory (Chronicle), supply chain integrity (Sentinel), clinical compliance (TrialGuard), enterprise architecture (Nexus EA), and robotics safety (Nexus Agent). We're starting with CodeGraft to solve the immediate crisis: AI code assistants generating architectural drift faster than human reviewers can catch it. By Year 5, we aim to be the de facto standard for ensuring that autonomous systems\u2014from code to robots\u2014do what they're designed to do.</p> <p>Does this comprehensively answer the critical questions for Invariant Continuum Technologies? Which areas need deeper detail or refinement?</p>"},{"location":"product/gap-analysis/","title":"\ud83d\udcca Substrate Platform: Gap Analysis","text":""},{"location":"product/gap-analysis/#summary","title":"Summary","text":"<p>Based on a comprehensive analysis of the codebase against the README.md requirements, the current implementation is a frontend prototype/MVP with visualization features, but it's missing the majority of the core platform capabilities described in the README. The README describes a full B2B SaaS platform for AI code governance, but the current implementation is primarily a UI shell with mock data.</p>"},{"location":"product/gap-analysis/#whats-implemented","title":"\u2705 What's Implemented","text":""},{"location":"product/gap-analysis/#1-basic-dashboard-structure","title":"1. Basic Dashboard Structure","text":"Requirement Status Implementation Project-scoped dashboard \u2705 Partial App has a single-project view structure Tab-based navigation \u2705 Done Sidebar with tabs (graph, memory, rag, policy, terminal, settings) Settings page \u2705 Done Comprehensive settings with LLM config, API settings, graph settings Dark mode UI \u2705 Done Modern dark theme with Tailwind CSS"},{"location":"product/gap-analysis/#2-visual-components","title":"2. Visual Components","text":"Component Status Details Graph visualization \u2705 Done Both Sigma.js (WebGL) and Cytoscape.js implementations Knowledge Fabric view \u2705 Done Interactive graph with lens switching (reality/intent/drift) Policy management UI \u2705 Done CRUD operations for policies with modals RAG Interface \u2705 Done GraphRAG Studio with semantic search UI Memory Interface \u2705 Done Chat-like interface for institutional memory"},{"location":"product/gap-analysis/#3-frontend-technical-stack","title":"3. Frontend Technical Stack","text":"Technology Required Implemented Vite \u2705 Vite 6.x React \u2705 React 19 Tailwind CSS \u2705 Tailwind CSS 4.x Zustand \u2705 State management implemented Lucide Icons \u2705 Used throughout"},{"location":"product/gap-analysis/#critical-gaps-not-implemented","title":"\u274c Critical Gaps (Not Implemented)","text":""},{"location":"product/gap-analysis/#1-multi-tenant-role-based-system-readme-lines-34-68","title":"1. Multi-Tenant + Role-Based System (README Lines 34-68)","text":"Requirement Status Notes Project creation flow \u274c Missing No project creation wizard Multiple projects per user \u274c Missing Single-project view only User roles \u274c Missing No role system (Owner, Admin, Engineer, etc.) Role-based feature access \u274c Missing Everyone sees the same UI Permission management \u274c Missing No permission controls User invitation system \u274c Missing No team management"},{"location":"product/gap-analysis/#2-authentication-team-management","title":"2. Authentication &amp; Team Management","text":"Requirement Status Notes User signup/login \u274c Missing No auth implementation Team creation/management \u274c Missing No team features Account settings \u274c Missing Only app settings exist OAuth/SSO integration \u274c Missing Configured in settings but not functional"},{"location":"product/gap-analysis/#3-core-platform-capabilities-readme-lines-152-172","title":"3. Core Platform Capabilities (README Lines 152-172)","text":"Capability Status Notes Architecture-aware code analysis \u274c Missing Only mock data displayed Policy-as-code enforcement \u26a0\ufe0f UI Only Policy UI exists but no OPA integration AI-generated code governance \u274c Missing No actual governance engine Dependency and data-flow graphs \u26a0\ufe0f Mock Graph displays mock data Continuous compliance validation \u274c Missing No validation pipeline Evidence-backed alerts \u274c Missing Only mock alerts Blast-radius and impact analysis \u274c Missing No impact analysis Living documentation &amp; decision history \u274c Missing No doc tracking Role-based dashboards \u274c Missing Single view for all users"},{"location":"product/gap-analysis/#4-dashboard-requirements-readme-lines-174-196","title":"4. Dashboard Requirements (README Lines 174-196)","text":"Dashboard Feature Status Notes Default landing page per project \u274c Missing No project context Content adapted by user role \u274c Missing No role awareness Surface insights, not raw alerts \u26a0\ufe0f Partial Shows some insights but with mock data Trends, risks, confidence levels \u26a0\ufe0f Partial Some metrics displayed but not real Drill-down executive \u2192 architectural \u2192 code-level \u274c Missing No hierarchical drill-down"},{"location":"product/gap-analysis/#5-core-problem-features-readme-lines-70-82","title":"5. Core Problem Features (README Lines 70-82)","text":"Feature Status Notes AI-generated code governance \u274c Missing No AI code analysis Architectural intent preservation \u274c Missing No intent tracking Tribal knowledge capture \u26a0\ufe0f UI Only Memory Interface exists but not functional Auditable evidence of correctness \u274c Missing No audit trail Security compliance \u274c Missing No security checks"},{"location":"product/gap-analysis/#6-persona-specific-features","title":"6. Persona-Specific Features","text":"<p>Engineering Leadership (VP/Head) *   Visibility into architectural health: \u26a0\ufe0f Mock data *   Proof of standards enforcement: \u274c Missing *   Reduced manual code review load: \u274c Missing *   SOC 2 / audit-ready evidence: \u274c Missing *   Confidence the system is refactorable: \u274c Missing</p> <p>Staff/Principal Engineers *   Architecture enforcement at scale: \u274c Missing *   Living documentation generated from reality: \u274c Missing *   Protection against pattern drift: \u26a0\ufe0f UI Only *   Data-backed proof architecture rules are followed: \u274c Missing</p> <p>Security &amp; AppSec *   Detect architectural security flaws: \u274c Missing *   Verify data flow boundaries: \u274c Missing *   AI-aware security analysis: \u274c Missing *   Continuous compliance evidence: \u274c Missing *   High signal, low false positives: \u274c Missing</p> <p>Product &amp; Engineering Management *   Traceability from requirements \u2192 code \u2192 behavior: \u274c Missing *   Visibility into technical debt: \u274c Missing *   Predictable roadmap confidence: \u274c Missing *   Reduced incidents and surprises: \u274c Missing</p> <p>Enterprise Leadership *   Independent validation: \u274c Missing *   Risk, security, AI governance transparency: \u274c Missing *   Audit-grade attestations: \u274c Missing *   Real-time \"actual vs planned\" architecture: \u26a0\ufe0f Graph exists but with mock data</p>"},{"location":"product/gap-analysis/#partial-implementations-mockui-only","title":"\u26a0\ufe0f Partial Implementations (Mock/UI Only)","text":""},{"location":"product/gap-analysis/#1-graph-visualization","title":"1. Graph Visualization","text":"<ul> <li>\u2705 Beautiful UI with Sigma.js and Cytoscape.js</li> <li>\u274c All data is mock/hardcoded in JSON files</li> <li>\u274c No real code analysis pipeline feeding the graph</li> </ul>"},{"location":"product/gap-analysis/#2-policy-engine","title":"2. Policy Engine","text":"<ul> <li>\u2705 Policy CRUD UI with modals</li> <li>\u2705 Status and severity indicators</li> <li>\u274c No OPA Rego integration</li> <li>\u274c No actual policy enforcement</li> </ul>"},{"location":"product/gap-analysis/#3-drift-detection","title":"3. Drift Detection","text":"<ul> <li>\u2705 Drift lens visualization</li> <li>\u2705 Drift resolver modal</li> <li>\u274c No actual drift detection algorithm</li> <li>\u274c Mock violations only</li> </ul>"},{"location":"product/gap-analysis/#4-llmrag-integration","title":"4. LLM/RAG Integration","text":"<ul> <li>\u2705 Settings UI for LLM configuration</li> <li>\u2705 RAG Interface UI</li> <li>\u274c No actual LLM integration</li> <li>\u274c No GraphRAG pipeline</li> <li>\u274c No vector database (Qdrant)</li> </ul>"},{"location":"product/gap-analysis/#5-connectors","title":"5. Connectors","text":"<ul> <li>\u2705 Connector settings UI (GitHub, Jira, Confluence, Slack)</li> <li>\u274c No actual OAuth flows</li> <li>\u274c No data ingestion pipelines</li> </ul>"},{"location":"product/gap-analysis/#architecture-comparison","title":"\ud83c\udfd7\ufe0f Architecture Comparison","text":""},{"location":"product/gap-analysis/#readme-vision-full-stack","title":"README Vision (Full Stack)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Frontend (React)                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Rust Ingestion (tree-sitter, stack-graphs)                       \u2502\n\u2502 Go Governance Engine (OPA)                                       \u2502\n\u2502 Python GraphRAG (FastAPI, vLLM)                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 PostgreSQL \u2502 Neo4j \u2502 Qdrant \u2502 Redis \u2502 NATS JetStream            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"product/gap-analysis/#current-implementation","title":"Current Implementation","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Frontend (React)                          \u2502\n\u2502              Vite + Tailwind + Zustand + Sigma.js                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                      Mock Data (JSON files)                      \u2502\n\u2502              No backend \u2502 No database \u2502 No APIs                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"product/gap-analysis/#files-needed-but-missing","title":"\ud83d\udcc1 Files Needed But Missing","text":"<p>Based on the ROADMAP.md, these components are not implemented:</p> <p>Backend Services *   Rust ingestion pipeline *   Go governance engine *   Python GraphRAG service</p> <p>Database Schemas *   PostgreSQL migrations *   Neo4j graph schema *   Qdrant vector collections</p> <p>Authentication *   Auth provider integration *   JWT/session management *   Role-based access control</p> <p>CI/CD Integration *   GitHub Actions for policy evaluation *   Pre-commit hooks *   Automated testing</p>"},{"location":"product/gap-analysis/#completion-estimate","title":"\ud83d\udcc8 Completion Estimate","text":"Category Estimated Completion Frontend UI/UX 70% Multi-tenancy &amp; RBAC 0% Backend Services 0% Database Layer 0% Authentication 0% AI/ML Integration 0% CI/CD Integration 0% Overall Platform ~15%"},{"location":"product/gap-analysis/#recommendations","title":"\ud83c\udfaf Recommendations","text":""},{"location":"product/gap-analysis/#immediate-priorities","title":"Immediate Priorities","text":"<ol> <li>Implement Authentication: User signup/login with role assignment.</li> <li>Add Project Management: Create/switch projects per user.</li> <li>Implement Basic Backend: Start with a simple API for CRUD operations.</li> <li>Connect Real Data Sources: GitHub connector as first integration.</li> </ol>"},{"location":"product/gap-analysis/#medium-term","title":"Medium-Term","text":"<ul> <li>Rust ingestion pipeline for code analysis.</li> <li>OPA integration for policy enforcement.</li> <li>Neo4j for graph storage.</li> <li>Basic RBAC implementation.</li> </ul>"},{"location":"product/gap-analysis/#long-term","title":"Long-Term","text":"<ul> <li>Full GraphRAG pipeline.</li> <li>Multi-tenancy.</li> <li>Enterprise features (SSO, audit logs).</li> <li>Kubernetes deployment.</li> </ul>"},{"location":"product/requirements/","title":"Structural Integrity Platform (Substrate) - Product Requirements","text":""},{"location":"product/requirements/#overview","title":"Overview","text":"<p>Substrate Platform is a governance layer over modern software delivery \u2014 powered by a live knowledge graph and an internal integration marketplace.</p> <p>AI has massively accelerated code creation but often at the cost of architectural consistency, security guarantees, and shared understanding. This platform restores control, visibility, and confidence creating a system that: - Governs AI-generated code - Preserves architectural intent - Captures tribal knowledge - Produces auditable, provable evidence of correctness, security, and compliance</p> <p>Think of this platform as: \"GitHub + Snyk + Jira + Architecture Governance + Institutional Memory \u2014 powered by a live knowledge graph.\"</p>"},{"location":"product/requirements/#core-problem-solved","title":"Core Problem Solved","text":"<p>The accelerated velocity of AI development has destroyed: - Architectural consistency - Security guarantees - Shared system understanding</p> <p>This platform solves these issues by: - Governing AI-generated code. - Enforcing architectural and security policies. - Capturing institutional knowledge. - Producing provable, auditable evidence.</p>"},{"location":"product/requirements/#key-capabilities","title":"Key Capabilities","text":"<p>The system is graph-native and supports: - Knowledge Graph: A unified graph of code, systems, data flows, and decisions. - Architecture-Aware Code Analysis: Deep understanding of system structure. - Policy-as-Code Enforcement: Governance for architecture, security, and compliance. - Continuous Compliance Validation: Automated checks against standards. - Evidence-Backed Alerts: High-signal, low-noise alerts based on impact and blast radius analysis. - Living Documentation: Documentation generated from reality, not static files. - Role-Based Dashboards: Consolidating data into views relevant for specific personas.</p>"},{"location":"product/requirements/#roles-permissions","title":"Roles &amp; Permissions","text":"<p>This is a multi-tenant, role-based system.</p>"},{"location":"product/requirements/#access-model","title":"Access Model","text":"<ul> <li>Organization: Top-level entity containing projects.</li> <li>Project: Contains installed connectors, users, policies, and knowledge graph data.</li> <li>Users: Belong to organizations and have roles within projects.</li> </ul>"},{"location":"product/requirements/#roles","title":"Roles","text":"<p>Role definitions are data-driven. Feature access is permission-based. -   Owner: Can invite users, assign roles, grant feature-level access. -   Admin -   Engineer -   Security -   Product -   Read-only / Executive</p>"},{"location":"product/requirements/#personas","title":"Personas","text":"<p>The platform serves multiple personas without duplicating logic: *   Engineering Leadership: Needs visibility into architectural health and audit-ready evidence. *   Staff / Principal Engineers: Needs architecture enforcement and drift protection. *   Security &amp; AppSec: Needs detection of architectural flaws and boundary verification. *   Product &amp; Engineering Management: Needs traceability and visibility into technical debt. *   Enterprise Leadership: Needs independent validation and risk transparency.</p>"},{"location":"product/requirements/#user-flow","title":"User Flow","text":"<ol> <li>Sign Up / Log In: User authenticates.</li> <li>Project Creation:<ul> <li>Set Project name &amp; description.</li> <li>Select data connectors from the Internal Marketplace.</li> <li>Provide minimal required configuration for connectors.</li> </ul> </li> <li>Dashboard: User lands on the project-scoped dashboard.</li> </ol> <p>Note: No dashboard exists without a project context.</p>"},{"location":"product/requirements/#internal-marketplace-data-connectors","title":"Internal Marketplace: Data Connectors","text":"<p>Connectors are treated as modular products, not hard-coded integrations. Use a \"Stripe-style integrations\" approach.</p>"},{"location":"product/requirements/#principles","title":"Principles","text":"<ul> <li>Modular &amp; Versioned: Connectors are composable.</li> <li>Project-Scoped Installation: Installed per project.</li> <li>API-Configured: Managed via generic configuration APIs.</li> <li>Permission-Governed: Access controlled by roles.</li> </ul>"},{"location":"product/requirements/#contracts","title":"Contracts","text":"<p>Every connector must: 1.  Expose a standard contract (metadata, auth requirements, capabilities). 2.  Be configured through a generic configuration API. 3.  Emit data/events in a normalized format.</p> <p>Rule: No connector-specific logic in the UI.</p>"},{"location":"product/requirements/#dashboard-requirements","title":"Dashboard Requirements","text":"<p>The dashboard is the default landing page per project.</p> <ul> <li>Data-Driven &amp; Role-Aware: Adapts content based on user role.</li> <li>Insight-Focused: Surfaces insights, trends, risks, and confidence levels (not just raw data).</li> <li>Drill-Down Capable: From Executive Summary -&gt; Architectural State -&gt; Policy Violations -&gt; Code-Level Evidence.</li> <li>Avoid: Static diagrams, manual documentation, UI-only business rules.</li> </ul>"},{"location":"product/strategy/","title":"Structural Integrity Platform: Product Strategy &amp; Market Positioning","text":""},{"location":"product/strategy/#executive-summary","title":"Executive Summary","text":"<p>Product: Structural Integrity Platform - AI-powered architectural drift detection and knowledge graph platform Core Innovation: Treats software architecture as a living knowledge graph, enforcing architectural intent through policy-as-code MarketSoftware development teams (5-500+ developers) Deployment: Cloud SaaS + Self-Hosted (air-gap capable for enterprise) Pricing: $200-500/developer/year (tier-dependent)</p>"},{"location":"product/strategy/#product-definition","title":"Product Definition","text":""},{"location":"product/strategy/#core-value-proposition","title":"Core Value Proposition","text":"<p>The Structural Integrity Platform solves three critical enterprise software problems:</p> <ol> <li> <p>Architectural Drift Detection: As AI code generation accelerates (GitHub Copilot, Cursor, etc.), maintaining architectural consistency becomes exponentially harder. The platform detects when AI-generated or human-written code violates architectural intent in real-time.</p> </li> <li> <p>Institutional Memory Preservation: Tribal knowledge evaporates as developers leave. The platform captures, graphs, and semantically indexes the \"why\" behind decisions from Slack conversations, PRs, ADRs, and meetings.</p> </li> <li> <p>Policy-as-Code Governance: Traditional architecture reviews don't scale. The platform enforces boundaries, layer constraints, and dependency rules through OPA/Rego policies evaluated against the knowledge graph in CI/CD pipelines (&lt;100ms per policy).</p> </li> </ol>"},{"location":"product/strategy/#trifunctional-architecture","title":"Trifunctional Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         DEVELOPER EXPERIENCE LAYER                       \u2502\n\u2502  VS Code Extension \u2502 GitHub Action \u2502 CLI \u2502 Web Dashboard \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               PLATFORM CORE SERVICES                      \u2502\n\u2502   Ingestion (Rust) \u2502 GraphRAG (Python) \u2502 Governance (Go) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            MULTI-MODAL KNOWLEDGE GRAPH                    \u2502\n\u2502    PostgreSQL \u2502 Neo4j \u2502 Qdrant \u2502 Redis \u2502 MinIO           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"product/strategy/#core-capabilities","title":"Core Capabilities","text":""},{"location":"product/strategy/#1-drift-detection-architectural-sentinel","title":"1. Drift Detection (Architectural Sentinel)","text":"<p>Problem: Developers bypass architectural patterns unknowingly; AI tools generate non-compliant code.</p> <p>Solution: Real-time policy evaluation against the knowledge graph.</p> <p>Developer Workflow: 1. Developer commits code \u2192 GitHub Action triggers \u2192 Platform ingests AST 2. Drift detected (e.g., \"Presentation layer calling Database directly\") 3. PR check fails with graph visualization showing violation path 4. Developer clicks \"View Policy\" \u2192 Sees Rego rule + rationale (ADR) 5. Developer fixes manually or accepts auto-suggested pattern 6. PR updated \u2192 Check passes</p> <p>Technical Implementation: - Rust ingestion: tree-sitter AST extraction (6.5ms for 2,157 lines) - Neo4j graph: Deterministic code structure graph - OPA evaluation: Sub-millisecond policy checks with <code>graph.reachable</code> - CI/CD integration: GitHub Actions, GitLab CI, Jenkins plugins</p> <p>Example Policies: - Layer violations: \"Infrastructure layer cannot depend on Domain layer\" - Circular dependencies: \"No component X can transitively depend on itself\" - Cross-boundary access: \"Frontend cannot access Database directly; must route through API\" - Dependency constraints: \"Ban log4j versions &lt; 2.17.0\"</p> <p>Graduated Enforcement: - Observe mode: Log violations, don't block (tune false positives) - Advise mode: Show warnings, allow override with justification - Enforce mode: Block PR merge on violation</p>"},{"location":"product/strategy/#2-institutional-memory-knowledge-graph-chronicle","title":"2. Institutional Memory (Knowledge Graph Chronicle)","text":"<p>Problem: 28.9% of GitHub projects contain outdated documentation; tribal knowledge lives in senior developers' heads.</p> <p>Solution: GraphRAG semantic indexing + staleness detection + notification system.</p> <p>Knowledge Capture Workflow: 1. Platform ingests multi-modal sources:    - Code + AST (tree-sitter)    - GitHub PRs, issues, discussions    - Jira tickets, epics, components    - Confluence/Notion docs    - Slack conversations (designated channels) 2. GraphRAG 7-phase pipeline:    - Entity extraction (Service, API, Component, Team, Person)    - Relationship extraction (depends_on, owns, implements, discusses)    - Hierarchical Leiden community detection    - LLM-generated community summaries 3. Qdrant vector embeddings for semantic search 4. Neo4j graph for traversal queries</p> <p>Query Strategies: - Local Search: \"What services depend on the auth service?\" (graph traversal) - Global Search: \"What are systemic reliability risks in our architecture?\" (map-reduce across community reports) - DRIFT Search: \"How does our payment service's fault tolerance compare to our overall system patterns?\" (hybrid breadth + depth)</p> <p>Staleness Detection: <pre><code>Freshness(t) = e^(-\u03bb \u00d7 \u0394t)\n</code></pre></p> <p>Decay rates (\u03bb): - Architectural docs: 0.01 (~70-day half-life) - API docs: 0.05 (~14-day half-life) - Deployment guides: 0.1 (~7-day half-life)</p> <p>Composite Staleness Score (0-1): - Time decay: 25% - Code drift (tracked files changed, doc unchanged): 30% - Link rot (broken URLs): 15% - Reference validity (APIs/classes still exist): 20% - Inverse view frequency: 10%</p> <p>Smart Notifications (fatigue prevention): - Dashboard flag at score &gt; 0.5 - Slack DM to document owner at day 3 - Team channel escalation at day 7 - Engineering manager report at day 14 - Deprecation/archival suggestion at day 30</p> <p>Batching: Daily/weekly digests, max 3 push notifications/hour/user</p>"},{"location":"product/strategy/#3-policy-enforcement-guardrails-at-scale","title":"3. Policy Enforcement (Guardrails at Scale)","text":"<p>Problem: Manual architecture reviews don't scale; inconsistency in enforcement.</p> <p>Solution: Rego policy-as-code evaluated against Neo4j graph state.</p> <p>Policy Builder Workflow (No-Code): 1. Architect uses visual policy builder:    - Select entity types (e.g., \"Service\")    - Define constraints (e.g., \"Must have owner\")    - Add graph traversal rules (e.g., \"Cannot transitively depend on self\") 2. Platform generates Rego code automatically 3. Policy starts in OBSERVE mode 4. Governance dashboard shows:    - Violations logged: 10    - False positive rate: 9%    - Developer override rate: 5% 5. Architect tunes policy (adjusts scope, adds exceptions) 6. Promote to ADVISE mode (warnings, allow bypass) 7. After confidence builds, promote to ENFORCE mode</p> <p>Sample Rego Policies:</p> <p>Circular Dependency Detection: <pre><code>package architecture.circular\nimport rego.v1\n\ncircular_dependency contains cycle if {\n    some component\n    data.architecture.dependencies[component]\n    some neighbor in data.architecture.dependencies[component]\n    reachable := graph.reachable(data.architecture.dependencies, {neighbor})\n    component in reachable\n    cycle := {\"component\": component, \"via\": neighbor,\n      \"msg\": sprintf(\"Circular: '%s' -&gt; '%s' -&gt; ... -&gt; '%s'\", \n                     [component, neighbor, component])}\n}\n</code></pre></p> <p>Layer Violation Detection: <pre><code>package architecture.layers\nimport rego.v1\n\nlayer_order := {\"presentation\": 4, \"application\": 3, \n                \"domain\": 2, \"infrastructure\": 1, \"data\": 0}\n\nlayer_violation contains violation if {\n    some source, targets in data.architecture.dependencies\n    some target in targets\n    layer_order[data.architecture.component_layers[source]] &lt; \n      layer_order[data.architecture.component_layers[target]]\n    violation := {\"source\": source, \"target\": target,\n      \"msg\": sprintf(\"Layer violation: '%s' (%s) depends on '%s' (%s)\",\n        [source, data.architecture.component_layers[source], \n         target, data.architecture.component_layers[target]])}\n}\n</code></pre></p> <p>Policy Effectiveness Metrics: - Precision: % of flagged violations that are true positives (target: &gt;85%) - False Positive Rate: Target &lt;10% - Developer Override Rate: Target &lt;15% (high rates trigger policy review)</p>"},{"location":"product/strategy/#target-market-segmentation","title":"Target Market &amp; Segmentation","text":""},{"location":"product/strategy/#primary-market-software-development-teams","title":"Primary Market: Software Development Teams","text":"<p>Small Teams (5-20 developers): - Pain Point: Informal architecture docs; knowledge in 1-2 senior developers' heads - Deployment: Docker Compose, self-hosted - Pricing: $200/dev/year ($1K-4K/year total) - Focus: Knowledge preservation, basic drift detection</p> <p>Growth Companies (20-100 developers): - Pain Point: Scaling pains; inconsistent architecture enforcement across teams - Deployment: Kubernetes, managed or self-hosted - Pricing: $350/dev/year ($7K-35K/year total) - Focus: Policy enforcement, multi-team coordination, DORA metrics</p> <p>Enterprise (100-500+ developers): - Pain Point: Decades of technical debt; compliance (SOC 2, ISO 27001); M&amp;A integration - Deployment: Multi-cluster Kubernetes, air-gapped options - Pricing: $500/dev/year + professional services ($50K-250K+ /year) - Focus: Advanced governance, audit trails, legacy system integration</p>"},{"location":"product/strategy/#secondary-markets","title":"Secondary Markets","text":"<p>Consulting Firms: - Use platform for client architecture assessments - White-label option for reports - Volume licensing</p> <p>Academia/Research: - Free tier for educational institutions - Research partnerships for GraphRAG/policy research</p>"},{"location":"product/strategy/#competitive-landscape","title":"Competitive Landscape","text":""},{"location":"product/strategy/#direct-competitors","title":"Direct Competitors","text":"<p>ArchUnit / jQAssistant: - Strengths: Mature, Java-focused, good for unit test-level checks - Weaknesses: No knowledge graph, no semantic search, limited language support - Our Advantage: Multi-language, GraphRAG, real-time CI/CD enforcement</p> <p>Sonargraph: - Strengths: Commercial support, good visualization - Weaknesses: Expensive ($1,500+/dev/year), legacy UI, no AI/ML - Our Advantage: Modern UI, AI-powered insights, 1/3 the cost</p> <p>Lattix: - Strengths: Dependency structure matrix visualization - Weaknesses: Manual dependency mapping, no policy automation - Our Advantage: Automated AST extraction, policy-as-code, semantic search</p>"},{"location":"product/strategy/#indirect-competitors","title":"Indirect Competitors","text":"<p>Backstage (Spotify): - Positioning: Service catalog, developer portal - Overlap: Entity model, ownership tracking - Differentiation: We add drift detection + policy enforcement + GraphRAG</p> <p>SonarQube/Code Climate: - Positioning: Code quality, security scanning - Overlap: Code analysis, CI/CD integration - Differentiation: We focus on architecture-level patterns, not code-level metrics</p>"},{"location":"product/strategy/#pricing-packaging","title":"Pricing &amp; Packaging","text":""},{"location":"product/strategy/#tier-1-essentials-200devyear","title":"Tier 1: Essentials ($200/dev/year)","text":"<ul> <li>Features:</li> <li>Code ingestion (Rust pipeline)</li> <li>Basic drift detection (5 pre-built policies)</li> <li>Neo4j graph storage</li> <li>GitHub/GitLab integration</li> <li>Community support</li> <li>Limits:</li> <li>1 organization</li> <li>20 repositories max</li> <li>7-day data retention</li> <li>Target: Small teams, startups</li> </ul>"},{"location":"product/strategy/#tier-2-professional-350devyear","title":"Tier 2: Professional ($350/dev/year)","text":"<ul> <li>All Essentials features,plus:</li> <li>GraphRAG semantic search (Qdrant)</li> <li>Custom policy builder (visual Rego editor)</li> <li>Slack/Jira/Confluence connectors</li> <li>Staleness detection + notifications</li> <li>DORA metrics integration</li> <li>90-day data retention</li> <li>Email support</li> <li>Target: Growth companies</li> </ul>"},{"location":"product/strategy/#tier-3-enterprise-500devyear","title":"Tier 3: Enterprise ($500/dev/year)","text":"<ul> <li>All Professional features, plus:</li> <li>Unlimited repositories</li> <li>Air-gapped deployment</li> <li>SAML SSO, RBAC, audit logs</li> <li>SLA (99.9% uptime)</li> <li>Dedicated support engineer</li> <li>Professional services (architecture reviews, custom policies)</li> <li>1-year+ data retention</li> <li>SOC 2 Type II, ISO 27001 compliance</li> <li>Target: Fortune 500, regulated industries</li> </ul>"},{"location":"product/strategy/#add-ons-all-tiers","title":"Add-Ons (All Tiers)","text":"<ul> <li>Additional LLM credits: $50/100K tokens (for teams exceeding base quotas)</li> <li>Custom connectors: $5K one-time + $500/month maintenance</li> <li>Architecture health audit: $10K-50K (professional services)</li> </ul>"},{"location":"product/strategy/#go-to-market-strategy","title":"Go-to-Market Strategy","text":""},{"location":"product/strategy/#phase-1-product-led-growth-months-0-12","title":"Phase 1: Product-Led Growth (Months 0-12)","text":"<p>Tactics: 1. Open-source \"Lite\" version: Core ingestion pipeline + basic policies (MIT license)    - Purpose: Developer adoption, community feedback, SEO 2. Freemium SaaS: Free tier (5 devs, 5 repos, 30-day retention) 3. Content marketing: Blog posts on architecture drift, GraphRAG tutorials 4. Developer evangelism: Conference talks, podcasts, YouTube tutorials</p> <p>Metrics: - 1,000 GitHub stars (6 months) - 500 free tier signups (12 months) - 20 paying customers (12 months, $50K ARR)</p>"},{"location":"product/strategy/#phase-2-sales-assisted-growth-months-12-24","title":"Phase 2: Sales-Assisted Growth (Months 12-24)","text":"<p>Tactics: 1. Outbound sales: Target Series B+ companies (100-500 devs) 2. Case studies: Publish ROI data from early customers 3. Partnerships: Integrate with Backstage, GitLab, Snyk 4. Certifications: SOC 2 Type II, ISO 27001</p> <p>Metrics: - 100 paying customers (24 months, $500K ARR) - 10 enterprise deals (&gt;100 devs)</p>"},{"location":"product/strategy/#phase-3-enterprise-expansion-months-24","title":"Phase 3: Enterprise Expansion (Months 24+)","text":"<p>Tactics: 1. Enterprise sales team: 5-10 AEs, 2-3 SEs 2. Channel partnerships: System integrators (Deloitte, Accenture) 3. Industry-specific solutions: FinTech, Healthcare, Government 4. FedRAMP, HIPAA, PCI DSS certifications</p> <p>Metrics: - $5M ARR (36 months) - 50+ enterprise customers</p>"},{"location":"product/strategy/#compliance-certifications","title":"Compliance &amp; Certifications","text":""},{"location":"product/strategy/#tier-1-minimum-viable-compliance-months-0-12","title":"Tier 1: Minimum Viable Compliance (Months 0-12)","text":"<p>Essential for SaaS market entry:</p> Certification Purpose Timeline Cost SOC 2 Type II Secure data handling 9-12 months $50K-100K GDPR Compliance EU customers (4% revenue fine risk) 3-6 months $20K-50K ISO 27001 Financial services, healthcare 6-12 months $30K-80K Privacy Shield / DPA Data processing agreements 3 months $10K-20K"},{"location":"product/strategy/#tier-2-competitive-advantage-months-12-24","title":"Tier 2: Competitive Advantage (Months 12-24)","text":"<p>Unlocks specialized high-value markets:</p> Certification Market Annual Value Timeline FedRAMP Moderate US federal government $100B+ market 12-18 months HIPAA Healthcare 18% of US GDP 6-12 months PCI DSS FinTech/payments Critical for fintech 6-9 months"},{"location":"product/strategy/#tier-3-industry-specific-months-24","title":"Tier 3: Industry-Specific (Months 24+)","text":"Certification Industry Notes 21 CFR Part 11 Pharmaceuticals Clinical trials, FDA ITAR Defense DoD contractors TISAX Automotive VW, BMW suppliers"},{"location":"product/strategy/#open-source-strategy","title":"Open Source Strategy","text":""},{"location":"product/strategy/#licensing-philosophy","title":"Licensing Philosophy","text":"<p>Open-source components (MIT License): - Core ingestion pipeline (Rust tree-sitter wrapper) - Pre-built Rego policy library - Neo4j/Qdrant schema definitions - CLI tool</p> <p>Proprietary components (Commercial License): - GraphRAG orchestration engine - Web dashboard (React + Sigma.js) - Advanced staleness detection - Multi-tenant SaaS infrastructure - Enterprise features (SAML SSO, RBAC, audit logs)</p> <p>Rationale: - Open-source drives adoption + community contributions - Proprietary components protect competitive moats - \"Open core\" model (GitLab, Sentry, Airbyte precedent)</p>"},{"location":"product/strategy/#dependency-license-compliance","title":"Dependency License Compliance","text":"<p>Allowed: - MIT, Apache 2.0, BSD (permissive)</p> <p>Forbidden: - GPL, AGPL (viral copyleft\u2014would force our code open-source)</p> <p>Automated Scanning: - Snyk license scanning in CI/CD - Block PRs introducing GPL/AGPL dependencies</p>"},{"location":"product/strategy/#security-trust-standards","title":"Security &amp; Trust Standards","text":"Standard Coverage Implementation OAuth 2.0 / OIDC Authentication SSO integration (Okta, Azure AD, Google) SAML 2.0 Enterprise SSO Fortune 500 requirement TLS 1.3 Data in transit All API calls encrypted AES-256 Data at rest PostgreSQL/Neo4j/Qdrant encryption RBAC Access control Role-based permissions (Admin, Architect, Developer, Viewer) Audit Logging Compliance Immutable trail: \"Who did what, when\" Secrets Management API keys, tokens Vault/AWS Secrets Manager integration"},{"location":"product/strategy/#cost-model-unit-economics","title":"Cost Model &amp; Unit Economics","text":""},{"location":"product/strategy/#cost-structure-small-team-example-20-developers","title":"Cost Structure (Small Team Example: 20 developers)","text":"<p>Infrastructure (Self-Hosted): - Docker Compose: $0 (runs on existing servers) - OR AWS EC2 (t3.xlarge \u00d7 2): ~$200/month</p> <p>LLM Costs: - gpt-4o-mini for GraphRAG indexing: ~$0.01 per processing pass - Initial 100K-line codebase: ~$50 one-time - Incremental updates: ~$5-10/month</p> <p>Storage: - PostgreSQL/Neo4j/Qdrant: ~50GB for 100K lines \u2192 ~$20/month (AWS)</p> <p>Total: ~$250/month operational cost for 20 devs</p> <p>Revenue: 20 devs \u00d7 $200/dev/year = $4,000/year ($333/month)</p> <p>Gross Margin: ($333 - $250) / $333 = ~25% at small scale</p> <p>Note: Margins improve significantly at scale due to: - Fixed infrastructure costs amortized over more users - Incremental GraphRAG updates (not full re-indexing) - LLM caching reducing redundant API calls</p>"},{"location":"product/strategy/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"product/strategy/#technical-risks","title":"Technical Risks","text":"<ol> <li>GraphRAG Incremental Indexing:</li> <li>Risk: Currently requires full re-index on changes</li> <li> <p>Mitigation: Corpus partitioning by service/team; re-index only changed partitions</p> </li> <li> <p>stack-graphs Archived Status:</p> </li> <li>Risk: GitHub repository archived (no active development)</li> <li> <p>Mitigation: Code remains functional; fork if needed; 150+ alternative tree-sitter grammars</p> </li> <li> <p>OPA-Neo4j Sync Latency:</p> </li> <li>Risk: Not real-time by default</li> <li>Mitigation: OPAL for event-driven sync; &lt;100ms policy evaluation sufficient for CI/CD</li> </ol>"},{"location":"product/strategy/#business-risks","title":"Business Risks","text":"<ol> <li>Competitor Response:</li> <li>Risk: Sonargraph/Lattix add AI features</li> <li> <p>Mitigation: Open-source moat; GraphRAG expertise; polyglot architecture</p> </li> <li> <p>Market Education:</p> </li> <li>Risk: \"Architecture drift\" not a recognized category</li> <li> <p>Mitigation: Content marketing; ROI calculators; free tier for trial</p> </li> <li> <p>Compliance Timelines:</p> </li> <li>Risk: SOC 2 delays enterprise sales</li> <li>Mitigation: Self-hosted option for early enterprise customers; parallel compliance workstream</li> </ol>"},{"location":"product/strategy/#success-metrics","title":"Success Metrics","text":""},{"location":"product/strategy/#north-star-metric","title":"North Star Metric","text":"<p>Architectural Violations Prevented (per customer per month)</p>"},{"location":"product/strategy/#supporting-metrics","title":"Supporting Metrics","text":"<p>Product Adoption: - Weekly Active Users (WAU) - Repositories connected - Policies configured - GraphRAG queries executed</p> <p>Customer Success: - Time to first value (TTFV): &lt;7 days - Net Promoter Score (NPS): &gt;50 - Churn rate: &lt;5% annual</p> <p>Revenue: - Annual Recurring Revenue (ARR) - Customer Acquisition Cost (CAC): &lt;$5K - Lifetime Value (LTV): &gt;$20K - LTV:CAC ratio: &gt;3:1</p>"},{"location":"product/strategy/#conclusion","title":"Conclusion","text":"<p>The Structural Integrity Platform addresses a critical yet underserved market: architecture governance at scale. By combining deterministic code analysis, GraphRAG semantic intelligence, and policy-as-code enforcement, we solve three enterprise pains (drift detection, knowledge preservation, scalable governance) with a single unified platform.</p> <p>The open-core business model balances community adoption with commercial sustainability. The polyglot architecture (Rust + Python + Go) provides best-in-class performance for each workload while maintaining a coherent knowledge graph substrate.</p> <p>Success hinges on three pillars: 1. Technical excellence: Dual-graph strategy (AST + LLM) for accuracy 2. Developer experience: Sub-100ms policy checks; beautiful graph visualization 3. Trust &amp; compliance: SOC 2, ISO 27001, air-gapped deployments for enterprise</p> <p>With a clear scaling path (Docker Compose \u2192 Kubernetes \u2192 Multi-cluster) and a favorable cost model (~$5-50 per full GraphRAG re-index), the platform is positioned to serve teams from 5 to 500+ developers.</p>"}]}